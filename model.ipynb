{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from read_data import read_data, get_squad_data_filter, update_config\n",
    "import flag as fg\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from helper import get_initializer, dropout, conv1d, multi_conv1d\n",
    "from helper import flatten, reconstruct, linear, highway_layer, highway_network, mask, exp_mask, softmax\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from helper import grouper\n",
    "from utils import index\n",
    "from tqdm import tqdm\n",
    "from read_data import DataSet\n",
    "\n",
    "config = fg.main(_)\n",
    "config.out_dir = os.path.join(config.out_base_dir, config.model_name, str(config.run_id).zfill(2))\n",
    "\n",
    "assert config.load or config.mode == 'train', \"config.load must be True if not training\"\n",
    "if not config.load and os.path.exists(config.out_dir):\n",
    "    shutil.rmtree(config.out_dir)\n",
    "\n",
    "config.save_dir = os.path.join(config.out_dir, \"save\")\n",
    "config.log_dir = os.path.join(config.out_dir, \"log\")\n",
    "config.eval_dir = os.path.join(config.out_dir, \"eval\")\n",
    "config.answer_dir = os.path.join(config.out_dir, \"answer\")\n",
    "if not os.path.exists(config.out_dir):\n",
    "    os.makedirs(config.out_dir)\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.mkdir(config.save_dir)\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.mkdir(config.log_dir)\n",
    "if not os.path.exists(config.answer_dir):\n",
    "    os.mkdir(config.answer_dir)\n",
    "if not os.path.exists(config.eval_dir):\n",
    "    os.mkdir(config.eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87507/87599 examples from train\n",
      "Loaded 10544/10570 examples from dev\n"
     ]
    }
   ],
   "source": [
    "data_filter = get_squad_data_filter(config)\n",
    "\n",
    "train_data = read_data(config, 'train', False, data_filter=data_filter)\n",
    "dev_data = read_data(config, 'dev', False, data_filter=data_filter)\n",
    "\n",
    "update_config(config, [train_data, dev_data])\n",
    "\n",
    "word2vec_dict = train_data.shared['lower_word2vec'] if config.lower_word else train_data.shared['word2vec']\n",
    "word2idx_dict = train_data.shared['word2idx']\n",
    "\n",
    "idx2vec_dict = {word2idx_dict[word]: vec for word, vec in word2vec_dict.items() if word in word2idx_dict}\n",
    "emb_mat = np.array([idx2vec_dict[idx] if idx in idx2vec_dict\n",
    "                    else np.random.multivariate_normal(np.zeros(config.word_emb_size), np.eye(config.word_emb_size))\n",
    "                    for idx in range(config.word_vocab_size)])\n",
    "config.emb_mat = emb_mat\n",
    "\n",
    "# pprint(config.__flags, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context and Ques Parameters\n",
    "N = config.batch_size\n",
    "M = config.max_num_sents\n",
    "JX = config.max_sent_size\n",
    "JQ = config.max_ques_size\n",
    "VW = config.word_vocab_size\n",
    "VC = config.char_vocab_size\n",
    "W = config.max_word_size\n",
    "d =  config.hidden_size\n",
    "dc = config.char_emb_size\n",
    "dw = config.word_emb_size\n",
    "dco = config.char_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "x = tf.placeholder('int32', [N, None, None], name='x')\n",
    "cx = tf.placeholder('int32', [N, None, None, W], name='cx')\n",
    "x_mask = tf.placeholder('bool', [N, None, None], name='x_mask')\n",
    "q = tf.placeholder('int32', [N, None], name='q')\n",
    "cq = tf.placeholder('int32', [N, None, W], name='cq')\n",
    "q_mask = tf.placeholder('bool', [N, None], name='q_mask')\n",
    "y1 = tf.placeholder('bool', [N, None, None], name='y1')\n",
    "y2 = tf.placeholder('bool', [N, None, None], name='y2')\n",
    "is_train = tf.placeholder('bool', [], name='is_train')\n",
    "new_emb_mat = tf.placeholder('float', [None, config.word_emb_size], name='new_emb_mat')\n",
    "\n",
    "global_step = tf.get_variable('global_step', shape=[], dtype='int32', initializer=tf.constant_initializer(0), trainable=False)\n",
    "tensor_dict = {}\n",
    "\n",
    "with tf.variable_scope(\"embedding_layer\"):\n",
    "    if config.use_char_emb:\n",
    "        with tf.variable_scope(\"char\"):\n",
    "\n",
    "            char_emb_mat = tf.get_variable(\"char_emb_mat\", shape=[VC, dc], dtype='float')\n",
    "    \n",
    "            Acx = tf.nn.embedding_lookup(char_emb_mat, cx)  # [N, M, JX, W, dc]\n",
    "            Acq = tf.nn.embedding_lookup(char_emb_mat, cq)  # [N, JQ, W, dc]\n",
    "            Acx = tf.reshape(Acx, [-1, JX, W, dc])\n",
    "            Acq = tf.reshape(Acq, [-1, JQ, W, dc])\n",
    "            \n",
    "            filter_sizes = list(map(int, config.out_channel_dims.split(',')))\n",
    "            heights = list(map(int, config.filter_heights.split(',')))\n",
    "            \n",
    "            with tf.variable_scope(\"conv\"):\n",
    "                xx = multi_conv1d(Acx, filter_sizes, heights, \"VALID\", is_train, config.keep_prob, scope=\"xx\")\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                qq = multi_conv1d(Acq, filter_sizes, heights, \"VALID\", is_train, config.keep_prob, scope=\"xx\")\n",
    "\n",
    "                xx = tf.reshape(xx, [-1, M, JX, dco])\n",
    "                qq = tf.reshape(qq, [-1, JQ, dco])\n",
    "            \n",
    "            \n",
    "    if config.use_word_emb:\n",
    "        with tf.name_scope(\"word\"):\n",
    "            \n",
    "            if config.mode == 'train':\n",
    "                word_emb_mat = tf.get_variable(\"word_emb_mat\", dtype='float', shape=[VW, dw], initializer=get_initializer(config.emb_mat))\n",
    "            else:\n",
    "                word_emb_mat = tf.get_variable(\"word_emb_mat\", shape=[VW, dw], dtype='float')\n",
    "            \n",
    "            if config.use_glove_for_unk:\n",
    "                word_emb_mat = tf.concat([word_emb_mat, new_emb_mat], 0)\n",
    "\n",
    "            Ax = tf.nn.embedding_lookup(word_emb_mat, x)  # [N, M, JX, d]\n",
    "            Aq = tf.nn.embedding_lookup(word_emb_mat, q)  # [N, JQ, d]\n",
    "            \n",
    "            tensor_dict['x'] = Ax\n",
    "            tensor_dict['q'] = Aq\n",
    "            \n",
    "        if config.use_char_emb:\n",
    "            xx = tf.concat([xx, Ax], 3)  # [N, M, JX, di]\n",
    "            qq = tf.concat([qq, Aq], 2)  # [N, JQ, di]\n",
    "        else:\n",
    "            xx = Ax\n",
    "            qq = Aq\n",
    "\n",
    "with tf.variable_scope(\"highway_network_layer\"):\n",
    "    xx = highway_network(xx, config.highway_num_layers, is_train=is_train)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    qq = highway_network(qq, config.highway_num_layers, is_train=is_train)\n",
    "\n",
    "    tensor_dict['xx'] = xx\n",
    "    tensor_dict['qq'] = qq\n",
    "    \n",
    "x_len = tf.reduce_sum(tf.cast(x_mask, 'int32'), 2)  # [N, M]\n",
    "q_len = tf.reduce_sum(tf.cast(q_mask, 'int32'), 1)  # [N]\n",
    "\n",
    "flat_len_q = None if q_len is None else tf.cast(flatten(q_len, 0), 'int64')\n",
    "flat_len_x = None if x_len is None else tf.cast(flatten(x_len, 0), 'int64')\n",
    "\n",
    "with tf.variable_scope(\"contextual_layer\"):\n",
    "    cell=tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "\n",
    "    flat_qq = flatten(qq, 2)  \n",
    "    (flat_fwu_outputs, flat_bwu_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_qq, sequence_length=flat_len_q, dtype='float', scope='lstm')\n",
    "    fw_u = reconstruct(flat_fwu_outputs, qq, 2)\n",
    "    bw_u = reconstruct(flat_bwu_outputs, qq, 2)\n",
    "    u = tf.concat([fw_u, bw_u], 2)\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    flat_xx = flatten(xx, 2)  \n",
    "    (flat_fwh_outputs, flat_bwh_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_xx, sequence_length=flat_len_x, dtype='float', scope='lstm')\n",
    "    fw_h = reconstruct(flat_fwh_outputs, xx, 2)\n",
    "    bw_h = reconstruct(flat_bwh_outputs, xx, 2)\n",
    "    h = tf.concat([fw_h, bw_h], 3)\n",
    "    \n",
    "    tensor_dict['u'] = u\n",
    "    tensor_dict['h'] = h\n",
    "    \n",
    "with tf.variable_scope(\"attention_layer\"):\n",
    "    h_aug = tf.tile(tf.expand_dims(h, 3), [1, 1, 1, JQ, 1])\n",
    "    u_aug = tf.tile(tf.expand_dims(tf.expand_dims(u, 1), 1), [1, M, JX, 1, 1])\n",
    "    h_mask_aug = tf.tile(tf.expand_dims(x_mask, 3), [1, 1, 1, JQ])\n",
    "    u_mask_aug = tf.tile(tf.expand_dims(tf.expand_dims(q_mask, 1), 1), [1, M, JX, 1])\n",
    "    hu_mask = h_mask_aug & u_mask_aug\n",
    "\n",
    "    h_u = h_aug * u_aug\n",
    "\n",
    "    with tf.variable_scope(\"similarity\"):\n",
    "        sim = linear([tf.concat([h_aug, u_aug, h_u], -1)], 1, is_train=is_train, scope=\"sim\")\n",
    "        sim = tf.squeeze(sim, [len(sim.get_shape().as_list())-1])\n",
    "        sim = exp_mask(sim, hu_mask)\n",
    "        \n",
    "        # Tensor Dict\n",
    "        a_u = tf.nn.softmax(sim)  \n",
    "        a_h = tf.nn.softmax(tf.reduce_max(sim, 3))\n",
    "        tensor_dict['a_u'] = a_u\n",
    "        tensor_dict['a_h'] = a_h\n",
    "        \n",
    "    with tf.variable_scope(\"context_2_query\"):\n",
    "        a = softmax(sim)\n",
    "        rank_u = len(u_aug.get_shape().as_list())\n",
    "        u_a = tf.reduce_sum(tf.expand_dims(a, -1) * u_aug, rank_u-2)\n",
    "\n",
    "    with tf.variable_scope(\"query_2_context\"):\n",
    "        b = softmax(tf.reduce_max(sim, 3))\n",
    "        rank_h = len(h.get_shape().as_list())\n",
    "        h_a = tf.reduce_sum(tf.expand_dims(b, -1) * h, rank_h-2)\n",
    "        h_a = tf.tile(tf.expand_dims(h_a, 2), [1, 1, JX, 1])\n",
    "    \n",
    "    with tf.variable_scope(\"final\"):\n",
    "        g = tf.concat([h, u_a, h * u_a, h * h_a], 3)\n",
    "            \n",
    "\n",
    "with tf.variable_scope(\"modeling_layer\"):\n",
    "    flat_g = flatten(g, 2)  \n",
    "    cell1 = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "    (flat_fw_g0_outputs, flat_bw_g0_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell1, cell1, flat_g, sequence_length=flat_len_x, dtype='float', scope='g0')\n",
    "    fw_g0 = reconstruct(flat_fw_g0_outputs, g, 2)\n",
    "    bw_g0 = reconstruct(flat_bw_g0_outputs, g, 2)\n",
    "\n",
    "    g0 = tf.concat([fw_g0, bw_g0], 3)\n",
    "\n",
    "    flat_g0 = flatten(g0, 2)\n",
    "    cell2 = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "\n",
    "    (flat_fw_g1_outputs, flat_bw_g1_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell2, cell2, flat_g0, sequence_length=flat_len_x, dtype='float', scope='g1')\n",
    "    fw_g1 = reconstruct(flat_fw_g1_outputs, g0, 2)\n",
    "    bw_g1 = reconstruct(flat_bw_g1_outputs, g0, 2)\n",
    "\n",
    "    g1 = tf.concat([fw_g1, bw_g1], 3)\n",
    "\n",
    "with tf.variable_scope(\"output_layer\"):\n",
    "    logits1 = linear([tf.concat([g1, g], -1)], 1, input_keep_prob=config.input_keep_prob, is_train=is_train, scope=\"logits1\")\n",
    "    logits1 = tf.squeeze(logits1, [len(logits1.get_shape().as_list())-1])\n",
    "    logits1 = exp_mask(logits1, x_mask)\n",
    "    \n",
    "    a = softmax(tf.reshape(logits1, [N, M * JX]))\n",
    "    g1_reshaped = tf.reshape(g1, [N, M * JX, 2 * d])\n",
    "    rank_g1 = len(g1_reshaped.get_shape().as_list())\n",
    "    a1i = tf.reduce_sum(tf.expand_dims(a, -1) * g1_reshaped, rank_g1-2)\n",
    "    a1i = tf.tile(tf.expand_dims(tf.expand_dims(a1i, 1), 1), [1, M, JX, 1])\n",
    "    \n",
    "    g2_input = tf.concat([g, g1, a1i, g1 * a1i], 3)\n",
    "    flat_input = flatten(g2_input, 2)  \n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(d,state_is_tuple=True);\n",
    "    (flat_fw_g2_outputs, flat_bw_g2_outputs), _ =tf.nn.bidirectional_dynamic_rnn(cell, cell, flat_input, sequence_length=flat_len_x, dtype='float', scope='g2')\n",
    "    fw_g2 = reconstruct(flat_fw_g2_outputs, g, 2)\n",
    "    bw_g2 = reconstruct(flat_bw_g2_outputs, g, 2)\n",
    "\n",
    "    g2 = tf.concat([fw_g2, bw_g2], 3)\n",
    "    \n",
    "    logits2 = linear([tf.concat([g2, g], -1)], 1, input_keep_prob=config.input_keep_prob, is_train=is_train, scope=\"logits2\")\n",
    "    logits2 = tf.squeeze(logits2, [len(logits2.get_shape().as_list())-1])\n",
    "    logits2 = exp_mask(logits2, x_mask)\n",
    "    \n",
    "    logits1 = tf.reshape(logits1, [-1, M * JX])\n",
    "    flat_yp1 = tf.nn.softmax(logits1) \n",
    "    yp1 = tf.reshape(flat_yp1, [-1, M, JX])\n",
    "    logits2 = tf.reshape(logits2, [-1, M * JX])\n",
    "    flat_yp2 = tf.nn.softmax(logits2)\n",
    "    yp2 = tf.reshape(flat_yp2, [-1, M, JX])\n",
    "    \n",
    "    tensor_dict['g1'] = g1\n",
    "    tensor_dict['g2'] = g2\n",
    "    \n",
    "\n",
    "#Loss \n",
    "loss_mask = tf.reduce_max(tf.cast(q_mask, 'float'), 1)\n",
    "losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits1, labels=tf.cast(tf.reshape(y1, [-1, M * JX]), 'float'))\n",
    "ce_loss1 = tf.reduce_mean(loss_mask * losses)\n",
    "ce_loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=tf.cast(tf.reshape(y2, [-1, M * JX]), 'float')))\n",
    "tf.add_to_collection('losses', ce_loss1)\n",
    "tf.add_to_collection(\"losses\", ce_loss2)\n",
    "\n",
    "loss = tf.add_n(tf.get_collection('losses'), name='loss')\n",
    "tf.summary.scalar(loss.op.name, loss)\n",
    "tf.add_to_collection('ema/scalar', loss)\n",
    "\n",
    "variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "for var in variables:\n",
    "    tensor_dict[var.name] = var\n",
    "\n",
    "var_ema = tf.train.ExponentialMovingAverage(config.var_decay)\n",
    "ema_op = var_ema.apply(tf.trainable_variables())\n",
    "\n",
    "if config.mode == 'train':\n",
    "    ema = tf.train.ExponentialMovingAverage(config.decay)\n",
    "    ema_op = ema.apply(tf.get_collection(\"ema/scalar\"))\n",
    "\n",
    "    for var in tf.get_collection(\"ema/scalar\"):\n",
    "        ema_var = ema.average(var)\n",
    "        tf.summary.scalar(ema_var.op.name, ema_var)\n",
    "    for var in tf.get_collection(\"ema/vector\"):\n",
    "        ema_var = ema.average(var)\n",
    "        tf.summary.histogram(ema_var.op.name, ema_var)\n",
    "    \n",
    "with tf.control_dependencies([ema_op]):\n",
    "    loss = tf.identity(loss)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "summary = tf.summary.merge(tf.get_collection(\"summaries\"))\n",
    "\n",
    "optimizer = tf.contrib.opt.NadamOptimizer()\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = os.path.join(config.save_dir, config.model_name)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer = tf.train.SummaryWriter(config.log_dir, graph=tf.get_default_graph())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = tf.train.latest_checkpoint(config.save_dir)\n",
    "# saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_steps = config.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = train_data.get_batches(batch_size, num_batches=num_steps, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_batches = (tuple(zip(grouper(idxs, batch_size, shorten=True, num_groups=1),\n",
    "                         data_set.divide(1))) for idxs, data_set in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataSet.get_batches at 0x7fbe882bda40>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7fbe884fddb0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  9.28it/s]\u001b[A\n",
      "362it [00:00, 1742.03it/s]\u001b[A\n",
      "725it [00:00, 2354.65it/s]\u001b[A\n",
      "1138it [00:00, 2788.68it/s]\u001b[A\n",
      "1499it [00:00, 2950.02it/s]\u001b[A\n",
      "1961it [00:00, 3223.58it/s]\u001b[A\n",
      "2400it [00:00, 3389.09it/s]\u001b[A\n",
      "2771it [00:00, 3427.38it/s]\u001b[A\n",
      "3160it [00:00, 3478.66it/s]\u001b[A\n",
      "3535it [00:01, 3500.11it/s]\u001b[A\n",
      "3935it [00:01, 3547.58it/s]\u001b[A\n",
      "4337it [00:01, 3587.35it/s]\u001b[A\n",
      "4725it [00:01, 3548.30it/s]\u001b[A\n",
      "5092it [00:01, 3540.21it/s]\u001b[A\n",
      "5452it [00:01, 3474.87it/s]\u001b[A\n",
      "5787it [00:01, 3445.63it/s]\u001b[A\n",
      "6113it [00:01, 3416.38it/s]\u001b[A\n",
      "6546it [00:01, 3464.45it/s]\u001b[A\n",
      "6898it [00:01, 3459.66it/s]\u001b[A\n",
      "7338it [00:02, 3504.19it/s]\u001b[A\n",
      "7784it [00:02, 3547.54it/s]\u001b[A\n",
      "8180it [00:02, 3548.01it/s]\u001b[A\n",
      "8628it [00:02, 3586.67it/s]\u001b[A\n",
      "9031it [00:02, 3575.17it/s]\u001b[A\n",
      "9461it [00:02, 3602.75it/s]\u001b[A\n",
      "9863it [00:02, 3617.43it/s]\u001b[A\n",
      "10261it [00:02, 3620.20it/s]\u001b[A\n",
      "10651it [00:02, 3628.61it/s]\u001b[A\n",
      "11040it [00:03, 3625.26it/s]\u001b[A\n",
      "11497it [00:03, 3655.21it/s]\u001b[A\n",
      "11899it [00:03, 3655.05it/s]\u001b[A\n",
      "12290it [00:03, 3636.70it/s]\u001b[A\n",
      "12659it [00:03, 3589.73it/s]\u001b[A\n",
      "12993it [00:03, 3571.97it/s]\u001b[A\n",
      "13317it [00:03, 3560.43it/s]\u001b[A\n",
      "13704it [00:03, 3568.53it/s]\u001b[A\n",
      "14111it [00:03, 3581.14it/s]\u001b[A\n",
      "14490it [00:04, 3586.23it/s]\u001b[A\n",
      "14856it [00:04, 3582.98it/s]\u001b[A\n",
      "15221it [00:04, 3584.22it/s]\u001b[A\n",
      "15674it [00:04, 3606.01it/s]\u001b[A\n",
      "16096it [00:04, 3619.60it/s]\u001b[A\n",
      "16495it [00:04, 3622.70it/s]\u001b[A\n",
      "16920it [00:04, 3635.97it/s]\u001b[A\n",
      "17322it [00:04, 3635.94it/s]\u001b[A\n",
      "17712it [00:04, 3638.09it/s]\u001b[A\n",
      "18099it [00:04, 3625.29it/s]\u001b[A\n",
      "18463it [00:05, 3611.68it/s]\u001b[A\n",
      "18831it [00:05, 3612.96it/s]\u001b[A\n",
      "19184it [00:05, 3610.68it/s]\u001b[A\n",
      "19588it [00:05, 3618.58it/s]\u001b[A\n",
      "19977it [00:05, 3622.42it/s]\u001b[A\n",
      "20000it [00:05, 3617.33it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(multi_batches):\n",
    "    bttt = batch\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = bttt[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-e14061eba53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-ef25bb681a25>\u001b[0m in \u001b[0;36mget_feed_dict\u001b[0;34m(batch, is_train, supervised)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mq_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJQ\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "get_feed_dict(ds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.len_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx = np.zeros([N, M, JX], dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, is_train_cond):\n",
    "        assert isinstance(batch, DataSet)\n",
    "        feed_dict = {}\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        temp_x = np.zeros([N, M, JX], dtype='int32')\n",
    "        temp_cx = np.zeros([N, M, JX, W], dtype='int32')\n",
    "        temp_mask = np.zeros([N, M, JX], dtype='bool')\n",
    "        temp_p.zeros([N, JQ], dtype='int32')\n",
    "        temp_np.zeros([N, JQ, W], dtype='int32')\n",
    "        temp_k = np.zeros([N, JQ], dtype='bool')\n",
    "\n",
    "        feed_dict[x] = temp_\n",
    "        feed_dict[x_mask] = temp_k\n",
    "        feed_dict[cx] = temp_cx\n",
    "        feed_dict[q] = temp_q\n",
    "        feed_dict[cq] = temp_cq\n",
    "        feed_dict[q_mask] = temp_q_mask\n",
    "        feed_dict[is_train] = is_train_cond\n",
    "        if config.use_glove_for_unk:\n",
    "            feed_dict[new_emb_mat] = batch.shared['new_emb_mat']\n",
    "\n",
    "        X = batch.data['x']\n",
    "        CX = batch.data['cx']\n",
    "\n",
    "        y = np.zeros([N, M, JX], dtype='bool')\n",
    "        y2 = np.zeros([N, M, JX], dtype='bool')\n",
    "        feed_dict[y] = y\n",
    "        feed_dict[y2] = y2\n",
    "\n",
    "        for i, (xi, cxi, yi) in enumerate(zip(X, CX, batch.data['y'])):\n",
    "            start_idx, stop_idx = random.choice(yi)\n",
    "            j, k = start_idx\n",
    "            j2, k2 = stop_idx\n",
    "            if config.single:\n",
    "                X[i] = [xi[j]]\n",
    "                CX[i] = [cxi[j]]\n",
    "                j, j2 = 0, 0\n",
    "            if config.squash:\n",
    "                offset = sum(map(len, xi[:j]))\n",
    "                j, k = 0, k + offset\n",
    "                offset = sum(map(len, xi[:j2]))\n",
    "                j2, k2 = 0, k2 + offset\n",
    "            y[i, j, k] = True\n",
    "            y2[i, j2, k2-1] = True\n",
    "\n",
    "        def _get_word(word):\n",
    "            d = batch.shared['word2idx']\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in d:\n",
    "                    return d[each]\n",
    "            if config.use_glove_for_unk:\n",
    "                d2 = batch.shared['new_word2idx']\n",
    "                for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                    if each in d2:\n",
    "                        return d2[each] + len(d)\n",
    "            return 1\n",
    "\n",
    "        def _get_char(char):\n",
    "            d = batch.shared['char2idx']\n",
    "            if char in d:\n",
    "                return d[char]\n",
    "            return 1\n",
    "\n",
    "        for i, xi in enumerate(X):\n",
    "            if config.squash:\n",
    "                xi = [list(itertools.chain(*xi))]\n",
    "            for j, xij in enumerate(xi):\n",
    "                if j == config.max_num_sents:\n",
    "                    break\n",
    "                for k, xijk in enumerate(xij):\n",
    "                    if k == config.max_sent_size:\n",
    "                        break\n",
    "                    each = _get_word(xijk)\n",
    "                    assert isinstance(each, int), each\n",
    "                    x[i, j, k] = each\n",
    "                    x_mask[i, j, k] = True\n",
    "\n",
    "        for i, cxi in enumerate(CX):\n",
    "            if config.squash:\n",
    "                cxi = [list(itertools.chain(*cxi))]\n",
    "            for j, cxij in enumerate(cxi):\n",
    "                if j == config.max_num_sents:\n",
    "                    break\n",
    "                for k, cxijk in enumerate(cxij):\n",
    "                    if k == config.max_sent_size:\n",
    "                        break\n",
    "                    for l, cxijkl in enumerate(cxijk):\n",
    "                        if l == config.max_word_size:\n",
    "                            break\n",
    "                        cx[i, j, k, l] = _get_char(cxijkl)\n",
    "\n",
    "        for i, qi in enumerate(batch.data['q']):\n",
    "            for j, qij in enumerate(qi):\n",
    "                q[i, j] = _get_word(qij)\n",
    "                q_mask[i, j] = True\n",
    "\n",
    "        for i, cqi in enumerate(batch.data['cq']):\n",
    "            for j, cqij in enumerate(cqi):\n",
    "                for k, cqijk in enumerate(cqij):\n",
    "                    cq[i, j, k] = _get_char(cqijk)\n",
    "                    if k + 1 == config.max_word_size:\n",
    "                        break\n",
    "\n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in bt:\n",
    "    b = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0it [38:59, ?it/s]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
