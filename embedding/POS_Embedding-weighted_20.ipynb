{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random\n",
    "\n",
    "with open('data/pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "weights = [1 - 4*count/total_count for word, count in word_counts.items()]\n",
    "weights = 2*np.multiply(weights, weights)\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.08, 0.1)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 5\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  20\n",
    "starter_learning_rate = 0.0001\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.96\n",
    "lambda_l2 = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:3'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "        labels = tf.placeholder(tf.int32, [None], name='labels')\n",
    "\n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        embed_drop = tf.nn.dropout(embed, 0.8)\n",
    "\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_w\") # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "\n",
    "        logits = tf.matmul(embed_drop, tf.transpose(softmax_w)) + softmax_b\n",
    "        \n",
    "        class_weight = tf.constant(weights)\n",
    "        weighted_logits = logits * class_weight\n",
    "\n",
    "\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "        \n",
    "        tv_all = tf.trainable_variables()\n",
    "\n",
    "        l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_all ])\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=weighted_logits)\n",
    "        \n",
    "        cost = tf.reduce_mean(loss) + l2_loss\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps, decay_rate)\n",
    "\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'pos_weighted_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/pos_weighted_20’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos_weighted_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Threshold: 0.08644539633845551 Length of Training words: 2425040\n",
      "Global Step: 100 Epoch 1/10 Iteration: 100 Avg. Training loss: 30.1618 0.0315 sec/batch\n",
      "Global Step: 200 Epoch 1/10 Iteration: 200 Avg. Training loss: 29.4988 0.0448 sec/batch\n",
      "Global Step: 300 Epoch 1/10 Iteration: 300 Avg. Training loss: 28.8032 0.0064 sec/batch\n",
      "Global Step: 400 Epoch 1/10 Iteration: 400 Avg. Training loss: 28.1691 0.0143 sec/batch\n",
      "Global Step: 500 Epoch 1/10 Iteration: 500 Avg. Training loss: 27.5454 0.0053 sec/batch\n",
      "Global Step: 600 Epoch 1/10 Iteration: 600 Avg. Training loss: 26.9275 0.0095 sec/batch\n",
      "Global Step: 700 Epoch 1/10 Iteration: 700 Avg. Training loss: 26.3362 0.0076 sec/batch\n",
      "Global Step: 800 Epoch 1/10 Iteration: 800 Avg. Training loss: 25.7386 0.0081 sec/batch\n",
      "Global Step: 900 Epoch 1/10 Iteration: 900 Avg. Training loss: 25.1564 0.0052 sec/batch\n",
      "Global Step: 1000 Epoch 1/10 Iteration: 1000 Avg. Training loss: 24.6026 0.0082 sec/batch\n",
      "Global Step: 1100 Epoch 1/10 Iteration: 1100 Avg. Training loss: 24.0966 0.0073 sec/batch\n",
      "Global Step: 1200 Epoch 1/10 Iteration: 1200 Avg. Training loss: 23.5957 0.0054 sec/batch\n",
      "Global Step: 1300 Epoch 1/10 Iteration: 1300 Avg. Training loss: 23.0861 0.0088 sec/batch\n",
      "Global Step: 1400 Epoch 1/10 Iteration: 1400 Avg. Training loss: 22.6186 0.0052 sec/batch\n",
      "Global Step: 1500 Epoch 1/10 Iteration: 1500 Avg. Training loss: 22.1272 0.0069 sec/batch\n",
      "Global Step: 1600 Epoch 1/10 Iteration: 1600 Avg. Training loss: 21.6789 0.0056 sec/batch\n",
      "Global Step: 1700 Epoch 1/10 Iteration: 1700 Avg. Training loss: 21.2146 0.0055 sec/batch\n",
      "Global Step: 1800 Epoch 1/10 Iteration: 1800 Avg. Training loss: 20.7970 0.0066 sec/batch\n",
      "Global Step: 1900 Epoch 1/10 Iteration: 1900 Avg. Training loss: 20.3648 0.0082 sec/batch\n",
      "Global Step: 2000 Epoch 1/10 Iteration: 2000 Avg. Training loss: 19.9372 0.0063 sec/batch\n",
      "Global Step: 2100 Epoch 1/10 Iteration: 2100 Avg. Training loss: 19.5289 0.0063 sec/batch\n",
      "Global Step: 2200 Epoch 1/10 Iteration: 2200 Avg. Training loss: 19.1182 0.0086 sec/batch\n",
      "Global Step: 2300 Epoch 1/10 Iteration: 2300 Avg. Training loss: 18.7453 0.0092 sec/batch\n",
      "Global Step: 2400 Epoch 1/10 Iteration: 2400 Avg. Training loss: 18.3571 0.0070 sec/batch\n",
      "Epoch 2/10 Threshold: 0.08012043280996421 Length of Training words: 2385615\n",
      "Global Step: 2500 Epoch 2/10 Iteration: 2500 Avg. Training loss: 18.0179 0.0062 sec/batch\n",
      "Global Step: 2600 Epoch 2/10 Iteration: 2600 Avg. Training loss: 17.6768 0.0071 sec/batch\n",
      "Global Step: 2700 Epoch 2/10 Iteration: 2700 Avg. Training loss: 17.3102 0.0067 sec/batch\n",
      "Global Step: 2800 Epoch 2/10 Iteration: 2800 Avg. Training loss: 16.9686 0.0069 sec/batch\n",
      "Global Step: 2900 Epoch 2/10 Iteration: 2900 Avg. Training loss: 16.6247 0.0069 sec/batch\n",
      "Global Step: 3000 Epoch 2/10 Iteration: 3000 Avg. Training loss: 16.3078 0.0058 sec/batch\n",
      "Global Step: 3100 Epoch 2/10 Iteration: 3100 Avg. Training loss: 16.0017 0.0072 sec/batch\n",
      "Global Step: 3200 Epoch 2/10 Iteration: 3200 Avg. Training loss: 15.6809 0.0053 sec/batch\n",
      "Global Step: 3300 Epoch 2/10 Iteration: 3300 Avg. Training loss: 15.3667 0.0060 sec/batch\n",
      "Global Step: 3400 Epoch 2/10 Iteration: 3400 Avg. Training loss: 15.0760 0.0058 sec/batch\n",
      "Global Step: 3500 Epoch 2/10 Iteration: 3500 Avg. Training loss: 14.7994 0.0066 sec/batch\n",
      "Global Step: 3600 Epoch 2/10 Iteration: 3600 Avg. Training loss: 14.5364 0.0052 sec/batch\n",
      "Global Step: 3700 Epoch 2/10 Iteration: 3700 Avg. Training loss: 14.2613 0.0071 sec/batch\n",
      "Global Step: 3800 Epoch 2/10 Iteration: 3800 Avg. Training loss: 14.0039 0.0080 sec/batch\n",
      "Global Step: 3900 Epoch 2/10 Iteration: 3900 Avg. Training loss: 13.7350 0.0062 sec/batch\n",
      "Global Step: 4000 Epoch 2/10 Iteration: 4000 Avg. Training loss: 13.4941 0.0055 sec/batch\n",
      "Global Step: 4100 Epoch 2/10 Iteration: 4100 Avg. Training loss: 13.2397 0.0056 sec/batch\n",
      "Global Step: 4200 Epoch 2/10 Iteration: 4200 Avg. Training loss: 13.0170 0.0055 sec/batch\n",
      "Global Step: 4300 Epoch 2/10 Iteration: 4300 Avg. Training loss: 12.7755 0.0060 sec/batch\n",
      "Global Step: 4400 Epoch 2/10 Iteration: 4400 Avg. Training loss: 12.5447 0.0060 sec/batch\n",
      "Global Step: 4500 Epoch 2/10 Iteration: 4500 Avg. Training loss: 12.3203 0.0073 sec/batch\n",
      "Global Step: 4600 Epoch 2/10 Iteration: 4600 Avg. Training loss: 12.0899 0.0062 sec/batch\n",
      "Global Step: 4700 Epoch 2/10 Iteration: 4700 Avg. Training loss: 11.8917 0.0084 sec/batch\n",
      "Global Step: 4800 Epoch 2/10 Iteration: 4800 Avg. Training loss: 11.6817 0.0062 sec/batch\n",
      "Epoch 3/10 Threshold: 0.0928698670093929 Length of Training words: 2463665\n",
      "Global Step: 4900 Epoch 3/10 Iteration: 4900 Avg. Training loss: 11.4973 0.0059 sec/batch\n",
      "Global Step: 5000 Epoch 3/10 Iteration: 5000 Avg. Training loss: 11.3075 0.0060 sec/batch\n",
      "Global Step: 5100 Epoch 3/10 Iteration: 5100 Avg. Training loss: 11.1122 0.0059 sec/batch\n",
      "Global Step: 5200 Epoch 3/10 Iteration: 5200 Avg. Training loss: 10.9139 0.0062 sec/batch\n",
      "Global Step: 5300 Epoch 3/10 Iteration: 5300 Avg. Training loss: 10.7252 0.0072 sec/batch\n",
      "Global Step: 5400 Epoch 3/10 Iteration: 5400 Avg. Training loss: 10.5481 0.0074 sec/batch\n",
      "Global Step: 5500 Epoch 3/10 Iteration: 5500 Avg. Training loss: 10.3849 0.0059 sec/batch\n",
      "Global Step: 5600 Epoch 3/10 Iteration: 5600 Avg. Training loss: 10.2089 0.0055 sec/batch\n",
      "Global Step: 5700 Epoch 3/10 Iteration: 5700 Avg. Training loss: 10.0313 0.0064 sec/batch\n",
      "Global Step: 5800 Epoch 3/10 Iteration: 5800 Avg. Training loss: 9.8746 0.0083 sec/batch\n",
      "Global Step: 5900 Epoch 3/10 Iteration: 5900 Avg. Training loss: 9.7156 0.0064 sec/batch\n",
      "Global Step: 6000 Epoch 3/10 Iteration: 6000 Avg. Training loss: 9.5656 0.0057 sec/batch\n",
      "Global Step: 6100 Epoch 3/10 Iteration: 6100 Avg. Training loss: 9.4250 0.0059 sec/batch\n",
      "Global Step: 6200 Epoch 3/10 Iteration: 6200 Avg. Training loss: 9.2649 0.0062 sec/batch\n",
      "Global Step: 6300 Epoch 3/10 Iteration: 6300 Avg. Training loss: 9.1356 0.0080 sec/batch\n",
      "Global Step: 6400 Epoch 3/10 Iteration: 6400 Avg. Training loss: 8.9847 0.0069 sec/batch\n",
      "Global Step: 6500 Epoch 3/10 Iteration: 6500 Avg. Training loss: 8.8413 0.0076 sec/batch\n",
      "Global Step: 6600 Epoch 3/10 Iteration: 6600 Avg. Training loss: 8.7314 0.0148 sec/batch\n",
      "Global Step: 6700 Epoch 3/10 Iteration: 6700 Avg. Training loss: 8.5937 0.0068 sec/batch\n",
      "Global Step: 6800 Epoch 3/10 Iteration: 6800 Avg. Training loss: 8.4676 0.0066 sec/batch\n",
      "Global Step: 6900 Epoch 3/10 Iteration: 6900 Avg. Training loss: 8.3403 0.0074 sec/batch\n",
      "Global Step: 7000 Epoch 3/10 Iteration: 7000 Avg. Training loss: 8.2102 0.0098 sec/batch\n",
      "Global Step: 7100 Epoch 3/10 Iteration: 7100 Avg. Training loss: 8.0901 0.0068 sec/batch\n",
      "Global Step: 7200 Epoch 3/10 Iteration: 7200 Avg. Training loss: 7.9733 0.0055 sec/batch\n",
      "Epoch 4/10 Threshold: 0.09033921578182288 Length of Training words: 2448934\n",
      "Global Step: 7300 Epoch 4/10 Iteration: 7300 Avg. Training loss: 7.8755 0.0046 sec/batch\n",
      "Global Step: 7400 Epoch 4/10 Iteration: 7400 Avg. Training loss: 7.7737 0.0082 sec/batch\n",
      "Global Step: 7500 Epoch 4/10 Iteration: 7500 Avg. Training loss: 7.6731 0.0072 sec/batch\n",
      "Global Step: 7600 Epoch 4/10 Iteration: 7600 Avg. Training loss: 7.5562 0.0075 sec/batch\n",
      "Global Step: 7700 Epoch 4/10 Iteration: 7700 Avg. Training loss: 7.4501 0.0077 sec/batch\n",
      "Global Step: 7800 Epoch 4/10 Iteration: 7800 Avg. Training loss: 7.3463 0.0058 sec/batch\n",
      "Global Step: 7900 Epoch 4/10 Iteration: 7900 Avg. Training loss: 7.2501 0.0063 sec/batch\n",
      "Global Step: 8000 Epoch 4/10 Iteration: 8000 Avg. Training loss: 7.1586 0.0069 sec/batch\n",
      "Global Step: 8100 Epoch 4/10 Iteration: 8100 Avg. Training loss: 7.0672 0.0079 sec/batch\n",
      "Global Step: 8200 Epoch 4/10 Iteration: 8200 Avg. Training loss: 6.9759 0.0084 sec/batch\n",
      "Global Step: 8300 Epoch 4/10 Iteration: 8300 Avg. Training loss: 6.8763 0.0086 sec/batch\n",
      "Global Step: 8400 Epoch 4/10 Iteration: 8400 Avg. Training loss: 6.7938 0.0060 sec/batch\n",
      "Global Step: 8500 Epoch 4/10 Iteration: 8500 Avg. Training loss: 6.7177 0.0053 sec/batch\n",
      "Global Step: 8600 Epoch 4/10 Iteration: 8600 Avg. Training loss: 6.6274 0.0086 sec/batch\n",
      "Global Step: 8700 Epoch 4/10 Iteration: 8700 Avg. Training loss: 6.5523 0.0059 sec/batch\n",
      "Global Step: 8800 Epoch 4/10 Iteration: 8800 Avg. Training loss: 6.4690 0.0066 sec/batch\n",
      "Global Step: 8900 Epoch 4/10 Iteration: 8900 Avg. Training loss: 6.3983 0.0053 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9000 Epoch 4/10 Iteration: 9000 Avg. Training loss: 6.3155 0.0149 sec/batch\n",
      "Global Step: 9100 Epoch 4/10 Iteration: 9100 Avg. Training loss: 6.2577 0.0067 sec/batch\n",
      "Global Step: 9200 Epoch 4/10 Iteration: 9200 Avg. Training loss: 6.1821 0.0067 sec/batch\n",
      "Global Step: 9300 Epoch 4/10 Iteration: 9300 Avg. Training loss: 6.1144 0.0105 sec/batch\n",
      "Global Step: 9400 Epoch 4/10 Iteration: 9400 Avg. Training loss: 6.0337 0.0083 sec/batch\n",
      "Global Step: 9500 Epoch 4/10 Iteration: 9500 Avg. Training loss: 5.9620 0.0072 sec/batch\n",
      "Global Step: 9600 Epoch 4/10 Iteration: 9600 Avg. Training loss: 5.9120 0.0061 sec/batch\n",
      "Global Step: 9700 Epoch 4/10 Iteration: 9700 Avg. Training loss: 5.8410 0.0067 sec/batch\n",
      "Epoch 5/10 Threshold: 0.08843282539255745 Length of Training words: 2437566\n",
      "Global Step: 9800 Epoch 5/10 Iteration: 9800 Avg. Training loss: 5.7945 0.0066 sec/batch\n",
      "Global Step: 9900 Epoch 5/10 Iteration: 9900 Avg. Training loss: 5.7403 0.0070 sec/batch\n",
      "Global Step: 10000 Epoch 5/10 Iteration: 10000 Avg. Training loss: 5.6795 0.0078 sec/batch\n",
      "Global Step: 10100 Epoch 5/10 Iteration: 10100 Avg. Training loss: 5.6124 0.0108 sec/batch\n",
      "Global Step: 10200 Epoch 5/10 Iteration: 10200 Avg. Training loss: 5.5547 0.0071 sec/batch\n",
      "Global Step: 10300 Epoch 5/10 Iteration: 10300 Avg. Training loss: 5.4958 0.0060 sec/batch\n",
      "Global Step: 10400 Epoch 5/10 Iteration: 10400 Avg. Training loss: 5.4561 0.0069 sec/batch\n",
      "Global Step: 10500 Epoch 5/10 Iteration: 10500 Avg. Training loss: 5.3989 0.0057 sec/batch\n",
      "Global Step: 10600 Epoch 5/10 Iteration: 10600 Avg. Training loss: 5.3462 0.0080 sec/batch\n",
      "Global Step: 10700 Epoch 5/10 Iteration: 10700 Avg. Training loss: 5.3012 0.0086 sec/batch\n",
      "Global Step: 10800 Epoch 5/10 Iteration: 10800 Avg. Training loss: 5.2454 0.0076 sec/batch\n",
      "Global Step: 10900 Epoch 5/10 Iteration: 10900 Avg. Training loss: 5.1992 0.0065 sec/batch\n",
      "Global Step: 11000 Epoch 5/10 Iteration: 11000 Avg. Training loss: 5.1622 0.0073 sec/batch\n",
      "Global Step: 11100 Epoch 5/10 Iteration: 11100 Avg. Training loss: 5.1080 0.0070 sec/batch\n",
      "Global Step: 11200 Epoch 5/10 Iteration: 11200 Avg. Training loss: 5.0755 0.0057 sec/batch\n",
      "Global Step: 11300 Epoch 5/10 Iteration: 11300 Avg. Training loss: 5.0254 0.0079 sec/batch\n",
      "Global Step: 11400 Epoch 5/10 Iteration: 11400 Avg. Training loss: 4.9770 0.0099 sec/batch\n",
      "Global Step: 11500 Epoch 5/10 Iteration: 11500 Avg. Training loss: 4.9571 0.0064 sec/batch\n",
      "Global Step: 11600 Epoch 5/10 Iteration: 11600 Avg. Training loss: 4.9094 0.0056 sec/batch\n",
      "Global Step: 11700 Epoch 5/10 Iteration: 11700 Avg. Training loss: 4.8766 0.0064 sec/batch\n",
      "Global Step: 11800 Epoch 5/10 Iteration: 11800 Avg. Training loss: 4.8303 0.0061 sec/batch\n",
      "Global Step: 11900 Epoch 5/10 Iteration: 11900 Avg. Training loss: 4.7901 0.0053 sec/batch\n",
      "Global Step: 12000 Epoch 5/10 Iteration: 12000 Avg. Training loss: 4.7541 0.0059 sec/batch\n",
      "Global Step: 12100 Epoch 5/10 Iteration: 12100 Avg. Training loss: 4.7162 0.0074 sec/batch\n",
      "Epoch 6/10 Threshold: 0.09995303675252906 Length of Training words: 2497927\n",
      "Global Step: 12200 Epoch 6/10 Iteration: 12200 Avg. Training loss: 4.6936 0.0039 sec/batch\n",
      "Global Step: 12300 Epoch 6/10 Iteration: 12300 Avg. Training loss: 4.6610 0.0066 sec/batch\n",
      "Global Step: 12400 Epoch 6/10 Iteration: 12400 Avg. Training loss: 4.6352 0.0062 sec/batch\n",
      "Global Step: 12500 Epoch 6/10 Iteration: 12500 Avg. Training loss: 4.5988 0.0063 sec/batch\n",
      "Global Step: 12600 Epoch 6/10 Iteration: 12600 Avg. Training loss: 4.5583 0.0060 sec/batch\n",
      "Global Step: 12700 Epoch 6/10 Iteration: 12700 Avg. Training loss: 4.5253 0.0060 sec/batch\n",
      "Global Step: 12800 Epoch 6/10 Iteration: 12800 Avg. Training loss: 4.4992 0.0078 sec/batch\n",
      "Global Step: 12900 Epoch 6/10 Iteration: 12900 Avg. Training loss: 4.4732 0.0066 sec/batch\n",
      "Global Step: 13000 Epoch 6/10 Iteration: 13000 Avg. Training loss: 4.4480 0.0066 sec/batch\n",
      "Global Step: 13100 Epoch 6/10 Iteration: 13100 Avg. Training loss: 4.4196 0.0070 sec/batch\n",
      "Global Step: 13200 Epoch 6/10 Iteration: 13200 Avg. Training loss: 4.3853 0.0056 sec/batch\n",
      "Global Step: 13300 Epoch 6/10 Iteration: 13300 Avg. Training loss: 4.3615 0.0080 sec/batch\n",
      "Global Step: 13400 Epoch 6/10 Iteration: 13400 Avg. Training loss: 4.3390 0.0068 sec/batch\n",
      "Global Step: 13500 Epoch 6/10 Iteration: 13500 Avg. Training loss: 4.3091 0.0083 sec/batch\n",
      "Global Step: 13600 Epoch 6/10 Iteration: 13600 Avg. Training loss: 4.2941 0.0061 sec/batch\n",
      "Global Step: 13700 Epoch 6/10 Iteration: 13700 Avg. Training loss: 4.2669 0.0067 sec/batch\n",
      "Global Step: 13800 Epoch 6/10 Iteration: 13800 Avg. Training loss: 4.2433 0.0064 sec/batch\n",
      "Global Step: 13900 Epoch 6/10 Iteration: 13900 Avg. Training loss: 4.2144 0.0071 sec/batch\n",
      "Global Step: 14000 Epoch 6/10 Iteration: 14000 Avg. Training loss: 4.2048 0.0109 sec/batch\n",
      "Global Step: 14100 Epoch 6/10 Iteration: 14100 Avg. Training loss: 4.1827 0.0074 sec/batch\n",
      "Global Step: 14200 Epoch 6/10 Iteration: 14200 Avg. Training loss: 4.1660 0.0066 sec/batch\n",
      "Global Step: 14300 Epoch 6/10 Iteration: 14300 Avg. Training loss: 4.1312 0.0054 sec/batch\n",
      "Global Step: 14400 Epoch 6/10 Iteration: 14400 Avg. Training loss: 4.1070 0.0057 sec/batch\n",
      "Global Step: 14500 Epoch 6/10 Iteration: 14500 Avg. Training loss: 4.0939 0.0075 sec/batch\n",
      "Global Step: 14600 Epoch 6/10 Iteration: 14600 Avg. Training loss: 4.0702 0.0088 sec/batch\n",
      "Epoch 7/10 Threshold: 0.09575346689023312 Length of Training words: 2478392\n",
      "Global Step: 14700 Epoch 7/10 Iteration: 14700 Avg. Training loss: 4.0615 0.0023 sec/batch\n",
      "Global Step: 14800 Epoch 7/10 Iteration: 14800 Avg. Training loss: 4.0501 0.0063 sec/batch\n",
      "Global Step: 14900 Epoch 7/10 Iteration: 14900 Avg. Training loss: 4.0339 0.0070 sec/batch\n",
      "Global Step: 15000 Epoch 7/10 Iteration: 15000 Avg. Training loss: 4.0132 0.0092 sec/batch\n",
      "Global Step: 15100 Epoch 7/10 Iteration: 15100 Avg. Training loss: 3.9873 0.0091 sec/batch\n",
      "Global Step: 15200 Epoch 7/10 Iteration: 15200 Avg. Training loss: 3.9687 0.0066 sec/batch\n",
      "Global Step: 15300 Epoch 7/10 Iteration: 15300 Avg. Training loss: 3.9537 0.0084 sec/batch\n",
      "Global Step: 15400 Epoch 7/10 Iteration: 15400 Avg. Training loss: 3.9408 0.0056 sec/batch\n",
      "Global Step: 15500 Epoch 7/10 Iteration: 15500 Avg. Training loss: 3.9242 0.0055 sec/batch\n",
      "Global Step: 15600 Epoch 7/10 Iteration: 15600 Avg. Training loss: 3.9143 0.0068 sec/batch\n",
      "Global Step: 15700 Epoch 7/10 Iteration: 15700 Avg. Training loss: 3.8875 0.0068 sec/batch\n",
      "Global Step: 15800 Epoch 7/10 Iteration: 15800 Avg. Training loss: 3.8749 0.0092 sec/batch\n",
      "Global Step: 15900 Epoch 7/10 Iteration: 15900 Avg. Training loss: 3.8686 0.0080 sec/batch\n",
      "Global Step: 16000 Epoch 7/10 Iteration: 16000 Avg. Training loss: 3.8490 0.0109 sec/batch\n",
      "Global Step: 16100 Epoch 7/10 Iteration: 16100 Avg. Training loss: 3.8376 0.0082 sec/batch\n",
      "Global Step: 16200 Epoch 7/10 Iteration: 16200 Avg. Training loss: 3.8210 0.0093 sec/batch\n",
      "Global Step: 16300 Epoch 7/10 Iteration: 16300 Avg. Training loss: 3.8128 0.0162 sec/batch\n",
      "Global Step: 16400 Epoch 7/10 Iteration: 16400 Avg. Training loss: 3.7934 0.0061 sec/batch\n",
      "Global Step: 16500 Epoch 7/10 Iteration: 16500 Avg. Training loss: 3.7944 0.0052 sec/batch\n",
      "Global Step: 16600 Epoch 7/10 Iteration: 16600 Avg. Training loss: 3.7761 0.0057 sec/batch\n",
      "Global Step: 16700 Epoch 7/10 Iteration: 16700 Avg. Training loss: 3.7686 0.0053 sec/batch\n",
      "Global Step: 16800 Epoch 7/10 Iteration: 16800 Avg. Training loss: 3.7446 0.0064 sec/batch\n",
      "Global Step: 16900 Epoch 7/10 Iteration: 16900 Avg. Training loss: 3.7265 0.0074 sec/batch\n",
      "Global Step: 17000 Epoch 7/10 Iteration: 17000 Avg. Training loss: 3.7305 0.0081 sec/batch\n",
      "Global Step: 17100 Epoch 7/10 Iteration: 17100 Avg. Training loss: 3.7125 0.0058 sec/batch\n",
      "Epoch 8/10 Threshold: 0.08528661586003657 Length of Training words: 2417397\n",
      "Global Step: 17200 Epoch 8/10 Iteration: 17200 Avg. Training loss: 3.7138 0.0054 sec/batch\n",
      "Global Step: 17300 Epoch 8/10 Iteration: 17300 Avg. Training loss: 3.7103 0.0083 sec/batch\n",
      "Global Step: 17400 Epoch 8/10 Iteration: 17400 Avg. Training loss: 3.7027 0.0096 sec/batch\n",
      "Global Step: 17500 Epoch 8/10 Iteration: 17500 Avg. Training loss: 3.6817 0.0073 sec/batch\n",
      "Global Step: 17600 Epoch 8/10 Iteration: 17600 Avg. Training loss: 3.6682 0.0071 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17700 Epoch 8/10 Iteration: 17700 Avg. Training loss: 3.6585 0.0059 sec/batch\n",
      "Global Step: 17800 Epoch 8/10 Iteration: 17800 Avg. Training loss: 3.6574 0.0089 sec/batch\n",
      "Global Step: 17900 Epoch 8/10 Iteration: 17900 Avg. Training loss: 3.6449 0.0061 sec/batch\n",
      "Global Step: 18000 Epoch 8/10 Iteration: 18000 Avg. Training loss: 3.6318 0.0082 sec/batch\n",
      "Global Step: 18100 Epoch 8/10 Iteration: 18100 Avg. Training loss: 3.6310 0.0078 sec/batch\n",
      "Global Step: 18200 Epoch 8/10 Iteration: 18200 Avg. Training loss: 3.6126 0.0096 sec/batch\n",
      "Global Step: 18300 Epoch 8/10 Iteration: 18300 Avg. Training loss: 3.6083 0.0062 sec/batch\n",
      "Global Step: 18400 Epoch 8/10 Iteration: 18400 Avg. Training loss: 3.6070 0.0052 sec/batch\n",
      "Global Step: 18500 Epoch 8/10 Iteration: 18500 Avg. Training loss: 3.5895 0.0057 sec/batch\n",
      "Global Step: 18600 Epoch 8/10 Iteration: 18600 Avg. Training loss: 3.5938 0.0062 sec/batch\n",
      "Global Step: 18700 Epoch 8/10 Iteration: 18700 Avg. Training loss: 3.5769 0.0088 sec/batch\n",
      "Global Step: 18800 Epoch 8/10 Iteration: 18800 Avg. Training loss: 3.5653 0.0074 sec/batch\n",
      "Global Step: 18900 Epoch 8/10 Iteration: 18900 Avg. Training loss: 3.5770 0.0086 sec/batch\n",
      "Global Step: 19000 Epoch 8/10 Iteration: 19000 Avg. Training loss: 3.5622 0.0057 sec/batch\n",
      "Global Step: 19100 Epoch 8/10 Iteration: 19100 Avg. Training loss: 3.5595 0.0053 sec/batch\n",
      "Global Step: 19200 Epoch 8/10 Iteration: 19200 Avg. Training loss: 3.5439 0.0074 sec/batch\n",
      "Global Step: 19300 Epoch 8/10 Iteration: 19300 Avg. Training loss: 3.5352 0.0102 sec/batch\n",
      "Global Step: 19400 Epoch 8/10 Iteration: 19400 Avg. Training loss: 3.5320 0.0074 sec/batch\n",
      "Global Step: 19500 Epoch 8/10 Iteration: 19500 Avg. Training loss: 3.5229 0.0056 sec/batch\n",
      "Epoch 9/10 Threshold: 0.09202668756192672 Length of Training words: 2458602\n",
      "Global Step: 19600 Epoch 9/10 Iteration: 19600 Avg. Training loss: 3.5238 0.0037 sec/batch\n",
      "Global Step: 19700 Epoch 9/10 Iteration: 19700 Avg. Training loss: 3.5265 0.0083 sec/batch\n",
      "Global Step: 19800 Epoch 9/10 Iteration: 19800 Avg. Training loss: 3.5186 0.0079 sec/batch\n",
      "Global Step: 19900 Epoch 9/10 Iteration: 19900 Avg. Training loss: 3.5098 0.0058 sec/batch\n",
      "Global Step: 20000 Epoch 9/10 Iteration: 20000 Avg. Training loss: 3.4991 0.0059 sec/batch\n",
      "Global Step: 20100 Epoch 9/10 Iteration: 20100 Avg. Training loss: 3.4900 0.0072 sec/batch\n",
      "Global Step: 20200 Epoch 9/10 Iteration: 20200 Avg. Training loss: 3.4901 0.0119 sec/batch\n",
      "Global Step: 20300 Epoch 9/10 Iteration: 20300 Avg. Training loss: 3.4864 0.0085 sec/batch\n",
      "Global Step: 20400 Epoch 9/10 Iteration: 20400 Avg. Training loss: 3.4783 0.0099 sec/batch\n",
      "Global Step: 20500 Epoch 9/10 Iteration: 20500 Avg. Training loss: 3.4831 0.0060 sec/batch\n",
      "Global Step: 20600 Epoch 9/10 Iteration: 20600 Avg. Training loss: 3.4664 0.0060 sec/batch\n",
      "Global Step: 20700 Epoch 9/10 Iteration: 20700 Avg. Training loss: 3.4619 0.0059 sec/batch\n",
      "Global Step: 20800 Epoch 9/10 Iteration: 20800 Avg. Training loss: 3.4682 0.0060 sec/batch\n",
      "Global Step: 20900 Epoch 9/10 Iteration: 20900 Avg. Training loss: 3.4600 0.0077 sec/batch\n",
      "Global Step: 21000 Epoch 9/10 Iteration: 21000 Avg. Training loss: 3.4535 0.0065 sec/batch\n",
      "Global Step: 21100 Epoch 9/10 Iteration: 21100 Avg. Training loss: 3.4532 0.0058 sec/batch\n",
      "Global Step: 21200 Epoch 9/10 Iteration: 21200 Avg. Training loss: 3.4481 0.0057 sec/batch\n",
      "Global Step: 21300 Epoch 9/10 Iteration: 21300 Avg. Training loss: 3.4450 0.0070 sec/batch\n",
      "Global Step: 21400 Epoch 9/10 Iteration: 21400 Avg. Training loss: 3.4523 0.0063 sec/batch\n",
      "Global Step: 21500 Epoch 9/10 Iteration: 21500 Avg. Training loss: 3.4412 0.0060 sec/batch\n",
      "Global Step: 21600 Epoch 9/10 Iteration: 21600 Avg. Training loss: 3.4395 0.0060 sec/batch\n",
      "Global Step: 21700 Epoch 9/10 Iteration: 21700 Avg. Training loss: 3.4274 0.0077 sec/batch\n",
      "Global Step: 21800 Epoch 9/10 Iteration: 21800 Avg. Training loss: 3.4186 0.0053 sec/batch\n",
      "Global Step: 21900 Epoch 9/10 Iteration: 21900 Avg. Training loss: 3.4236 0.0074 sec/batch\n",
      "Global Step: 22000 Epoch 9/10 Iteration: 22000 Avg. Training loss: 3.4234 0.0062 sec/batch\n",
      "Epoch 10/10 Threshold: 0.0964316575716998 Length of Training words: 2482577\n",
      "Global Step: 22100 Epoch 10/10 Iteration: 22100 Avg. Training loss: 3.4264 0.0063 sec/batch\n",
      "Global Step: 22200 Epoch 10/10 Iteration: 22200 Avg. Training loss: 3.4325 0.0064 sec/batch\n",
      "Global Step: 22300 Epoch 10/10 Iteration: 22300 Avg. Training loss: 3.4252 0.0063 sec/batch\n",
      "Global Step: 22400 Epoch 10/10 Iteration: 22400 Avg. Training loss: 3.4146 0.0092 sec/batch\n",
      "Global Step: 22500 Epoch 10/10 Iteration: 22500 Avg. Training loss: 3.4086 0.0062 sec/batch\n",
      "Global Step: 22600 Epoch 10/10 Iteration: 22600 Avg. Training loss: 3.4028 0.0061 sec/batch\n",
      "Global Step: 22700 Epoch 10/10 Iteration: 22700 Avg. Training loss: 3.4134 0.0073 sec/batch\n",
      "Global Step: 22800 Epoch 10/10 Iteration: 22800 Avg. Training loss: 3.4057 0.0064 sec/batch\n",
      "Global Step: 22900 Epoch 10/10 Iteration: 22900 Avg. Training loss: 3.4008 0.0084 sec/batch\n",
      "Global Step: 23000 Epoch 10/10 Iteration: 23000 Avg. Training loss: 3.4042 0.0059 sec/batch\n",
      "Global Step: 23100 Epoch 10/10 Iteration: 23100 Avg. Training loss: 3.3930 0.0065 sec/batch\n",
      "Global Step: 23200 Epoch 10/10 Iteration: 23200 Avg. Training loss: 3.3961 0.0071 sec/batch\n",
      "Global Step: 23300 Epoch 10/10 Iteration: 23300 Avg. Training loss: 3.3999 0.0053 sec/batch\n",
      "Global Step: 23400 Epoch 10/10 Iteration: 23400 Avg. Training loss: 3.3876 0.0062 sec/batch\n",
      "Global Step: 23500 Epoch 10/10 Iteration: 23500 Avg. Training loss: 3.3963 0.0083 sec/batch\n",
      "Global Step: 23600 Epoch 10/10 Iteration: 23600 Avg. Training loss: 3.3866 0.0073 sec/batch\n",
      "Global Step: 23700 Epoch 10/10 Iteration: 23700 Avg. Training loss: 3.3803 0.0057 sec/batch\n",
      "Global Step: 23800 Epoch 10/10 Iteration: 23800 Avg. Training loss: 3.3961 0.0063 sec/batch\n",
      "Global Step: 23900 Epoch 10/10 Iteration: 23900 Avg. Training loss: 3.3953 0.0082 sec/batch\n",
      "Global Step: 24000 Epoch 10/10 Iteration: 24000 Avg. Training loss: 3.3878 0.0056 sec/batch\n",
      "Global Step: 24100 Epoch 10/10 Iteration: 24100 Avg. Training loss: 3.3856 0.0077 sec/batch\n",
      "Global Step: 24200 Epoch 10/10 Iteration: 24200 Avg. Training loss: 3.3766 0.0073 sec/batch\n",
      "Global Step: 24300 Epoch 10/10 Iteration: 24300 Avg. Training loss: 3.3706 0.0059 sec/batch\n",
      "Global Step: 24400 Epoch 10/10 Iteration: 24400 Avg. Training loss: 3.3749 0.0063 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    batch_loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            batch_loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(batch_loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                batch_loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/\" + model_name + \"/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/' + model_name))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
