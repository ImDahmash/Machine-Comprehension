{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 275616), ('ORGANIZATION', 8190), ('PERSON', 7738), ('LOCATION', 7501)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ner_test.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 299045\n",
      "Unique words: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 90810), (1, 8190), (2, 7738), (3, 7501)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "## Your code here\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "threshold = 1e-1\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "word_counts = Counter(train_words)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    #     labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 3 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#         valid_examples = np.append(valid_examples, \n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/ner’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/ner/ner.ckpt\n",
      "Global Step: 5800 Epoch 1/100 Iteration: 100 Avg. Training loss: 0.8470 0.0131 sec/batch\n",
      "Global Step: 5900 Epoch 2/100 Iteration: 200 Avg. Training loss: 0.8536 0.0111 sec/batch\n",
      "Global Step: 6000 Epoch 3/100 Iteration: 300 Avg. Training loss: 0.8483 0.0094 sec/batch\n",
      "Global Step: 6100 Epoch 4/100 Iteration: 400 Avg. Training loss: 0.8519 0.0074 sec/batch\n",
      "Global Step: 6200 Epoch 5/100 Iteration: 500 Avg. Training loss: 0.8387 0.0053 sec/batch\n",
      "Global Step: 6300 Epoch 6/100 Iteration: 600 Avg. Training loss: 0.8556 0.0050 sec/batch\n",
      "Global Step: 6400 Epoch 7/100 Iteration: 700 Avg. Training loss: 0.8463 0.0022 sec/batch\n",
      "Global Step: 6500 Epoch 8/100 Iteration: 800 Avg. Training loss: 0.8399 0.0004 sec/batch\n",
      "Global Step: 6600 Epoch 8/100 Iteration: 900 Avg. Training loss: 0.8446 0.0149 sec/batch\n",
      "Global Step: 6700 Epoch 9/100 Iteration: 1000 Avg. Training loss: 0.8555 0.0120 sec/batch\n",
      "Global Step: 6800 Epoch 10/100 Iteration: 1100 Avg. Training loss: 0.8421 0.0105 sec/batch\n",
      "Global Step: 6900 Epoch 11/100 Iteration: 1200 Avg. Training loss: 0.8576 0.0089 sec/batch\n",
      "Global Step: 7000 Epoch 12/100 Iteration: 1300 Avg. Training loss: 0.8354 0.0059 sec/batch\n",
      "Global Step: 7100 Epoch 13/100 Iteration: 1400 Avg. Training loss: 0.8505 0.0042 sec/batch\n",
      "Global Step: 7200 Epoch 14/100 Iteration: 1500 Avg. Training loss: 0.8507 0.0026 sec/batch\n",
      "Global Step: 7300 Epoch 15/100 Iteration: 1600 Avg. Training loss: 0.8441 0.0006 sec/batch\n",
      "Global Step: 7400 Epoch 15/100 Iteration: 1700 Avg. Training loss: 0.8424 0.0138 sec/batch\n",
      "Global Step: 7500 Epoch 16/100 Iteration: 1800 Avg. Training loss: 0.8585 0.0131 sec/batch\n",
      "Global Step: 7600 Epoch 17/100 Iteration: 1900 Avg. Training loss: 0.8417 0.0107 sec/batch\n",
      "Global Step: 7700 Epoch 18/100 Iteration: 2000 Avg. Training loss: 0.8539 0.0084 sec/batch\n",
      "Global Step: 7800 Epoch 19/100 Iteration: 2100 Avg. Training loss: 0.8286 0.0065 sec/batch\n",
      "Global Step: 7900 Epoch 20/100 Iteration: 2200 Avg. Training loss: 0.8541 0.0053 sec/batch\n",
      "Global Step: 8000 Epoch 21/100 Iteration: 2300 Avg. Training loss: 0.8506 0.0025 sec/batch\n",
      "Global Step: 8100 Epoch 22/100 Iteration: 2400 Avg. Training loss: 0.8473 0.0008 sec/batch\n",
      "Global Step: 8200 Epoch 22/100 Iteration: 2500 Avg. Training loss: 0.8398 0.0135 sec/batch\n",
      "Global Step: 8300 Epoch 23/100 Iteration: 2600 Avg. Training loss: 0.8596 0.0128 sec/batch\n",
      "Global Step: 8400 Epoch 24/100 Iteration: 2700 Avg. Training loss: 0.8400 0.0118 sec/batch\n",
      "Global Step: 8500 Epoch 25/100 Iteration: 2800 Avg. Training loss: 0.8610 0.0096 sec/batch\n",
      "Global Step: 8600 Epoch 26/100 Iteration: 2900 Avg. Training loss: 0.8252 0.0072 sec/batch\n",
      "Global Step: 8700 Epoch 27/100 Iteration: 3000 Avg. Training loss: 0.8545 0.0047 sec/batch\n",
      "Global Step: 8800 Epoch 28/100 Iteration: 3100 Avg. Training loss: 0.8511 0.0032 sec/batch\n",
      "Global Step: 8900 Epoch 29/100 Iteration: 3200 Avg. Training loss: 0.8488 0.0014 sec/batch\n",
      "Global Step: 9000 Epoch 29/100 Iteration: 3300 Avg. Training loss: 0.8350 0.0143 sec/batch\n",
      "Global Step: 9100 Epoch 30/100 Iteration: 3400 Avg. Training loss: 0.8545 0.0141 sec/batch\n",
      "Global Step: 9200 Epoch 31/100 Iteration: 3500 Avg. Training loss: 0.8411 0.0110 sec/batch\n",
      "Global Step: 9300 Epoch 32/100 Iteration: 3600 Avg. Training loss: 0.8686 0.0092 sec/batch\n",
      "Global Step: 9400 Epoch 33/100 Iteration: 3700 Avg. Training loss: 0.8263 0.0080 sec/batch\n",
      "Global Step: 9500 Epoch 34/100 Iteration: 3800 Avg. Training loss: 0.8521 0.0020 sec/batch\n",
      "Global Step: 9600 Epoch 35/100 Iteration: 3900 Avg. Training loss: 0.8525 0.0030 sec/batch\n",
      "Global Step: 9700 Epoch 36/100 Iteration: 4000 Avg. Training loss: 0.8504 0.0016 sec/batch\n",
      "Global Step: 9800 Epoch 36/100 Iteration: 4100 Avg. Training loss: 0.8298 0.0139 sec/batch\n",
      "Global Step: 9900 Epoch 37/100 Iteration: 4200 Avg. Training loss: 0.8440 0.0129 sec/batch\n",
      "Global Step: 10000 Epoch 38/100 Iteration: 4300 Avg. Training loss: 0.8502 0.0115 sec/batch\n",
      "Global Step: 10100 Epoch 39/100 Iteration: 4400 Avg. Training loss: 0.8657 0.0102 sec/batch\n",
      "Global Step: 10200 Epoch 40/100 Iteration: 4500 Avg. Training loss: 0.8307 0.0073 sec/batch\n",
      "Global Step: 10300 Epoch 41/100 Iteration: 4600 Avg. Training loss: 0.8515 0.0060 sec/batch\n",
      "Global Step: 10400 Epoch 42/100 Iteration: 4700 Avg. Training loss: 0.8573 0.0036 sec/batch\n",
      "Global Step: 10500 Epoch 43/100 Iteration: 4800 Avg. Training loss: 0.8474 0.0015 sec/batch\n",
      "Global Step: 10600 Epoch 43/100 Iteration: 4900 Avg. Training loss: 0.8364 0.0136 sec/batch\n",
      "Global Step: 10700 Epoch 44/100 Iteration: 5000 Avg. Training loss: 0.8340 0.0142 sec/batch\n",
      "Global Step: 10800 Epoch 45/100 Iteration: 5100 Avg. Training loss: 0.8543 0.0115 sec/batch\n",
      "Global Step: 10900 Epoch 46/100 Iteration: 5200 Avg. Training loss: 0.8562 0.0105 sec/batch\n",
      "Global Step: 11000 Epoch 47/100 Iteration: 5300 Avg. Training loss: 0.8409 0.0074 sec/batch\n",
      "Global Step: 11100 Epoch 48/100 Iteration: 5400 Avg. Training loss: 0.8465 0.0063 sec/batch\n",
      "Global Step: 11200 Epoch 49/100 Iteration: 5500 Avg. Training loss: 0.8581 0.0043 sec/batch\n",
      "Global Step: 11300 Epoch 50/100 Iteration: 5600 Avg. Training loss: 0.8428 0.0018 sec/batch\n",
      "Global Step: 11400 Epoch 50/100 Iteration: 5700 Avg. Training loss: 0.8333 0.0138 sec/batch\n",
      "Global Step: 11500 Epoch 51/100 Iteration: 5800 Avg. Training loss: 0.8468 0.0146 sec/batch\n",
      "Global Step: 11600 Epoch 52/100 Iteration: 5900 Avg. Training loss: 0.8525 0.0127 sec/batch\n",
      "Global Step: 11700 Epoch 53/100 Iteration: 6000 Avg. Training loss: 0.8449 0.0098 sec/batch\n",
      "Global Step: 11800 Epoch 54/100 Iteration: 6100 Avg. Training loss: 0.8503 0.0089 sec/batch\n",
      "Global Step: 11900 Epoch 55/100 Iteration: 6200 Avg. Training loss: 0.8394 0.0056 sec/batch\n",
      "Global Step: 12000 Epoch 56/100 Iteration: 6300 Avg. Training loss: 0.8534 0.0026 sec/batch\n",
      "Global Step: 12100 Epoch 57/100 Iteration: 6400 Avg. Training loss: 0.8456 0.0022 sec/batch\n",
      "Global Step: 12200 Epoch 58/100 Iteration: 6500 Avg. Training loss: 0.8399 0.0002 sec/batch\n",
      "Global Step: 12300 Epoch 58/100 Iteration: 6600 Avg. Training loss: 0.8445 0.0133 sec/batch\n",
      "Global Step: 12400 Epoch 59/100 Iteration: 6700 Avg. Training loss: 0.8566 0.0128 sec/batch\n",
      "Global Step: 12500 Epoch 60/100 Iteration: 6800 Avg. Training loss: 0.8425 0.0096 sec/batch\n",
      "Global Step: 12600 Epoch 61/100 Iteration: 6900 Avg. Training loss: 0.8562 0.0085 sec/batch\n",
      "Global Step: 12700 Epoch 62/100 Iteration: 7000 Avg. Training loss: 0.8357 0.0069 sec/batch\n",
      "Global Step: 12800 Epoch 63/100 Iteration: 7100 Avg. Training loss: 0.8509 0.0042 sec/batch\n",
      "Global Step: 12900 Epoch 64/100 Iteration: 7200 Avg. Training loss: 0.8501 0.0028 sec/batch\n",
      "Global Step: 13000 Epoch 65/100 Iteration: 7300 Avg. Training loss: 0.8439 0.0007 sec/batch\n",
      "Global Step: 13100 Epoch 65/100 Iteration: 7400 Avg. Training loss: 0.8411 0.0149 sec/batch\n",
      "Global Step: 13200 Epoch 66/100 Iteration: 7500 Avg. Training loss: 0.8583 0.0124 sec/batch\n",
      "Global Step: 13300 Epoch 67/100 Iteration: 7600 Avg. Training loss: 0.8422 0.0109 sec/batch\n",
      "Global Step: 13400 Epoch 68/100 Iteration: 7700 Avg. Training loss: 0.8573 0.0086 sec/batch\n",
      "Global Step: 13500 Epoch 69/100 Iteration: 7800 Avg. Training loss: 0.8293 0.0066 sec/batch\n",
      "Global Step: 13600 Epoch 70/100 Iteration: 7900 Avg. Training loss: 0.8538 0.0054 sec/batch\n",
      "Global Step: 13700 Epoch 71/100 Iteration: 8000 Avg. Training loss: 0.8497 0.0027 sec/batch\n",
      "Global Step: 13800 Epoch 72/100 Iteration: 8100 Avg. Training loss: 0.8465 0.0010 sec/batch\n",
      "Global Step: 13900 Epoch 72/100 Iteration: 8200 Avg. Training loss: 0.8396 0.0149 sec/batch\n",
      "Global Step: 14000 Epoch 73/100 Iteration: 8300 Avg. Training loss: 0.8583 0.0122 sec/batch\n",
      "Global Step: 14100 Epoch 74/100 Iteration: 8400 Avg. Training loss: 0.8413 0.0107 sec/batch\n",
      "Global Step: 14200 Epoch 75/100 Iteration: 8500 Avg. Training loss: 0.8634 0.0100 sec/batch\n",
      "Global Step: 14300 Epoch 76/100 Iteration: 8600 Avg. Training loss: 0.8264 0.0072 sec/batch\n",
      "Global Step: 14400 Epoch 77/100 Iteration: 8700 Avg. Training loss: 0.8535 0.0051 sec/batch\n",
      "Global Step: 14500 Epoch 78/100 Iteration: 8800 Avg. Training loss: 0.8515 0.0031 sec/batch\n",
      "Global Step: 14600 Epoch 79/100 Iteration: 8900 Avg. Training loss: 0.8500 0.0012 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 14700 Epoch 79/100 Iteration: 9000 Avg. Training loss: 0.8348 0.0144 sec/batch\n",
      "Global Step: 14800 Epoch 80/100 Iteration: 9100 Avg. Training loss: 0.8545 0.0137 sec/batch\n",
      "Global Step: 14900 Epoch 81/100 Iteration: 9200 Avg. Training loss: 0.8406 0.0110 sec/batch\n",
      "Global Step: 15000 Epoch 82/100 Iteration: 9300 Avg. Training loss: 0.8697 0.0096 sec/batch\n",
      "Global Step: 15100 Epoch 83/100 Iteration: 9400 Avg. Training loss: 0.8263 0.0081 sec/batch\n",
      "Global Step: 15200 Epoch 84/100 Iteration: 9500 Avg. Training loss: 0.8509 0.0053 sec/batch\n",
      "Global Step: 15300 Epoch 85/100 Iteration: 9600 Avg. Training loss: 0.8525 0.0036 sec/batch\n",
      "Global Step: 15400 Epoch 86/100 Iteration: 9700 Avg. Training loss: 0.8512 0.0015 sec/batch\n",
      "Global Step: 15500 Epoch 86/100 Iteration: 9800 Avg. Training loss: 0.8307 0.0138 sec/batch\n",
      "Global Step: 15600 Epoch 87/100 Iteration: 9900 Avg. Training loss: 0.8433 0.0137 sec/batch\n",
      "Global Step: 15700 Epoch 88/100 Iteration: 10000 Avg. Training loss: 0.8494 0.0124 sec/batch\n",
      "Global Step: 15800 Epoch 89/100 Iteration: 10100 Avg. Training loss: 0.8670 0.0085 sec/batch\n",
      "Global Step: 15900 Epoch 90/100 Iteration: 10200 Avg. Training loss: 0.8304 0.0078 sec/batch\n",
      "Global Step: 16000 Epoch 91/100 Iteration: 10300 Avg. Training loss: 0.8512 0.0054 sec/batch\n",
      "Global Step: 16100 Epoch 92/100 Iteration: 10400 Avg. Training loss: 0.8556 0.0034 sec/batch\n",
      "Global Step: 16200 Epoch 93/100 Iteration: 10500 Avg. Training loss: 0.8482 0.0018 sec/batch\n",
      "Global Step: 16300 Epoch 93/100 Iteration: 10600 Avg. Training loss: 0.8356 0.0151 sec/batch\n",
      "Global Step: 16400 Epoch 94/100 Iteration: 10700 Avg. Training loss: 0.8362 0.0141 sec/batch\n",
      "Global Step: 16500 Epoch 95/100 Iteration: 10800 Avg. Training loss: 0.8550 0.0124 sec/batch\n",
      "Global Step: 16600 Epoch 96/100 Iteration: 10900 Avg. Training loss: 0.8562 0.0102 sec/batch\n",
      "Global Step: 16700 Epoch 97/100 Iteration: 11000 Avg. Training loss: 0.8411 0.0081 sec/batch\n",
      "Global Step: 16800 Epoch 98/100 Iteration: 11100 Avg. Training loss: 0.8467 0.0059 sec/batch\n",
      "Global Step: 16900 Epoch 99/100 Iteration: 11200 Avg. Training loss: 0.8567 0.0038 sec/batch\n",
      "Global Step: 17000 Epoch 100/100 Iteration: 11300 Avg. Training loss: 0.8433 0.0019 sec/batch\n",
      "Global Step: 17100 Epoch 100/100 Iteration: 11400 Avg. Training loss: 0.8322 0.0146 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/ner'))\n",
    "    embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "                        \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/ner/ner.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/text8.ckpt\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/ner'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAALNCAYAAAAWWk8sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5582f1b048>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 358,
       "width": 410
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
