{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.06, 0.09)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/pos’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Threshold: 0.08708670843710356 Length of Training words: 2429632\n",
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 6.8498 0.0283 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 4.8515 0.0158 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 3.8954 0.0156 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 3.4002 0.0161 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 3.1780 0.0152 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 3.0604 0.0136 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 2.9929 0.0124 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 2.9768 0.0178 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 2.9223 0.0158 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 2.8990 0.0155 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 2.8639 0.0148 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 2.8691 0.0139 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 2.8690 0.0154 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 2.8716 0.0153 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 2.8565 0.0152 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 2.8583 0.0159 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 2.8374 0.0144 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 2.8769 0.0162 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 2.8491 0.0173 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 2.8620 0.0167 sec/batch\n",
      "Global Step: 2100 Epoch 1/50 Iteration: 2100 Avg. Training loss: 2.8236 0.0151 sec/batch\n",
      "Global Step: 2200 Epoch 1/50 Iteration: 2200 Avg. Training loss: 2.8194 0.0168 sec/batch\n",
      "Global Step: 2300 Epoch 1/50 Iteration: 2300 Avg. Training loss: 2.8519 0.0148 sec/batch\n",
      "Global Step: 2400 Epoch 1/50 Iteration: 2400 Avg. Training loss: 2.8353 0.0141 sec/batch\n",
      "Epoch 2/50 Threshold: 0.06967566519489501 Length of Training words: 2311639\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 2.8783 0.0118 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 2.9035 0.0156 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 2.9013 0.0142 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 2.8740 0.0157 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 2.8744 0.0150 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 2.8715 0.0170 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 2.8728 0.0162 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 2.8967 0.0147 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 2.8784 0.0164 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 2.8672 0.0172 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 2.8673 0.0161 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 2.8763 0.0160 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 2.8746 0.0151 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 2.8721 0.0163 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 2.8834 0.0161 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 2.8555 0.0147 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 2.8963 0.0148 sec/batch\n",
      "Global Step: 4200 Epoch 2/50 Iteration: 4200 Avg. Training loss: 2.8781 0.0137 sec/batch\n",
      "Global Step: 4300 Epoch 2/50 Iteration: 4300 Avg. Training loss: 2.8910 0.0157 sec/batch\n",
      "Global Step: 4400 Epoch 2/50 Iteration: 4400 Avg. Training loss: 2.8711 0.0152 sec/batch\n",
      "Global Step: 4500 Epoch 2/50 Iteration: 4500 Avg. Training loss: 2.8581 0.0151 sec/batch\n",
      "Global Step: 4600 Epoch 2/50 Iteration: 4600 Avg. Training loss: 2.8675 0.0169 sec/batch\n",
      "Global Step: 4700 Epoch 2/50 Iteration: 4700 Avg. Training loss: 2.8752 0.0175 sec/batch\n",
      "Epoch 3/50 Threshold: 0.08060656649666245 Length of Training words: 2388627\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 2.8617 0.0085 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 2.8708 0.0134 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 2.8717 0.0140 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 2.8455 0.0136 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 2.8450 0.0147 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 2.8402 0.0151 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 2.8491 0.0145 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 2.8555 0.0156 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 2.8543 0.0142 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 2.8610 0.0135 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 2.8374 0.0157 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 2.8408 0.0182 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 2.8511 0.0186 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 2.8511 0.0152 sec/batch\n",
      "Global Step: 6200 Epoch 3/50 Iteration: 6200 Avg. Training loss: 2.8521 0.0143 sec/batch\n",
      "Global Step: 6300 Epoch 3/50 Iteration: 6300 Avg. Training loss: 2.8438 0.0152 sec/batch\n",
      "Global Step: 6400 Epoch 3/50 Iteration: 6400 Avg. Training loss: 2.8316 0.0167 sec/batch\n",
      "Global Step: 6500 Epoch 3/50 Iteration: 6500 Avg. Training loss: 2.8734 0.0152 sec/batch\n",
      "Global Step: 6600 Epoch 3/50 Iteration: 6600 Avg. Training loss: 2.8546 0.0148 sec/batch\n",
      "Global Step: 6700 Epoch 3/50 Iteration: 6700 Avg. Training loss: 2.8733 0.0171 sec/batch\n",
      "Global Step: 6800 Epoch 3/50 Iteration: 6800 Avg. Training loss: 2.8315 0.0166 sec/batch\n",
      "Global Step: 6900 Epoch 3/50 Iteration: 6900 Avg. Training loss: 2.8233 0.0167 sec/batch\n",
      "Global Step: 7000 Epoch 3/50 Iteration: 7000 Avg. Training loss: 2.8571 0.0148 sec/batch\n",
      "Global Step: 7100 Epoch 3/50 Iteration: 7100 Avg. Training loss: 2.8430 0.0178 sec/batch\n",
      "Epoch 4/50 Threshold: 0.0859893440396867 Length of Training words: 2421835\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 2.8485 0.0097 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 2.8605 0.0137 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 2.8588 0.0150 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 2.8267 0.0182 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 2.8364 0.0151 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 2.8293 0.0154 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 2.8401 0.0170 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 2.8421 0.0135 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 2.8450 0.0133 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 2.8486 0.0130 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 2.8212 0.0177 sec/batch\n",
      "Global Step: 8300 Epoch 4/50 Iteration: 8300 Avg. Training loss: 2.8276 0.0144 sec/batch\n",
      "Global Step: 8400 Epoch 4/50 Iteration: 8400 Avg. Training loss: 2.8445 0.0146 sec/batch\n",
      "Global Step: 8500 Epoch 4/50 Iteration: 8500 Avg. Training loss: 2.8315 0.0155 sec/batch\n",
      "Global Step: 8600 Epoch 4/50 Iteration: 8600 Avg. Training loss: 2.8451 0.0154 sec/batch\n",
      "Global Step: 8700 Epoch 4/50 Iteration: 8700 Avg. Training loss: 2.8367 0.0142 sec/batch\n",
      "Global Step: 8800 Epoch 4/50 Iteration: 8800 Avg. Training loss: 2.8189 0.0149 sec/batch\n",
      "Global Step: 8900 Epoch 4/50 Iteration: 8900 Avg. Training loss: 2.8636 0.0132 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9000 Epoch 4/50 Iteration: 9000 Avg. Training loss: 2.8367 0.0152 sec/batch\n",
      "Global Step: 9100 Epoch 4/50 Iteration: 9100 Avg. Training loss: 2.8509 0.0162 sec/batch\n",
      "Global Step: 9200 Epoch 4/50 Iteration: 9200 Avg. Training loss: 2.8298 0.0165 sec/batch\n",
      "Global Step: 9300 Epoch 4/50 Iteration: 9300 Avg. Training loss: 2.8201 0.0155 sec/batch\n",
      "Global Step: 9400 Epoch 4/50 Iteration: 9400 Avg. Training loss: 2.8320 0.0141 sec/batch\n",
      "Global Step: 9500 Epoch 4/50 Iteration: 9500 Avg. Training loss: 2.8372 0.0174 sec/batch\n",
      "Epoch 5/50 Threshold: 0.0848752245043963 Length of Training words: 2415331\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 2.8358 0.0085 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 2.8597 0.0174 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 2.8593 0.0166 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 2.8449 0.0134 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 2.8310 0.0141 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 2.8239 0.0136 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 2.8519 0.0145 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 10400 Epoch 5/50 Iteration: 10400 Avg. Training loss: 2.8355 0.0140 sec/batch\n",
      "Global Step: 10500 Epoch 5/50 Iteration: 10500 Avg. Training loss: 2.8666 0.0180 sec/batch\n",
      "Global Step: 10600 Epoch 5/50 Iteration: 10600 Avg. Training loss: 2.8232 0.0172 sec/batch\n",
      "Global Step: 10700 Epoch 5/50 Iteration: 10700 Avg. Training loss: 2.8223 0.0182 sec/batch\n",
      "Global Step: 10800 Epoch 5/50 Iteration: 10800 Avg. Training loss: 2.8563 0.0133 sec/batch\n",
      "Global Step: 10900 Epoch 5/50 Iteration: 10900 Avg. Training loss: 2.8248 0.0176 sec/batch\n",
      "Global Step: 11000 Epoch 5/50 Iteration: 11000 Avg. Training loss: 2.8444 0.0150 sec/batch\n",
      "Global Step: 11100 Epoch 5/50 Iteration: 11100 Avg. Training loss: 2.8403 0.0157 sec/batch\n",
      "Global Step: 11200 Epoch 5/50 Iteration: 11200 Avg. Training loss: 2.8208 0.0142 sec/batch\n",
      "Global Step: 11300 Epoch 5/50 Iteration: 11300 Avg. Training loss: 2.8621 0.0165 sec/batch\n",
      "Global Step: 11400 Epoch 5/50 Iteration: 11400 Avg. Training loss: 2.8416 0.0128 sec/batch\n",
      "Global Step: 11500 Epoch 5/50 Iteration: 11500 Avg. Training loss: 2.8561 0.0143 sec/batch\n",
      "Global Step: 11600 Epoch 5/50 Iteration: 11600 Avg. Training loss: 2.8463 0.0141 sec/batch\n",
      "Global Step: 11700 Epoch 5/50 Iteration: 11700 Avg. Training loss: 2.8133 0.0172 sec/batch\n",
      "Global Step: 11800 Epoch 5/50 Iteration: 11800 Avg. Training loss: 2.8268 0.0172 sec/batch\n",
      "Global Step: 11900 Epoch 5/50 Iteration: 11900 Avg. Training loss: 2.8307 0.0157 sec/batch\n",
      "Epoch 6/50 Threshold: 0.06057974082760326 Length of Training words: 2234998\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 2.8612 0.0057 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 2.9153 0.0152 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 2.9147 0.0160 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 2.8999 0.0145 sec/batch\n",
      "Global Step: 12400 Epoch 6/50 Iteration: 12400 Avg. Training loss: 2.8952 0.0145 sec/batch\n",
      "Global Step: 12500 Epoch 6/50 Iteration: 12500 Avg. Training loss: 2.8881 0.0160 sec/batch\n",
      "Global Step: 12600 Epoch 6/50 Iteration: 12600 Avg. Training loss: 2.8990 0.0178 sec/batch\n",
      "Global Step: 12700 Epoch 6/50 Iteration: 12700 Avg. Training loss: 2.9125 0.0155 sec/batch\n",
      "Global Step: 12800 Epoch 6/50 Iteration: 12800 Avg. Training loss: 2.8985 0.0142 sec/batch\n",
      "Global Step: 12900 Epoch 6/50 Iteration: 12900 Avg. Training loss: 2.8875 0.0160 sec/batch\n",
      "Global Step: 13000 Epoch 6/50 Iteration: 13000 Avg. Training loss: 2.8875 0.0151 sec/batch\n",
      "Global Step: 13100 Epoch 6/50 Iteration: 13100 Avg. Training loss: 2.9001 0.0158 sec/batch\n",
      "Global Step: 13200 Epoch 6/50 Iteration: 13200 Avg. Training loss: 2.8940 0.0142 sec/batch\n",
      "Global Step: 13300 Epoch 6/50 Iteration: 13300 Avg. Training loss: 2.8987 0.0155 sec/batch\n",
      "Global Step: 13400 Epoch 6/50 Iteration: 13400 Avg. Training loss: 2.8988 0.0147 sec/batch\n",
      "Global Step: 13500 Epoch 6/50 Iteration: 13500 Avg. Training loss: 2.8796 0.0146 sec/batch\n",
      "Global Step: 13600 Epoch 6/50 Iteration: 13600 Avg. Training loss: 2.9229 0.0173 sec/batch\n",
      "Global Step: 13700 Epoch 6/50 Iteration: 13700 Avg. Training loss: 2.8978 0.0188 sec/batch\n",
      "Global Step: 13800 Epoch 6/50 Iteration: 13800 Avg. Training loss: 2.9174 0.0175 sec/batch\n",
      "Global Step: 13900 Epoch 6/50 Iteration: 13900 Avg. Training loss: 2.8748 0.0151 sec/batch\n",
      "Global Step: 14000 Epoch 6/50 Iteration: 14000 Avg. Training loss: 2.8746 0.0157 sec/batch\n",
      "Global Step: 14100 Epoch 6/50 Iteration: 14100 Avg. Training loss: 2.8978 0.0157 sec/batch\n",
      "Epoch 7/50 Threshold: 0.0850077106535117 Length of Training words: 2416591\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 2.8935 0.0005 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 2.8553 0.0173 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 2.8703 0.0138 sec/batch\n",
      "Global Step: 14500 Epoch 7/50 Iteration: 14500 Avg. Training loss: 2.8535 0.0124 sec/batch\n",
      "Global Step: 14600 Epoch 7/50 Iteration: 14600 Avg. Training loss: 2.8206 0.0168 sec/batch\n",
      "Global Step: 14700 Epoch 7/50 Iteration: 14700 Avg. Training loss: 2.8345 0.0153 sec/batch\n",
      "Global Step: 14800 Epoch 7/50 Iteration: 14800 Avg. Training loss: 2.8407 0.0142 sec/batch\n",
      "Global Step: 14900 Epoch 7/50 Iteration: 14900 Avg. Training loss: 2.8304 0.0154 sec/batch\n",
      "Global Step: 15000 Epoch 7/50 Iteration: 15000 Avg. Training loss: 2.8644 0.0141 sec/batch\n",
      "Global Step: 15100 Epoch 7/50 Iteration: 15100 Avg. Training loss: 2.8404 0.0161 sec/batch\n",
      "Global Step: 15200 Epoch 7/50 Iteration: 15200 Avg. Training loss: 2.8351 0.0156 sec/batch\n",
      "Global Step: 15300 Epoch 7/50 Iteration: 15300 Avg. Training loss: 2.8273 0.0154 sec/batch\n",
      "Global Step: 15400 Epoch 7/50 Iteration: 15400 Avg. Training loss: 2.8339 0.0151 sec/batch\n",
      "Global Step: 15500 Epoch 7/50 Iteration: 15500 Avg. Training loss: 2.8274 0.0165 sec/batch\n",
      "Global Step: 15600 Epoch 7/50 Iteration: 15600 Avg. Training loss: 2.8442 0.0156 sec/batch\n",
      "Global Step: 15700 Epoch 7/50 Iteration: 15700 Avg. Training loss: 2.8390 0.0145 sec/batch\n",
      "Global Step: 15800 Epoch 7/50 Iteration: 15800 Avg. Training loss: 2.8466 0.0144 sec/batch\n",
      "Global Step: 15900 Epoch 7/50 Iteration: 15900 Avg. Training loss: 2.8240 0.0137 sec/batch\n",
      "Global Step: 16000 Epoch 7/50 Iteration: 16000 Avg. Training loss: 2.8630 0.0160 sec/batch\n",
      "Global Step: 16100 Epoch 7/50 Iteration: 16100 Avg. Training loss: 2.8418 0.0159 sec/batch\n",
      "Global Step: 16200 Epoch 7/50 Iteration: 16200 Avg. Training loss: 2.8544 0.0152 sec/batch\n",
      "Global Step: 16300 Epoch 7/50 Iteration: 16300 Avg. Training loss: 2.8151 0.0183 sec/batch\n",
      "Global Step: 16400 Epoch 7/50 Iteration: 16400 Avg. Training loss: 2.8179 0.0167 sec/batch\n",
      "Global Step: 16500 Epoch 7/50 Iteration: 16500 Avg. Training loss: 2.8445 0.0160 sec/batch\n",
      "Global Step: 16600 Epoch 7/50 Iteration: 16600 Avg. Training loss: 2.8349 0.0171 sec/batch\n",
      "Epoch 8/50 Threshold: 0.08467242047489035 Length of Training words: 2414420\n",
      "Global Step: 16700 Epoch 8/50 Iteration: 16700 Avg. Training loss: 2.8454 0.0163 sec/batch\n",
      "Global Step: 16800 Epoch 8/50 Iteration: 16800 Avg. Training loss: 2.8691 0.0147 sec/batch\n",
      "Global Step: 16900 Epoch 8/50 Iteration: 16900 Avg. Training loss: 2.8577 0.0149 sec/batch\n",
      "Global Step: 17000 Epoch 8/50 Iteration: 17000 Avg. Training loss: 2.8350 0.0139 sec/batch\n",
      "Global Step: 17100 Epoch 8/50 Iteration: 17100 Avg. Training loss: 2.8341 0.0148 sec/batch\n",
      "Global Step: 17200 Epoch 8/50 Iteration: 17200 Avg. Training loss: 2.8312 0.0130 sec/batch\n",
      "Global Step: 17300 Epoch 8/50 Iteration: 17300 Avg. Training loss: 2.8392 0.0140 sec/batch\n",
      "Global Step: 17400 Epoch 8/50 Iteration: 17400 Avg. Training loss: 2.8545 0.0137 sec/batch\n",
      "Global Step: 17500 Epoch 8/50 Iteration: 17500 Avg. Training loss: 2.8421 0.0141 sec/batch\n",
      "Global Step: 17600 Epoch 8/50 Iteration: 17600 Avg. Training loss: 2.8445 0.0170 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17700 Epoch 8/50 Iteration: 17700 Avg. Training loss: 2.8237 0.0144 sec/batch\n",
      "Global Step: 17800 Epoch 8/50 Iteration: 17800 Avg. Training loss: 2.8300 0.0127 sec/batch\n",
      "Global Step: 17900 Epoch 8/50 Iteration: 17900 Avg. Training loss: 2.8458 0.0158 sec/batch\n",
      "Global Step: 18000 Epoch 8/50 Iteration: 18000 Avg. Training loss: 2.8464 0.0136 sec/batch\n",
      "Global Step: 18100 Epoch 8/50 Iteration: 18100 Avg. Training loss: 2.8346 0.0166 sec/batch\n",
      "Global Step: 18200 Epoch 8/50 Iteration: 18200 Avg. Training loss: 2.8472 0.0168 sec/batch\n",
      "Global Step: 18300 Epoch 8/50 Iteration: 18300 Avg. Training loss: 2.8252 0.0158 sec/batch\n",
      "Global Step: 18400 Epoch 8/50 Iteration: 18400 Avg. Training loss: 2.8632 0.0176 sec/batch\n",
      "Global Step: 18500 Epoch 8/50 Iteration: 18500 Avg. Training loss: 2.8403 0.0161 sec/batch\n",
      "Global Step: 18600 Epoch 8/50 Iteration: 18600 Avg. Training loss: 2.8588 0.0135 sec/batch\n",
      "Global Step: 18700 Epoch 8/50 Iteration: 18700 Avg. Training loss: 2.8214 0.0137 sec/batch\n",
      "Global Step: 18800 Epoch 8/50 Iteration: 18800 Avg. Training loss: 2.8139 0.0155 sec/batch\n",
      "Global Step: 18900 Epoch 8/50 Iteration: 18900 Avg. Training loss: 2.8470 0.0151 sec/batch\n",
      "Global Step: 19000 Epoch 8/50 Iteration: 19000 Avg. Training loss: 2.8332 0.0172 sec/batch\n",
      "Epoch 9/50 Threshold: 0.08782582800636445 Length of Training words: 2434382\n",
      "Global Step: 19100 Epoch 9/50 Iteration: 19100 Avg. Training loss: 2.8397 0.0130 sec/batch\n",
      "Global Step: 19200 Epoch 9/50 Iteration: 19200 Avg. Training loss: 2.8512 0.0173 sec/batch\n",
      "Global Step: 19300 Epoch 9/50 Iteration: 19300 Avg. Training loss: 2.8553 0.0164 sec/batch\n",
      "Global Step: 19400 Epoch 9/50 Iteration: 19400 Avg. Training loss: 2.8236 0.0150 sec/batch\n",
      "Global Step: 19500 Epoch 9/50 Iteration: 19500 Avg. Training loss: 2.8288 0.0146 sec/batch\n",
      "Global Step: 19600 Epoch 9/50 Iteration: 19600 Avg. Training loss: 2.8261 0.0143 sec/batch\n",
      "Global Step: 19700 Epoch 9/50 Iteration: 19700 Avg. Training loss: 2.8345 0.0138 sec/batch\n",
      "Global Step: 19800 Epoch 9/50 Iteration: 19800 Avg. Training loss: 2.8411 0.0150 sec/batch\n",
      "Global Step: 19900 Epoch 9/50 Iteration: 19900 Avg. Training loss: 2.8350 0.0155 sec/batch\n",
      "Global Step: 20000 Epoch 9/50 Iteration: 20000 Avg. Training loss: 2.8528 0.0161 sec/batch\n",
      "Global Step: 20100 Epoch 9/50 Iteration: 20100 Avg. Training loss: 2.8137 0.0150 sec/batch\n",
      "Global Step: 20200 Epoch 9/50 Iteration: 20200 Avg. Training loss: 2.8243 0.0162 sec/batch\n",
      "Global Step: 20300 Epoch 9/50 Iteration: 20300 Avg. Training loss: 2.8447 0.0161 sec/batch\n",
      "Global Step: 20400 Epoch 9/50 Iteration: 20400 Avg. Training loss: 2.8207 0.0160 sec/batch\n",
      "Global Step: 20500 Epoch 9/50 Iteration: 20500 Avg. Training loss: 2.8408 0.0153 sec/batch\n",
      "Global Step: 20600 Epoch 9/50 Iteration: 20600 Avg. Training loss: 2.8344 0.0136 sec/batch\n",
      "Global Step: 20700 Epoch 9/50 Iteration: 20700 Avg. Training loss: 2.8122 0.0140 sec/batch\n",
      "Global Step: 20800 Epoch 9/50 Iteration: 20800 Avg. Training loss: 2.8549 0.0149 sec/batch\n",
      "Global Step: 20900 Epoch 9/50 Iteration: 20900 Avg. Training loss: 2.8309 0.0150 sec/batch\n",
      "Global Step: 21000 Epoch 9/50 Iteration: 21000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 21100 Epoch 9/50 Iteration: 21100 Avg. Training loss: 2.8344 0.0172 sec/batch\n",
      "Global Step: 21200 Epoch 9/50 Iteration: 21200 Avg. Training loss: 2.8136 0.0154 sec/batch\n",
      "Global Step: 21300 Epoch 9/50 Iteration: 21300 Avg. Training loss: 2.8192 0.0151 sec/batch\n",
      "Global Step: 21400 Epoch 9/50 Iteration: 21400 Avg. Training loss: 2.8302 0.0147 sec/batch\n",
      "Epoch 10/50 Threshold: 0.08681111929218609 Length of Training words: 2427436\n",
      "Global Step: 21500 Epoch 10/50 Iteration: 21500 Avg. Training loss: 2.8344 0.0059 sec/batch\n",
      "Global Step: 21600 Epoch 10/50 Iteration: 21600 Avg. Training loss: 2.8455 0.0158 sec/batch\n",
      "Global Step: 21700 Epoch 10/50 Iteration: 21700 Avg. Training loss: 2.8662 0.0176 sec/batch\n",
      "Global Step: 21800 Epoch 10/50 Iteration: 21800 Avg. Training loss: 2.8459 0.0143 sec/batch\n",
      "Global Step: 21900 Epoch 10/50 Iteration: 21900 Avg. Training loss: 2.8286 0.0145 sec/batch\n",
      "Global Step: 22000 Epoch 10/50 Iteration: 22000 Avg. Training loss: 2.8245 0.0144 sec/batch\n",
      "Global Step: 22100 Epoch 10/50 Iteration: 22100 Avg. Training loss: 2.8291 0.0187 sec/batch\n",
      "Global Step: 22200 Epoch 10/50 Iteration: 22200 Avg. Training loss: 2.8406 0.0187 sec/batch\n",
      "Global Step: 22300 Epoch 10/50 Iteration: 22300 Avg. Training loss: 2.8366 0.0160 sec/batch\n",
      "Global Step: 22400 Epoch 10/50 Iteration: 22400 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 22500 Epoch 10/50 Iteration: 22500 Avg. Training loss: 2.8220 0.0154 sec/batch\n",
      "Global Step: 22600 Epoch 10/50 Iteration: 22600 Avg. Training loss: 2.8159 0.0148 sec/batch\n",
      "Global Step: 22700 Epoch 10/50 Iteration: 22700 Avg. Training loss: 2.8466 0.0155 sec/batch\n",
      "Global Step: 22800 Epoch 10/50 Iteration: 22800 Avg. Training loss: 2.8275 0.0137 sec/batch\n",
      "Global Step: 22900 Epoch 10/50 Iteration: 22900 Avg. Training loss: 2.8319 0.0152 sec/batch\n",
      "Global Step: 23000 Epoch 10/50 Iteration: 23000 Avg. Training loss: 2.8433 0.0153 sec/batch\n",
      "Global Step: 23100 Epoch 10/50 Iteration: 23100 Avg. Training loss: 2.8259 0.0154 sec/batch\n",
      "Global Step: 23200 Epoch 10/50 Iteration: 23200 Avg. Training loss: 2.8376 0.0145 sec/batch\n",
      "Global Step: 23300 Epoch 10/50 Iteration: 23300 Avg. Training loss: 2.8581 0.0167 sec/batch\n",
      "Global Step: 23400 Epoch 10/50 Iteration: 23400 Avg. Training loss: 2.8369 0.0155 sec/batch\n",
      "Global Step: 23500 Epoch 10/50 Iteration: 23500 Avg. Training loss: 2.8404 0.0117 sec/batch\n",
      "Global Step: 23600 Epoch 10/50 Iteration: 23600 Avg. Training loss: 2.8159 0.0146 sec/batch\n",
      "Global Step: 23700 Epoch 10/50 Iteration: 23700 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 23800 Epoch 10/50 Iteration: 23800 Avg. Training loss: 2.8264 0.0151 sec/batch\n",
      "Epoch 11/50 Threshold: 0.08986173649493497 Length of Training words: 2445824\n",
      "Global Step: 23900 Epoch 11/50 Iteration: 23900 Avg. Training loss: 2.8282 0.0015 sec/batch\n",
      "Global Step: 24000 Epoch 11/50 Iteration: 24000 Avg. Training loss: 2.8505 0.0152 sec/batch\n",
      "Global Step: 24100 Epoch 11/50 Iteration: 24100 Avg. Training loss: 2.8602 0.0140 sec/batch\n",
      "Global Step: 24200 Epoch 11/50 Iteration: 24200 Avg. Training loss: 2.8411 0.0137 sec/batch\n",
      "Global Step: 24300 Epoch 11/50 Iteration: 24300 Avg. Training loss: 2.8116 0.0134 sec/batch\n",
      "Global Step: 24400 Epoch 11/50 Iteration: 24400 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 24500 Epoch 11/50 Iteration: 24500 Avg. Training loss: 2.8330 0.0149 sec/batch\n",
      "Global Step: 24600 Epoch 11/50 Iteration: 24600 Avg. Training loss: 2.8196 0.0159 sec/batch\n",
      "Global Step: 24700 Epoch 11/50 Iteration: 24700 Avg. Training loss: 2.8562 0.0134 sec/batch\n",
      "Global Step: 24800 Epoch 11/50 Iteration: 24800 Avg. Training loss: 2.8342 0.0162 sec/batch\n",
      "Global Step: 24900 Epoch 11/50 Iteration: 24900 Avg. Training loss: 2.8242 0.0138 sec/batch\n",
      "Global Step: 25000 Epoch 11/50 Iteration: 25000 Avg. Training loss: 2.8181 0.0175 sec/batch\n",
      "Global Step: 25100 Epoch 11/50 Iteration: 25100 Avg. Training loss: 2.8224 0.0222 sec/batch\n",
      "Global Step: 25200 Epoch 11/50 Iteration: 25200 Avg. Training loss: 2.8228 0.0155 sec/batch\n",
      "Global Step: 25300 Epoch 11/50 Iteration: 25300 Avg. Training loss: 2.8374 0.0185 sec/batch\n",
      "Global Step: 25400 Epoch 11/50 Iteration: 25400 Avg. Training loss: 2.8314 0.0150 sec/batch\n",
      "Global Step: 25500 Epoch 11/50 Iteration: 25500 Avg. Training loss: 2.8323 0.0132 sec/batch\n",
      "Global Step: 25600 Epoch 11/50 Iteration: 25600 Avg. Training loss: 2.8125 0.0152 sec/batch\n",
      "Global Step: 25700 Epoch 11/50 Iteration: 25700 Avg. Training loss: 2.8541 0.0151 sec/batch\n",
      "Global Step: 25800 Epoch 11/50 Iteration: 25800 Avg. Training loss: 2.8278 0.0160 sec/batch\n",
      "Global Step: 25900 Epoch 11/50 Iteration: 25900 Avg. Training loss: 2.8493 0.0146 sec/batch\n",
      "Global Step: 26000 Epoch 11/50 Iteration: 26000 Avg. Training loss: 2.8121 0.0156 sec/batch\n",
      "Global Step: 26100 Epoch 11/50 Iteration: 26100 Avg. Training loss: 2.8029 0.0142 sec/batch\n",
      "Global Step: 26200 Epoch 11/50 Iteration: 26200 Avg. Training loss: 2.8370 0.0149 sec/batch\n",
      "Global Step: 26300 Epoch 11/50 Iteration: 26300 Avg. Training loss: 2.8203 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Threshold: 0.08110778818964093 Length of Training words: 2392894\n",
      "Global Step: 26400 Epoch 12/50 Iteration: 26400 Avg. Training loss: 2.8435 0.0091 sec/batch\n",
      "Global Step: 26500 Epoch 12/50 Iteration: 26500 Avg. Training loss: 2.8679 0.0172 sec/batch\n",
      "Global Step: 26600 Epoch 12/50 Iteration: 26600 Avg. Training loss: 2.8693 0.0159 sec/batch\n",
      "Global Step: 26700 Epoch 12/50 Iteration: 26700 Avg. Training loss: 2.8375 0.0153 sec/batch\n",
      "Global Step: 26800 Epoch 12/50 Iteration: 26800 Avg. Training loss: 2.8435 0.0164 sec/batch\n",
      "Global Step: 26900 Epoch 12/50 Iteration: 26900 Avg. Training loss: 2.8369 0.0145 sec/batch\n",
      "Global Step: 27000 Epoch 12/50 Iteration: 27000 Avg. Training loss: 2.8479 0.0164 sec/batch\n",
      "Global Step: 27100 Epoch 12/50 Iteration: 27100 Avg. Training loss: 2.8547 0.0155 sec/batch\n",
      "Global Step: 27200 Epoch 12/50 Iteration: 27200 Avg. Training loss: 2.8543 0.0150 sec/batch\n",
      "Global Step: 27300 Epoch 12/50 Iteration: 27300 Avg. Training loss: 2.8543 0.0140 sec/batch\n",
      "Global Step: 27400 Epoch 12/50 Iteration: 27400 Avg. Training loss: 2.8292 0.0176 sec/batch\n",
      "Global Step: 27500 Epoch 12/50 Iteration: 27500 Avg. Training loss: 2.8413 0.0163 sec/batch\n",
      "Global Step: 27600 Epoch 12/50 Iteration: 27600 Avg. Training loss: 2.8461 0.0153 sec/batch\n",
      "Global Step: 27700 Epoch 12/50 Iteration: 27700 Avg. Training loss: 2.8538 0.0149 sec/batch\n",
      "Global Step: 27800 Epoch 12/50 Iteration: 27800 Avg. Training loss: 2.8460 0.0162 sec/batch\n",
      "Global Step: 27900 Epoch 12/50 Iteration: 27900 Avg. Training loss: 2.8436 0.0162 sec/batch\n",
      "Global Step: 28000 Epoch 12/50 Iteration: 28000 Avg. Training loss: 2.8351 0.0163 sec/batch\n",
      "Global Step: 28100 Epoch 12/50 Iteration: 28100 Avg. Training loss: 2.8640 0.0164 sec/batch\n",
      "Global Step: 28200 Epoch 12/50 Iteration: 28200 Avg. Training loss: 2.8487 0.0128 sec/batch\n",
      "Global Step: 28300 Epoch 12/50 Iteration: 28300 Avg. Training loss: 2.8673 0.0152 sec/batch\n",
      "Global Step: 28400 Epoch 12/50 Iteration: 28400 Avg. Training loss: 2.8257 0.0147 sec/batch\n",
      "Global Step: 28500 Epoch 12/50 Iteration: 28500 Avg. Training loss: 2.8200 0.0152 sec/batch\n",
      "Global Step: 28600 Epoch 12/50 Iteration: 28600 Avg. Training loss: 2.8543 0.0156 sec/batch\n",
      "Global Step: 28700 Epoch 12/50 Iteration: 28700 Avg. Training loss: 2.8426 0.0189 sec/batch\n",
      "Epoch 13/50 Threshold: 0.08717083632765375 Length of Training words: 2430009\n",
      "Global Step: 28800 Epoch 13/50 Iteration: 28800 Avg. Training loss: 2.8433 0.0119 sec/batch\n",
      "Global Step: 28900 Epoch 13/50 Iteration: 28900 Avg. Training loss: 2.8544 0.0155 sec/batch\n",
      "Global Step: 29000 Epoch 13/50 Iteration: 29000 Avg. Training loss: 2.8561 0.0183 sec/batch\n",
      "Global Step: 29100 Epoch 13/50 Iteration: 29100 Avg. Training loss: 2.8250 0.0165 sec/batch\n",
      "Global Step: 29200 Epoch 13/50 Iteration: 29200 Avg. Training loss: 2.8316 0.0147 sec/batch\n",
      "Global Step: 29300 Epoch 13/50 Iteration: 29300 Avg. Training loss: 2.8254 0.0164 sec/batch\n",
      "Global Step: 29400 Epoch 13/50 Iteration: 29400 Avg. Training loss: 2.8380 0.0132 sec/batch\n",
      "Global Step: 29500 Epoch 13/50 Iteration: 29500 Avg. Training loss: 2.8419 0.0152 sec/batch\n",
      "Global Step: 29600 Epoch 13/50 Iteration: 29600 Avg. Training loss: 2.8412 0.0165 sec/batch\n",
      "Global Step: 29700 Epoch 13/50 Iteration: 29700 Avg. Training loss: 2.8453 0.0150 sec/batch\n",
      "Global Step: 29800 Epoch 13/50 Iteration: 29800 Avg. Training loss: 2.8164 0.0160 sec/batch\n",
      "Global Step: 29900 Epoch 13/50 Iteration: 29900 Avg. Training loss: 2.8203 0.0165 sec/batch\n",
      "Global Step: 30000 Epoch 13/50 Iteration: 30000 Avg. Training loss: 2.8475 0.0163 sec/batch\n",
      "Global Step: 30100 Epoch 13/50 Iteration: 30100 Avg. Training loss: 2.8233 0.0131 sec/batch\n",
      "Global Step: 30200 Epoch 13/50 Iteration: 30200 Avg. Training loss: 2.8436 0.0148 sec/batch\n",
      "Global Step: 30300 Epoch 13/50 Iteration: 30300 Avg. Training loss: 2.8325 0.0129 sec/batch\n",
      "Global Step: 30400 Epoch 13/50 Iteration: 30400 Avg. Training loss: 2.8146 0.0169 sec/batch\n",
      "Global Step: 30500 Epoch 13/50 Iteration: 30500 Avg. Training loss: 2.8589 0.0126 sec/batch\n",
      "Global Step: 30600 Epoch 13/50 Iteration: 30600 Avg. Training loss: 2.8321 0.0148 sec/batch\n",
      "Global Step: 30700 Epoch 13/50 Iteration: 30700 Avg. Training loss: 2.8510 0.0167 sec/batch\n",
      "Global Step: 30800 Epoch 13/50 Iteration: 30800 Avg. Training loss: 2.8292 0.0177 sec/batch\n",
      "Global Step: 30900 Epoch 13/50 Iteration: 30900 Avg. Training loss: 2.8177 0.0145 sec/batch\n",
      "Global Step: 31000 Epoch 13/50 Iteration: 31000 Avg. Training loss: 2.8219 0.0189 sec/batch\n",
      "Global Step: 31100 Epoch 13/50 Iteration: 31100 Avg. Training loss: 2.8284 0.0155 sec/batch\n",
      "Epoch 14/50 Threshold: 0.07736795802688062 Length of Training words: 2369330\n",
      "Global Step: 31200 Epoch 14/50 Iteration: 31200 Avg. Training loss: 2.8465 0.0079 sec/batch\n",
      "Global Step: 31300 Epoch 14/50 Iteration: 31300 Avg. Training loss: 2.8703 0.0140 sec/batch\n",
      "Global Step: 31400 Epoch 14/50 Iteration: 31400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 31500 Epoch 14/50 Iteration: 31500 Avg. Training loss: 2.8589 0.0144 sec/batch\n",
      "Global Step: 31600 Epoch 14/50 Iteration: 31600 Avg. Training loss: 2.8476 0.0167 sec/batch\n",
      "Global Step: 31700 Epoch 14/50 Iteration: 31700 Avg. Training loss: 2.8344 0.0144 sec/batch\n",
      "Global Step: 31800 Epoch 14/50 Iteration: 31800 Avg. Training loss: 2.8710 0.0176 sec/batch\n",
      "Global Step: 31900 Epoch 14/50 Iteration: 31900 Avg. Training loss: 2.8550 0.0190 sec/batch\n",
      "Global Step: 32000 Epoch 14/50 Iteration: 32000 Avg. Training loss: 2.8547 0.0152 sec/batch\n",
      "Global Step: 32100 Epoch 14/50 Iteration: 32100 Avg. Training loss: 2.8728 0.0167 sec/batch\n",
      "Global Step: 32200 Epoch 14/50 Iteration: 32200 Avg. Training loss: 2.8362 0.0161 sec/batch\n",
      "Global Step: 32300 Epoch 14/50 Iteration: 32300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 32400 Epoch 14/50 Iteration: 32400 Avg. Training loss: 2.8629 0.0148 sec/batch\n",
      "Global Step: 32500 Epoch 14/50 Iteration: 32500 Avg. Training loss: 2.8484 0.0145 sec/batch\n",
      "Global Step: 32600 Epoch 14/50 Iteration: 32600 Avg. Training loss: 2.8589 0.0150 sec/batch\n",
      "Global Step: 32700 Epoch 14/50 Iteration: 32700 Avg. Training loss: 2.8483 0.0118 sec/batch\n",
      "Global Step: 32800 Epoch 14/50 Iteration: 32800 Avg. Training loss: 2.8380 0.0158 sec/batch\n",
      "Global Step: 32900 Epoch 14/50 Iteration: 32900 Avg. Training loss: 2.8775 0.0144 sec/batch\n",
      "Global Step: 33000 Epoch 14/50 Iteration: 33000 Avg. Training loss: 2.8572 0.0178 sec/batch\n",
      "Global Step: 33100 Epoch 14/50 Iteration: 33100 Avg. Training loss: 2.8785 0.0160 sec/batch\n",
      "Global Step: 33200 Epoch 14/50 Iteration: 33200 Avg. Training loss: 2.8371 0.0187 sec/batch\n",
      "Global Step: 33300 Epoch 14/50 Iteration: 33300 Avg. Training loss: 2.8276 0.0170 sec/batch\n",
      "Global Step: 33400 Epoch 14/50 Iteration: 33400 Avg. Training loss: 2.8591 0.0198 sec/batch\n",
      "Global Step: 33500 Epoch 14/50 Iteration: 33500 Avg. Training loss: 2.8482 0.0157 sec/batch\n",
      "Epoch 15/50 Threshold: 0.07867502562390928 Length of Training words: 2377258\n",
      "Global Step: 33600 Epoch 15/50 Iteration: 33600 Avg. Training loss: 2.8611 0.0127 sec/batch\n",
      "Global Step: 33700 Epoch 15/50 Iteration: 33700 Avg. Training loss: 2.8720 0.0163 sec/batch\n",
      "Global Step: 33800 Epoch 15/50 Iteration: 33800 Avg. Training loss: 2.8717 0.0160 sec/batch\n",
      "Global Step: 33900 Epoch 15/50 Iteration: 33900 Avg. Training loss: 2.8478 0.0143 sec/batch\n",
      "Global Step: 34000 Epoch 15/50 Iteration: 34000 Avg. Training loss: 2.8473 0.0149 sec/batch\n",
      "Global Step: 34100 Epoch 15/50 Iteration: 34100 Avg. Training loss: 2.8435 0.0159 sec/batch\n",
      "Global Step: 34200 Epoch 15/50 Iteration: 34200 Avg. Training loss: 2.8486 0.0162 sec/batch\n",
      "Global Step: 34300 Epoch 15/50 Iteration: 34300 Avg. Training loss: 2.8660 0.0167 sec/batch\n",
      "Global Step: 34400 Epoch 15/50 Iteration: 34400 Avg. Training loss: 2.8560 0.0142 sec/batch\n",
      "Global Step: 34500 Epoch 15/50 Iteration: 34500 Avg. Training loss: 2.8551 0.0142 sec/batch\n",
      "Global Step: 34600 Epoch 15/50 Iteration: 34600 Avg. Training loss: 2.8281 0.0140 sec/batch\n",
      "Global Step: 34700 Epoch 15/50 Iteration: 34700 Avg. Training loss: 2.8506 0.0139 sec/batch\n",
      "Global Step: 34800 Epoch 15/50 Iteration: 34800 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 34900 Epoch 15/50 Iteration: 34900 Avg. Training loss: 2.8578 0.0156 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 35000 Epoch 15/50 Iteration: 35000 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 35100 Epoch 15/50 Iteration: 35100 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 35200 Epoch 15/50 Iteration: 35200 Avg. Training loss: 2.8403 0.0138 sec/batch\n",
      "Global Step: 35300 Epoch 15/50 Iteration: 35300 Avg. Training loss: 2.8762 0.0164 sec/batch\n",
      "Global Step: 35400 Epoch 15/50 Iteration: 35400 Avg. Training loss: 2.8581 0.0161 sec/batch\n",
      "Global Step: 35500 Epoch 15/50 Iteration: 35500 Avg. Training loss: 2.8638 0.0139 sec/batch\n",
      "Global Step: 35600 Epoch 15/50 Iteration: 35600 Avg. Training loss: 2.8266 0.0152 sec/batch\n",
      "Global Step: 35700 Epoch 15/50 Iteration: 35700 Avg. Training loss: 2.8329 0.0153 sec/batch\n",
      "Global Step: 35800 Epoch 15/50 Iteration: 35800 Avg. Training loss: 2.8430 0.0177 sec/batch\n",
      "Global Step: 35900 Epoch 15/50 Iteration: 35900 Avg. Training loss: 2.8503 0.0130 sec/batch\n",
      "Epoch 16/50 Threshold: 0.0720019299680255 Length of Training words: 2329165\n",
      "Global Step: 36000 Epoch 16/50 Iteration: 36000 Avg. Training loss: 2.8825 0.0148 sec/batch\n",
      "Global Step: 36100 Epoch 16/50 Iteration: 36100 Avg. Training loss: 2.8978 0.0151 sec/batch\n",
      "Global Step: 36200 Epoch 16/50 Iteration: 36200 Avg. Training loss: 2.8758 0.0147 sec/batch\n",
      "Global Step: 36300 Epoch 16/50 Iteration: 36300 Avg. Training loss: 2.8573 0.0144 sec/batch\n",
      "Global Step: 36400 Epoch 16/50 Iteration: 36400 Avg. Training loss: 2.8587 0.0150 sec/batch\n",
      "Global Step: 36500 Epoch 16/50 Iteration: 36500 Avg. Training loss: 2.8645 0.0132 sec/batch\n",
      "Global Step: 36600 Epoch 16/50 Iteration: 36600 Avg. Training loss: 2.8690 0.0175 sec/batch\n",
      "Global Step: 36700 Epoch 16/50 Iteration: 36700 Avg. Training loss: 2.8724 0.0186 sec/batch\n",
      "Global Step: 36800 Epoch 16/50 Iteration: 36800 Avg. Training loss: 2.8878 0.0156 sec/batch\n",
      "Global Step: 36900 Epoch 16/50 Iteration: 36900 Avg. Training loss: 2.8542 0.0145 sec/batch\n",
      "Global Step: 37000 Epoch 16/50 Iteration: 37000 Avg. Training loss: 2.8491 0.0160 sec/batch\n",
      "Global Step: 37100 Epoch 16/50 Iteration: 37100 Avg. Training loss: 2.8816 0.0162 sec/batch\n",
      "Global Step: 37200 Epoch 16/50 Iteration: 37200 Avg. Training loss: 2.8555 0.0143 sec/batch\n",
      "Global Step: 37300 Epoch 16/50 Iteration: 37300 Avg. Training loss: 2.8749 0.0158 sec/batch\n",
      "Global Step: 37400 Epoch 16/50 Iteration: 37400 Avg. Training loss: 2.8679 0.0150 sec/batch\n",
      "Global Step: 37500 Epoch 16/50 Iteration: 37500 Avg. Training loss: 2.8492 0.0180 sec/batch\n",
      "Global Step: 37600 Epoch 16/50 Iteration: 37600 Avg. Training loss: 2.8883 0.0165 sec/batch\n",
      "Global Step: 37700 Epoch 16/50 Iteration: 37700 Avg. Training loss: 2.8683 0.0145 sec/batch\n",
      "Global Step: 37800 Epoch 16/50 Iteration: 37800 Avg. Training loss: 2.8865 0.0156 sec/batch\n",
      "Global Step: 37900 Epoch 16/50 Iteration: 37900 Avg. Training loss: 2.8610 0.0161 sec/batch\n",
      "Global Step: 38000 Epoch 16/50 Iteration: 38000 Avg. Training loss: 2.8436 0.0163 sec/batch\n",
      "Global Step: 38100 Epoch 16/50 Iteration: 38100 Avg. Training loss: 2.8677 0.0152 sec/batch\n",
      "Global Step: 38200 Epoch 16/50 Iteration: 38200 Avg. Training loss: 2.8611 0.0151 sec/batch\n",
      "Epoch 17/50 Threshold: 0.08720321176489212 Length of Training words: 2429828\n",
      "Global Step: 38300 Epoch 17/50 Iteration: 38300 Avg. Training loss: 2.8484 0.0092 sec/batch\n",
      "Global Step: 38400 Epoch 17/50 Iteration: 38400 Avg. Training loss: 2.8508 0.0141 sec/batch\n",
      "Global Step: 38500 Epoch 17/50 Iteration: 38500 Avg. Training loss: 2.8600 0.0147 sec/batch\n",
      "Global Step: 38600 Epoch 17/50 Iteration: 38600 Avg. Training loss: 2.8243 0.0113 sec/batch\n",
      "Global Step: 38700 Epoch 17/50 Iteration: 38700 Avg. Training loss: 2.8304 0.0119 sec/batch\n",
      "Global Step: 38800 Epoch 17/50 Iteration: 38800 Avg. Training loss: 2.8248 0.0125 sec/batch\n",
      "Global Step: 38900 Epoch 17/50 Iteration: 38900 Avg. Training loss: 2.8357 0.0140 sec/batch\n",
      "Global Step: 39000 Epoch 17/50 Iteration: 39000 Avg. Training loss: 2.8408 0.0191 sec/batch\n",
      "Global Step: 39100 Epoch 17/50 Iteration: 39100 Avg. Training loss: 2.8364 0.0169 sec/batch\n",
      "Global Step: 39200 Epoch 17/50 Iteration: 39200 Avg. Training loss: 2.8535 0.0160 sec/batch\n",
      "Global Step: 39300 Epoch 17/50 Iteration: 39300 Avg. Training loss: 2.8166 0.0167 sec/batch\n",
      "Global Step: 39400 Epoch 17/50 Iteration: 39400 Avg. Training loss: 2.8239 0.0158 sec/batch\n",
      "Global Step: 39500 Epoch 17/50 Iteration: 39500 Avg. Training loss: 2.8466 0.0130 sec/batch\n",
      "Global Step: 39600 Epoch 17/50 Iteration: 39600 Avg. Training loss: 2.8177 0.0142 sec/batch\n",
      "Global Step: 39700 Epoch 17/50 Iteration: 39700 Avg. Training loss: 2.8436 0.0166 sec/batch\n",
      "Global Step: 39800 Epoch 17/50 Iteration: 39800 Avg. Training loss: 2.8375 0.0138 sec/batch\n",
      "Global Step: 39900 Epoch 17/50 Iteration: 39900 Avg. Training loss: 2.8115 0.0134 sec/batch\n",
      "Global Step: 40000 Epoch 17/50 Iteration: 40000 Avg. Training loss: 2.8589 0.0158 sec/batch\n",
      "Global Step: 40100 Epoch 17/50 Iteration: 40100 Avg. Training loss: 2.8330 0.0148 sec/batch\n",
      "Global Step: 40200 Epoch 17/50 Iteration: 40200 Avg. Training loss: 2.8518 0.0157 sec/batch\n",
      "Global Step: 40300 Epoch 17/50 Iteration: 40300 Avg. Training loss: 2.8338 0.0133 sec/batch\n",
      "Global Step: 40400 Epoch 17/50 Iteration: 40400 Avg. Training loss: 2.8156 0.0137 sec/batch\n",
      "Global Step: 40500 Epoch 17/50 Iteration: 40500 Avg. Training loss: 2.8206 0.0139 sec/batch\n",
      "Global Step: 40600 Epoch 17/50 Iteration: 40600 Avg. Training loss: 2.8285 0.0118 sec/batch\n",
      "Epoch 18/50 Threshold: 0.07863276721785589 Length of Training words: 2376633\n",
      "Global Step: 40700 Epoch 18/50 Iteration: 40700 Avg. Training loss: 2.8469 0.0056 sec/batch\n",
      "Global Step: 40800 Epoch 18/50 Iteration: 40800 Avg. Training loss: 2.8651 0.0117 sec/batch\n",
      "Global Step: 40900 Epoch 18/50 Iteration: 40900 Avg. Training loss: 2.8781 0.0132 sec/batch\n",
      "Global Step: 41000 Epoch 18/50 Iteration: 41000 Avg. Training loss: 2.8563 0.0117 sec/batch\n",
      "Global Step: 41100 Epoch 18/50 Iteration: 41100 Avg. Training loss: 2.8441 0.0126 sec/batch\n",
      "Global Step: 41200 Epoch 18/50 Iteration: 41200 Avg. Training loss: 2.8358 0.0129 sec/batch\n",
      "Global Step: 41300 Epoch 18/50 Iteration: 41300 Avg. Training loss: 2.8593 0.0126 sec/batch\n",
      "Global Step: 41400 Epoch 18/50 Iteration: 41400 Avg. Training loss: 2.8597 0.0126 sec/batch\n",
      "Global Step: 41500 Epoch 18/50 Iteration: 41500 Avg. Training loss: 2.8469 0.0143 sec/batch\n",
      "Global Step: 41600 Epoch 18/50 Iteration: 41600 Avg. Training loss: 2.8755 0.0138 sec/batch\n",
      "Global Step: 41700 Epoch 18/50 Iteration: 41700 Avg. Training loss: 2.8338 0.0155 sec/batch\n",
      "Global Step: 41800 Epoch 18/50 Iteration: 41800 Avg. Training loss: 2.8415 0.0143 sec/batch\n",
      "Global Step: 41900 Epoch 18/50 Iteration: 41900 Avg. Training loss: 2.8631 0.0147 sec/batch\n",
      "Global Step: 42000 Epoch 18/50 Iteration: 42000 Avg. Training loss: 2.8376 0.0160 sec/batch\n",
      "Global Step: 42100 Epoch 18/50 Iteration: 42100 Avg. Training loss: 2.8589 0.0148 sec/batch\n",
      "Global Step: 42200 Epoch 18/50 Iteration: 42200 Avg. Training loss: 2.8495 0.0156 sec/batch\n",
      "Global Step: 42300 Epoch 18/50 Iteration: 42300 Avg. Training loss: 2.8306 0.0156 sec/batch\n",
      "Global Step: 42400 Epoch 18/50 Iteration: 42400 Avg. Training loss: 2.8768 0.0153 sec/batch\n",
      "Global Step: 42500 Epoch 18/50 Iteration: 42500 Avg. Training loss: 2.8515 0.0144 sec/batch\n",
      "Global Step: 42600 Epoch 18/50 Iteration: 42600 Avg. Training loss: 2.8700 0.0133 sec/batch\n",
      "Global Step: 42700 Epoch 18/50 Iteration: 42700 Avg. Training loss: 2.8418 0.0129 sec/batch\n",
      "Global Step: 42800 Epoch 18/50 Iteration: 42800 Avg. Training loss: 2.8286 0.0127 sec/batch\n",
      "Global Step: 42900 Epoch 18/50 Iteration: 42900 Avg. Training loss: 2.8542 0.0137 sec/batch\n",
      "Global Step: 43000 Epoch 18/50 Iteration: 43000 Avg. Training loss: 2.8467 0.0126 sec/batch\n",
      "Epoch 19/50 Threshold: 0.084364621536794 Length of Training words: 2411957\n",
      "Global Step: 43100 Epoch 19/50 Iteration: 43100 Avg. Training loss: 2.8475 0.0077 sec/batch\n",
      "Global Step: 43200 Epoch 19/50 Iteration: 43200 Avg. Training loss: 2.8590 0.0144 sec/batch\n",
      "Global Step: 43300 Epoch 19/50 Iteration: 43300 Avg. Training loss: 2.8653 0.0121 sec/batch\n",
      "Global Step: 43400 Epoch 19/50 Iteration: 43400 Avg. Training loss: 2.8360 0.0134 sec/batch\n",
      "Global Step: 43500 Epoch 19/50 Iteration: 43500 Avg. Training loss: 2.8348 0.0142 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 43600 Epoch 19/50 Iteration: 43600 Avg. Training loss: 2.8298 0.0130 sec/batch\n",
      "Global Step: 43700 Epoch 19/50 Iteration: 43700 Avg. Training loss: 2.8439 0.0126 sec/batch\n",
      "Global Step: 43800 Epoch 19/50 Iteration: 43800 Avg. Training loss: 2.8480 0.0170 sec/batch\n",
      "Global Step: 43900 Epoch 19/50 Iteration: 43900 Avg. Training loss: 2.8422 0.0172 sec/batch\n",
      "Global Step: 44000 Epoch 19/50 Iteration: 44000 Avg. Training loss: 2.8582 0.0153 sec/batch\n",
      "Global Step: 44100 Epoch 19/50 Iteration: 44100 Avg. Training loss: 2.8216 0.0152 sec/batch\n",
      "Global Step: 44200 Epoch 19/50 Iteration: 44200 Avg. Training loss: 2.8287 0.0126 sec/batch\n",
      "Global Step: 44300 Epoch 19/50 Iteration: 44300 Avg. Training loss: 2.8507 0.0135 sec/batch\n",
      "Global Step: 44400 Epoch 19/50 Iteration: 44400 Avg. Training loss: 2.8275 0.0149 sec/batch\n",
      "Global Step: 44500 Epoch 19/50 Iteration: 44500 Avg. Training loss: 2.8478 0.0157 sec/batch\n",
      "Global Step: 44600 Epoch 19/50 Iteration: 44600 Avg. Training loss: 2.8371 0.0156 sec/batch\n",
      "Global Step: 44700 Epoch 19/50 Iteration: 44700 Avg. Training loss: 2.8216 0.0141 sec/batch\n",
      "Global Step: 44800 Epoch 19/50 Iteration: 44800 Avg. Training loss: 2.8652 0.0153 sec/batch\n",
      "Global Step: 44900 Epoch 19/50 Iteration: 44900 Avg. Training loss: 2.8342 0.0136 sec/batch\n",
      "Global Step: 45000 Epoch 19/50 Iteration: 45000 Avg. Training loss: 2.8565 0.0120 sec/batch\n",
      "Global Step: 45100 Epoch 19/50 Iteration: 45100 Avg. Training loss: 2.8320 0.0129 sec/batch\n",
      "Global Step: 45200 Epoch 19/50 Iteration: 45200 Avg. Training loss: 2.8217 0.0132 sec/batch\n",
      "Global Step: 45300 Epoch 19/50 Iteration: 45300 Avg. Training loss: 2.8350 0.0144 sec/batch\n",
      "Global Step: 45400 Epoch 19/50 Iteration: 45400 Avg. Training loss: 2.8393 0.0121 sec/batch\n",
      "Epoch 20/50 Threshold: 0.06092402220480751 Length of Training words: 2238418\n",
      "Global Step: 45500 Epoch 20/50 Iteration: 45500 Avg. Training loss: 2.8707 0.0072 sec/batch\n",
      "Global Step: 45600 Epoch 20/50 Iteration: 45600 Avg. Training loss: 2.9149 0.0124 sec/batch\n",
      "Global Step: 45700 Epoch 20/50 Iteration: 45700 Avg. Training loss: 2.9167 0.0126 sec/batch\n",
      "Global Step: 45800 Epoch 20/50 Iteration: 45800 Avg. Training loss: 2.8921 0.0117 sec/batch\n",
      "Global Step: 45900 Epoch 20/50 Iteration: 45900 Avg. Training loss: 2.8866 0.0116 sec/batch\n",
      "Global Step: 46000 Epoch 20/50 Iteration: 46000 Avg. Training loss: 2.8914 0.0139 sec/batch\n",
      "Global Step: 46100 Epoch 20/50 Iteration: 46100 Avg. Training loss: 2.8929 0.0158 sec/batch\n",
      "Global Step: 46200 Epoch 20/50 Iteration: 46200 Avg. Training loss: 2.9094 0.0138 sec/batch\n",
      "Global Step: 46300 Epoch 20/50 Iteration: 46300 Avg. Training loss: 2.9070 0.0130 sec/batch\n",
      "Global Step: 46400 Epoch 20/50 Iteration: 46400 Avg. Training loss: 2.8830 0.0156 sec/batch\n",
      "Global Step: 46500 Epoch 20/50 Iteration: 46500 Avg. Training loss: 2.8782 0.0147 sec/batch\n",
      "Global Step: 46600 Epoch 20/50 Iteration: 46600 Avg. Training loss: 2.9095 0.0153 sec/batch\n",
      "Global Step: 46700 Epoch 20/50 Iteration: 46700 Avg. Training loss: 2.8839 0.0135 sec/batch\n",
      "Global Step: 46800 Epoch 20/50 Iteration: 46800 Avg. Training loss: 2.9048 0.0163 sec/batch\n",
      "Global Step: 46900 Epoch 20/50 Iteration: 46900 Avg. Training loss: 2.8951 0.0152 sec/batch\n",
      "Global Step: 47000 Epoch 20/50 Iteration: 47000 Avg. Training loss: 2.8764 0.0155 sec/batch\n",
      "Global Step: 47100 Epoch 20/50 Iteration: 47100 Avg. Training loss: 2.9205 0.0149 sec/batch\n",
      "Global Step: 47200 Epoch 20/50 Iteration: 47200 Avg. Training loss: 2.8997 0.0120 sec/batch\n",
      "Global Step: 47300 Epoch 20/50 Iteration: 47300 Avg. Training loss: 2.9113 0.0122 sec/batch\n",
      "Global Step: 47400 Epoch 20/50 Iteration: 47400 Avg. Training loss: 2.8768 0.0139 sec/batch\n",
      "Global Step: 47500 Epoch 20/50 Iteration: 47500 Avg. Training loss: 2.8812 0.0124 sec/batch\n",
      "Global Step: 47600 Epoch 20/50 Iteration: 47600 Avg. Training loss: 2.8851 0.0131 sec/batch\n",
      "Epoch 21/50 Threshold: 0.06706143014707991 Length of Training words: 2289788\n",
      "Global Step: 47700 Epoch 21/50 Iteration: 47700 Avg. Training loss: 2.8957 0.0020 sec/batch\n",
      "Global Step: 47800 Epoch 21/50 Iteration: 47800 Avg. Training loss: 2.8904 0.0126 sec/batch\n",
      "Global Step: 47900 Epoch 21/50 Iteration: 47900 Avg. Training loss: 2.9109 0.0124 sec/batch\n",
      "Global Step: 48000 Epoch 21/50 Iteration: 48000 Avg. Training loss: 2.8883 0.0127 sec/batch\n",
      "Global Step: 48100 Epoch 21/50 Iteration: 48100 Avg. Training loss: 2.8717 0.0141 sec/batch\n",
      "Global Step: 48200 Epoch 21/50 Iteration: 48200 Avg. Training loss: 2.8689 0.0142 sec/batch\n",
      "Global Step: 48300 Epoch 21/50 Iteration: 48300 Avg. Training loss: 2.8858 0.0149 sec/batch\n",
      "Global Step: 48400 Epoch 21/50 Iteration: 48400 Avg. Training loss: 2.8819 0.0152 sec/batch\n",
      "Global Step: 48500 Epoch 21/50 Iteration: 48500 Avg. Training loss: 2.8791 0.0160 sec/batch\n",
      "Global Step: 48600 Epoch 21/50 Iteration: 48600 Avg. Training loss: 2.8976 0.0150 sec/batch\n",
      "Global Step: 48700 Epoch 21/50 Iteration: 48700 Avg. Training loss: 2.8656 0.0150 sec/batch\n",
      "Global Step: 48800 Epoch 21/50 Iteration: 48800 Avg. Training loss: 2.8699 0.0140 sec/batch\n",
      "Global Step: 48900 Epoch 21/50 Iteration: 48900 Avg. Training loss: 2.8773 0.0165 sec/batch\n",
      "Global Step: 49000 Epoch 21/50 Iteration: 49000 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 49100 Epoch 21/50 Iteration: 49100 Avg. Training loss: 2.8780 0.0171 sec/batch\n",
      "Global Step: 49200 Epoch 21/50 Iteration: 49200 Avg. Training loss: 2.8829 0.0150 sec/batch\n",
      "Global Step: 49300 Epoch 21/50 Iteration: 49300 Avg. Training loss: 2.8680 0.0130 sec/batch\n",
      "Global Step: 49400 Epoch 21/50 Iteration: 49400 Avg. Training loss: 2.9020 0.0130 sec/batch\n",
      "Global Step: 49500 Epoch 21/50 Iteration: 49500 Avg. Training loss: 2.8804 0.0127 sec/batch\n",
      "Global Step: 49600 Epoch 21/50 Iteration: 49600 Avg. Training loss: 2.8893 0.0136 sec/batch\n",
      "Global Step: 49700 Epoch 21/50 Iteration: 49700 Avg. Training loss: 2.8556 0.0127 sec/batch\n",
      "Global Step: 49800 Epoch 21/50 Iteration: 49800 Avg. Training loss: 2.8666 0.0138 sec/batch\n",
      "Global Step: 49900 Epoch 21/50 Iteration: 49900 Avg. Training loss: 2.8745 0.0135 sec/batch\n",
      "Epoch 22/50 Threshold: 0.07384798892845924 Length of Training words: 2344481\n",
      "Global Step: 50000 Epoch 22/50 Iteration: 50000 Avg. Training loss: 2.8710 0.0043 sec/batch\n",
      "Global Step: 50100 Epoch 22/50 Iteration: 50100 Avg. Training loss: 2.8709 0.0139 sec/batch\n",
      "Global Step: 50200 Epoch 22/50 Iteration: 50200 Avg. Training loss: 2.8957 0.0150 sec/batch\n",
      "Global Step: 50300 Epoch 22/50 Iteration: 50300 Avg. Training loss: 2.8718 0.0127 sec/batch\n",
      "Global Step: 50400 Epoch 22/50 Iteration: 50400 Avg. Training loss: 2.8550 0.0156 sec/batch\n",
      "Global Step: 50500 Epoch 22/50 Iteration: 50500 Avg. Training loss: 2.8489 0.0155 sec/batch\n",
      "Global Step: 50600 Epoch 22/50 Iteration: 50600 Avg. Training loss: 2.8670 0.0153 sec/batch\n",
      "Global Step: 50700 Epoch 22/50 Iteration: 50700 Avg. Training loss: 2.8698 0.0156 sec/batch\n",
      "Global Step: 50800 Epoch 22/50 Iteration: 50800 Avg. Training loss: 2.8572 0.0148 sec/batch\n",
      "Global Step: 50900 Epoch 22/50 Iteration: 50900 Avg. Training loss: 2.8870 0.0133 sec/batch\n",
      "Global Step: 51000 Epoch 22/50 Iteration: 51000 Avg. Training loss: 2.8440 0.0124 sec/batch\n",
      "Global Step: 51100 Epoch 22/50 Iteration: 51100 Avg. Training loss: 2.8533 0.0152 sec/batch\n",
      "Global Step: 51200 Epoch 22/50 Iteration: 51200 Avg. Training loss: 2.8699 0.0147 sec/batch\n",
      "Global Step: 51300 Epoch 22/50 Iteration: 51300 Avg. Training loss: 2.8494 0.0143 sec/batch\n",
      "Global Step: 51400 Epoch 22/50 Iteration: 51400 Avg. Training loss: 2.8701 0.0146 sec/batch\n",
      "Global Step: 51500 Epoch 22/50 Iteration: 51500 Avg. Training loss: 2.8587 0.0146 sec/batch\n",
      "Global Step: 51600 Epoch 22/50 Iteration: 51600 Avg. Training loss: 2.8490 0.0124 sec/batch\n",
      "Global Step: 51700 Epoch 22/50 Iteration: 51700 Avg. Training loss: 2.8839 0.0129 sec/batch\n",
      "Global Step: 51800 Epoch 22/50 Iteration: 51800 Avg. Training loss: 2.8656 0.0130 sec/batch\n",
      "Global Step: 51900 Epoch 22/50 Iteration: 51900 Avg. Training loss: 2.8848 0.0125 sec/batch\n",
      "Global Step: 52000 Epoch 22/50 Iteration: 52000 Avg. Training loss: 2.8423 0.0138 sec/batch\n",
      "Global Step: 52100 Epoch 22/50 Iteration: 52100 Avg. Training loss: 2.8388 0.0139 sec/batch\n",
      "Global Step: 52200 Epoch 22/50 Iteration: 52200 Avg. Training loss: 2.8699 0.0129 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 52300 Epoch 22/50 Iteration: 52300 Avg. Training loss: 2.8550 0.0132 sec/batch\n",
      "Epoch 23/50 Threshold: 0.07978773587287813 Length of Training words: 2383586\n",
      "Global Step: 52400 Epoch 23/50 Iteration: 52400 Avg. Training loss: 2.8550 0.0097 sec/batch\n",
      "Global Step: 52500 Epoch 23/50 Iteration: 52500 Avg. Training loss: 2.8775 0.0147 sec/batch\n",
      "Global Step: 52600 Epoch 23/50 Iteration: 52600 Avg. Training loss: 2.8682 0.0155 sec/batch\n",
      "Global Step: 52700 Epoch 23/50 Iteration: 52700 Avg. Training loss: 2.8443 0.0162 sec/batch\n",
      "Global Step: 52800 Epoch 23/50 Iteration: 52800 Avg. Training loss: 2.8399 0.0147 sec/batch\n",
      "Global Step: 52900 Epoch 23/50 Iteration: 52900 Avg. Training loss: 2.8438 0.0173 sec/batch\n",
      "Global Step: 53000 Epoch 23/50 Iteration: 53000 Avg. Training loss: 2.8473 0.0147 sec/batch\n",
      "Global Step: 53100 Epoch 23/50 Iteration: 53100 Avg. Training loss: 2.8656 0.0167 sec/batch\n",
      "Global Step: 53200 Epoch 23/50 Iteration: 53200 Avg. Training loss: 2.8504 0.0156 sec/batch\n",
      "Global Step: 53300 Epoch 23/50 Iteration: 53300 Avg. Training loss: 2.8504 0.0166 sec/batch\n",
      "Global Step: 53400 Epoch 23/50 Iteration: 53400 Avg. Training loss: 2.8326 0.0156 sec/batch\n",
      "Global Step: 53500 Epoch 23/50 Iteration: 53500 Avg. Training loss: 2.8474 0.0140 sec/batch\n",
      "Global Step: 53600 Epoch 23/50 Iteration: 53600 Avg. Training loss: 2.8433 0.0134 sec/batch\n",
      "Global Step: 53700 Epoch 23/50 Iteration: 53700 Avg. Training loss: 2.8552 0.0142 sec/batch\n",
      "Global Step: 53800 Epoch 23/50 Iteration: 53800 Avg. Training loss: 2.8489 0.0139 sec/batch\n",
      "Global Step: 53900 Epoch 23/50 Iteration: 53900 Avg. Training loss: 2.8549 0.0100 sec/batch\n",
      "Global Step: 54000 Epoch 23/50 Iteration: 54000 Avg. Training loss: 2.8393 0.0120 sec/batch\n",
      "Global Step: 54100 Epoch 23/50 Iteration: 54100 Avg. Training loss: 2.8720 0.0134 sec/batch\n",
      "Global Step: 54200 Epoch 23/50 Iteration: 54200 Avg. Training loss: 2.8509 0.0123 sec/batch\n",
      "Global Step: 54300 Epoch 23/50 Iteration: 54300 Avg. Training loss: 2.8595 0.0141 sec/batch\n",
      "Global Step: 54400 Epoch 23/50 Iteration: 54400 Avg. Training loss: 2.8235 0.0159 sec/batch\n",
      "Global Step: 54500 Epoch 23/50 Iteration: 54500 Avg. Training loss: 2.8347 0.0173 sec/batch\n",
      "Global Step: 54600 Epoch 23/50 Iteration: 54600 Avg. Training loss: 2.8431 0.0149 sec/batch\n",
      "Global Step: 54700 Epoch 23/50 Iteration: 54700 Avg. Training loss: 2.8479 0.0154 sec/batch\n",
      "Epoch 24/50 Threshold: 0.0780015896553169 Length of Training words: 2372332\n",
      "Global Step: 54800 Epoch 24/50 Iteration: 54800 Avg. Training loss: 2.8681 0.0129 sec/batch\n",
      "Global Step: 54900 Epoch 24/50 Iteration: 54900 Avg. Training loss: 2.8848 0.0151 sec/batch\n",
      "Global Step: 55000 Epoch 24/50 Iteration: 55000 Avg. Training loss: 2.8635 0.0135 sec/batch\n",
      "Global Step: 55100 Epoch 24/50 Iteration: 55100 Avg. Training loss: 2.8363 0.0169 sec/batch\n",
      "Global Step: 55200 Epoch 24/50 Iteration: 55200 Avg. Training loss: 2.8517 0.0175 sec/batch\n",
      "Global Step: 55300 Epoch 24/50 Iteration: 55300 Avg. Training loss: 2.8504 0.0152 sec/batch\n",
      "Global Step: 55400 Epoch 24/50 Iteration: 55400 Avg. Training loss: 2.8488 0.0162 sec/batch\n",
      "Global Step: 55500 Epoch 24/50 Iteration: 55500 Avg. Training loss: 2.8712 0.0149 sec/batch\n",
      "Global Step: 55600 Epoch 24/50 Iteration: 55600 Avg. Training loss: 2.8663 0.0151 sec/batch\n",
      "Global Step: 55700 Epoch 24/50 Iteration: 55700 Avg. Training loss: 2.8392 0.0144 sec/batch\n",
      "Global Step: 55800 Epoch 24/50 Iteration: 55800 Avg. Training loss: 2.8393 0.0137 sec/batch\n",
      "Global Step: 55900 Epoch 24/50 Iteration: 55900 Avg. Training loss: 2.8572 0.0121 sec/batch\n",
      "Global Step: 56000 Epoch 24/50 Iteration: 56000 Avg. Training loss: 2.8480 0.0133 sec/batch\n",
      "Global Step: 56100 Epoch 24/50 Iteration: 56100 Avg. Training loss: 2.8465 0.0165 sec/batch\n",
      "Global Step: 56200 Epoch 24/50 Iteration: 56200 Avg. Training loss: 2.8622 0.0161 sec/batch\n",
      "Global Step: 56300 Epoch 24/50 Iteration: 56300 Avg. Training loss: 2.8479 0.0135 sec/batch\n",
      "Global Step: 56400 Epoch 24/50 Iteration: 56400 Avg. Training loss: 2.8543 0.0144 sec/batch\n",
      "Global Step: 56500 Epoch 24/50 Iteration: 56500 Avg. Training loss: 2.8700 0.0149 sec/batch\n",
      "Global Step: 56600 Epoch 24/50 Iteration: 56600 Avg. Training loss: 2.8564 0.0142 sec/batch\n",
      "Global Step: 56700 Epoch 24/50 Iteration: 56700 Avg. Training loss: 2.8645 0.0153 sec/batch\n",
      "Global Step: 56800 Epoch 24/50 Iteration: 56800 Avg. Training loss: 2.8281 0.0164 sec/batch\n",
      "Global Step: 56900 Epoch 24/50 Iteration: 56900 Avg. Training loss: 2.8405 0.0150 sec/batch\n",
      "Global Step: 57000 Epoch 24/50 Iteration: 57000 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Epoch 25/50 Threshold: 0.08888214480232033 Length of Training words: 2439982\n",
      "Global Step: 57100 Epoch 25/50 Iteration: 57100 Avg. Training loss: 2.8459 0.0050 sec/batch\n",
      "Global Step: 57200 Epoch 25/50 Iteration: 57200 Avg. Training loss: 2.8390 0.0158 sec/batch\n",
      "Global Step: 57300 Epoch 25/50 Iteration: 57300 Avg. Training loss: 2.8665 0.0159 sec/batch\n",
      "Global Step: 57400 Epoch 25/50 Iteration: 57400 Avg. Training loss: 2.8340 0.0161 sec/batch\n",
      "Global Step: 57500 Epoch 25/50 Iteration: 57500 Avg. Training loss: 2.8204 0.0156 sec/batch\n",
      "Global Step: 57600 Epoch 25/50 Iteration: 57600 Avg. Training loss: 2.8238 0.0147 sec/batch\n",
      "Global Step: 57700 Epoch 25/50 Iteration: 57700 Avg. Training loss: 2.8264 0.0164 sec/batch\n",
      "Global Step: 57800 Epoch 25/50 Iteration: 57800 Avg. Training loss: 2.8334 0.0129 sec/batch\n",
      "Global Step: 57900 Epoch 25/50 Iteration: 57900 Avg. Training loss: 2.8425 0.0142 sec/batch\n",
      "Global Step: 58000 Epoch 25/50 Iteration: 58000 Avg. Training loss: 2.8515 0.0137 sec/batch\n",
      "Global Step: 58100 Epoch 25/50 Iteration: 58100 Avg. Training loss: 2.8138 0.0134 sec/batch\n",
      "Global Step: 58200 Epoch 25/50 Iteration: 58200 Avg. Training loss: 2.8194 0.0123 sec/batch\n",
      "Global Step: 58300 Epoch 25/50 Iteration: 58300 Avg. Training loss: 2.8313 0.0131 sec/batch\n",
      "Global Step: 58400 Epoch 25/50 Iteration: 58400 Avg. Training loss: 2.8262 0.0141 sec/batch\n",
      "Global Step: 58500 Epoch 25/50 Iteration: 58500 Avg. Training loss: 2.8276 0.0120 sec/batch\n",
      "Global Step: 58600 Epoch 25/50 Iteration: 58600 Avg. Training loss: 2.8294 0.0125 sec/batch\n",
      "Global Step: 58700 Epoch 25/50 Iteration: 58700 Avg. Training loss: 2.8341 0.0139 sec/batch\n",
      "Global Step: 58800 Epoch 25/50 Iteration: 58800 Avg. Training loss: 2.8255 0.0140 sec/batch\n",
      "Global Step: 58900 Epoch 25/50 Iteration: 58900 Avg. Training loss: 2.8509 0.0144 sec/batch\n",
      "Global Step: 59000 Epoch 25/50 Iteration: 59000 Avg. Training loss: 2.8332 0.0173 sec/batch\n",
      "Global Step: 59100 Epoch 25/50 Iteration: 59100 Avg. Training loss: 2.8422 0.0174 sec/batch\n",
      "Global Step: 59200 Epoch 25/50 Iteration: 59200 Avg. Training loss: 2.8070 0.0156 sec/batch\n",
      "Global Step: 59300 Epoch 25/50 Iteration: 59300 Avg. Training loss: 2.8117 0.0152 sec/batch\n",
      "Global Step: 59400 Epoch 25/50 Iteration: 59400 Avg. Training loss: 2.8299 0.0140 sec/batch\n",
      "Global Step: 59500 Epoch 25/50 Iteration: 59500 Avg. Training loss: 2.8283 0.0153 sec/batch\n",
      "Epoch 26/50 Threshold: 0.07345678619703419 Length of Training words: 2341446\n",
      "Global Step: 59600 Epoch 26/50 Iteration: 59600 Avg. Training loss: 2.8632 0.0132 sec/batch\n",
      "Global Step: 59700 Epoch 26/50 Iteration: 59700 Avg. Training loss: 2.9014 0.0175 sec/batch\n",
      "Global Step: 59800 Epoch 26/50 Iteration: 59800 Avg. Training loss: 2.8739 0.0173 sec/batch\n",
      "Global Step: 59900 Epoch 26/50 Iteration: 59900 Avg. Training loss: 2.8502 0.0133 sec/batch\n",
      "Global Step: 60000 Epoch 26/50 Iteration: 60000 Avg. Training loss: 2.8588 0.0154 sec/batch\n",
      "Global Step: 60100 Epoch 26/50 Iteration: 60100 Avg. Training loss: 2.8628 0.0153 sec/batch\n",
      "Global Step: 60200 Epoch 26/50 Iteration: 60200 Avg. Training loss: 2.8578 0.0140 sec/batch\n",
      "Global Step: 60300 Epoch 26/50 Iteration: 60300 Avg. Training loss: 2.8807 0.0139 sec/batch\n",
      "Global Step: 60400 Epoch 26/50 Iteration: 60400 Avg. Training loss: 2.8789 0.0155 sec/batch\n",
      "Global Step: 60500 Epoch 26/50 Iteration: 60500 Avg. Training loss: 2.8458 0.0132 sec/batch\n",
      "Global Step: 60600 Epoch 26/50 Iteration: 60600 Avg. Training loss: 2.8506 0.0154 sec/batch\n",
      "Global Step: 60700 Epoch 26/50 Iteration: 60700 Avg. Training loss: 2.8664 0.0173 sec/batch\n",
      "Global Step: 60800 Epoch 26/50 Iteration: 60800 Avg. Training loss: 2.8588 0.0144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 60900 Epoch 26/50 Iteration: 60900 Avg. Training loss: 2.8592 0.0140 sec/batch\n",
      "Global Step: 61000 Epoch 26/50 Iteration: 61000 Avg. Training loss: 2.8718 0.0146 sec/batch\n",
      "Global Step: 61100 Epoch 26/50 Iteration: 61100 Avg. Training loss: 2.8490 0.0119 sec/batch\n",
      "Global Step: 61200 Epoch 26/50 Iteration: 61200 Avg. Training loss: 2.8788 0.0142 sec/batch\n",
      "Global Step: 61300 Epoch 26/50 Iteration: 61300 Avg. Training loss: 2.8717 0.0144 sec/batch\n",
      "Global Step: 61400 Epoch 26/50 Iteration: 61400 Avg. Training loss: 2.8806 0.0137 sec/batch\n",
      "Global Step: 61500 Epoch 26/50 Iteration: 61500 Avg. Training loss: 2.8656 0.0122 sec/batch\n",
      "Global Step: 61600 Epoch 26/50 Iteration: 61600 Avg. Training loss: 2.8393 0.0152 sec/batch\n",
      "Global Step: 61700 Epoch 26/50 Iteration: 61700 Avg. Training loss: 2.8497 0.0163 sec/batch\n",
      "Global Step: 61800 Epoch 26/50 Iteration: 61800 Avg. Training loss: 2.8591 0.0134 sec/batch\n",
      "Epoch 27/50 Threshold: 0.06887011513088591 Length of Training words: 2304434\n",
      "Global Step: 61900 Epoch 27/50 Iteration: 61900 Avg. Training loss: 2.8719 0.0065 sec/batch\n",
      "Global Step: 62000 Epoch 27/50 Iteration: 62000 Avg. Training loss: 2.8947 0.0164 sec/batch\n",
      "Global Step: 62100 Epoch 27/50 Iteration: 62100 Avg. Training loss: 2.8940 0.0152 sec/batch\n",
      "Global Step: 62200 Epoch 27/50 Iteration: 62200 Avg. Training loss: 2.8749 0.0151 sec/batch\n",
      "Global Step: 62300 Epoch 27/50 Iteration: 62300 Avg. Training loss: 2.8666 0.0146 sec/batch\n",
      "Global Step: 62400 Epoch 27/50 Iteration: 62400 Avg. Training loss: 2.8653 0.0144 sec/batch\n",
      "Global Step: 62500 Epoch 27/50 Iteration: 62500 Avg. Training loss: 2.8751 0.0145 sec/batch\n",
      "Global Step: 62600 Epoch 27/50 Iteration: 62600 Avg. Training loss: 2.8865 0.0162 sec/batch\n",
      "Global Step: 62700 Epoch 27/50 Iteration: 62700 Avg. Training loss: 2.8800 0.0163 sec/batch\n",
      "Global Step: 62800 Epoch 27/50 Iteration: 62800 Avg. Training loss: 2.8802 0.0151 sec/batch\n",
      "Global Step: 62900 Epoch 27/50 Iteration: 62900 Avg. Training loss: 2.8586 0.0163 sec/batch\n",
      "Global Step: 63000 Epoch 27/50 Iteration: 63000 Avg. Training loss: 2.8810 0.0153 sec/batch\n",
      "Global Step: 63100 Epoch 27/50 Iteration: 63100 Avg. Training loss: 2.8715 0.0163 sec/batch\n",
      "Global Step: 63200 Epoch 27/50 Iteration: 63200 Avg. Training loss: 2.8706 0.0136 sec/batch\n",
      "Global Step: 63300 Epoch 27/50 Iteration: 63300 Avg. Training loss: 2.8690 0.0142 sec/batch\n",
      "Global Step: 63400 Epoch 27/50 Iteration: 63400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 63500 Epoch 27/50 Iteration: 63500 Avg. Training loss: 2.8745 0.0145 sec/batch\n",
      "Global Step: 63600 Epoch 27/50 Iteration: 63600 Avg. Training loss: 2.8927 0.0141 sec/batch\n",
      "Global Step: 63700 Epoch 27/50 Iteration: 63700 Avg. Training loss: 2.8815 0.0149 sec/batch\n",
      "Global Step: 63800 Epoch 27/50 Iteration: 63800 Avg. Training loss: 2.8809 0.0149 sec/batch\n",
      "Global Step: 63900 Epoch 27/50 Iteration: 63900 Avg. Training loss: 2.8529 0.0154 sec/batch\n",
      "Global Step: 64000 Epoch 27/50 Iteration: 64000 Avg. Training loss: 2.8606 0.0159 sec/batch\n",
      "Global Step: 64100 Epoch 27/50 Iteration: 64100 Avg. Training loss: 2.8647 0.0147 sec/batch\n",
      "Epoch 28/50 Threshold: 0.08345389394934948 Length of Training words: 2406327\n",
      "Global Step: 64200 Epoch 28/50 Iteration: 64200 Avg. Training loss: 2.8669 0.0068 sec/batch\n",
      "Global Step: 64300 Epoch 28/50 Iteration: 64300 Avg. Training loss: 2.8533 0.0153 sec/batch\n",
      "Global Step: 64400 Epoch 28/50 Iteration: 64400 Avg. Training loss: 2.8678 0.0171 sec/batch\n",
      "Global Step: 64500 Epoch 28/50 Iteration: 64500 Avg. Training loss: 2.8451 0.0160 sec/batch\n",
      "Global Step: 64600 Epoch 28/50 Iteration: 64600 Avg. Training loss: 2.8374 0.0173 sec/batch\n",
      "Global Step: 64700 Epoch 28/50 Iteration: 64700 Avg. Training loss: 2.8261 0.0156 sec/batch\n",
      "Global Step: 64800 Epoch 28/50 Iteration: 64800 Avg. Training loss: 2.8465 0.0147 sec/batch\n",
      "Global Step: 64900 Epoch 28/50 Iteration: 64900 Avg. Training loss: 2.8482 0.0152 sec/batch\n",
      "Global Step: 65000 Epoch 28/50 Iteration: 65000 Avg. Training loss: 2.8399 0.0148 sec/batch\n",
      "Global Step: 65100 Epoch 28/50 Iteration: 65100 Avg. Training loss: 2.8677 0.0125 sec/batch\n",
      "Global Step: 65200 Epoch 28/50 Iteration: 65200 Avg. Training loss: 2.8238 0.0138 sec/batch\n",
      "Global Step: 65300 Epoch 28/50 Iteration: 65300 Avg. Training loss: 2.8220 0.0142 sec/batch\n",
      "Global Step: 65400 Epoch 28/50 Iteration: 65400 Avg. Training loss: 2.8588 0.0149 sec/batch\n",
      "Global Step: 65500 Epoch 28/50 Iteration: 65500 Avg. Training loss: 2.8271 0.0172 sec/batch\n",
      "Global Step: 65600 Epoch 28/50 Iteration: 65600 Avg. Training loss: 2.8475 0.0139 sec/batch\n",
      "Global Step: 65700 Epoch 28/50 Iteration: 65700 Avg. Training loss: 2.8460 0.0169 sec/batch\n",
      "Global Step: 65800 Epoch 28/50 Iteration: 65800 Avg. Training loss: 2.8226 0.0142 sec/batch\n",
      "Global Step: 65900 Epoch 28/50 Iteration: 65900 Avg. Training loss: 2.8628 0.0144 sec/batch\n",
      "Global Step: 66000 Epoch 28/50 Iteration: 66000 Avg. Training loss: 2.8420 0.0164 sec/batch\n",
      "Global Step: 66100 Epoch 28/50 Iteration: 66100 Avg. Training loss: 2.8640 0.0122 sec/batch\n",
      "Global Step: 66200 Epoch 28/50 Iteration: 66200 Avg. Training loss: 2.8489 0.0126 sec/batch\n",
      "Global Step: 66300 Epoch 28/50 Iteration: 66300 Avg. Training loss: 2.8178 0.0141 sec/batch\n",
      "Global Step: 66400 Epoch 28/50 Iteration: 66400 Avg. Training loss: 2.8261 0.0148 sec/batch\n",
      "Global Step: 66500 Epoch 28/50 Iteration: 66500 Avg. Training loss: 2.8332 0.0136 sec/batch\n",
      "Epoch 29/50 Threshold: 0.07780942376694977 Length of Training words: 2371334\n",
      "Global Step: 66600 Epoch 29/50 Iteration: 66600 Avg. Training loss: 2.8515 0.0051 sec/batch\n",
      "Global Step: 66700 Epoch 29/50 Iteration: 66700 Avg. Training loss: 2.8699 0.0144 sec/batch\n",
      "Global Step: 66800 Epoch 29/50 Iteration: 66800 Avg. Training loss: 2.8806 0.0142 sec/batch\n",
      "Global Step: 66900 Epoch 29/50 Iteration: 66900 Avg. Training loss: 2.8606 0.0174 sec/batch\n",
      "Global Step: 67000 Epoch 29/50 Iteration: 67000 Avg. Training loss: 2.8471 0.0158 sec/batch\n",
      "Global Step: 67100 Epoch 29/50 Iteration: 67100 Avg. Training loss: 2.8366 0.0155 sec/batch\n",
      "Global Step: 67200 Epoch 29/50 Iteration: 67200 Avg. Training loss: 2.8587 0.0172 sec/batch\n",
      "Global Step: 67300 Epoch 29/50 Iteration: 67300 Avg. Training loss: 2.8587 0.0173 sec/batch\n",
      "Global Step: 67400 Epoch 29/50 Iteration: 67400 Avg. Training loss: 2.8475 0.0143 sec/batch\n",
      "Global Step: 67500 Epoch 29/50 Iteration: 67500 Avg. Training loss: 2.8783 0.0162 sec/batch\n",
      "Global Step: 67600 Epoch 29/50 Iteration: 67600 Avg. Training loss: 2.8358 0.0146 sec/batch\n",
      "Global Step: 67700 Epoch 29/50 Iteration: 67700 Avg. Training loss: 2.8432 0.0171 sec/batch\n",
      "Global Step: 67800 Epoch 29/50 Iteration: 67800 Avg. Training loss: 2.8625 0.0133 sec/batch\n",
      "Global Step: 67900 Epoch 29/50 Iteration: 67900 Avg. Training loss: 2.8385 0.0154 sec/batch\n",
      "Global Step: 68000 Epoch 29/50 Iteration: 68000 Avg. Training loss: 2.8606 0.0161 sec/batch\n",
      "Global Step: 68100 Epoch 29/50 Iteration: 68100 Avg. Training loss: 2.8533 0.0178 sec/batch\n",
      "Global Step: 68200 Epoch 29/50 Iteration: 68200 Avg. Training loss: 2.8337 0.0147 sec/batch\n",
      "Global Step: 68300 Epoch 29/50 Iteration: 68300 Avg. Training loss: 2.8782 0.0142 sec/batch\n",
      "Global Step: 68400 Epoch 29/50 Iteration: 68400 Avg. Training loss: 2.8537 0.0148 sec/batch\n",
      "Global Step: 68500 Epoch 29/50 Iteration: 68500 Avg. Training loss: 2.8706 0.0155 sec/batch\n",
      "Global Step: 68600 Epoch 29/50 Iteration: 68600 Avg. Training loss: 2.8400 0.0144 sec/batch\n",
      "Global Step: 68700 Epoch 29/50 Iteration: 68700 Avg. Training loss: 2.8288 0.0144 sec/batch\n",
      "Global Step: 68800 Epoch 29/50 Iteration: 68800 Avg. Training loss: 2.8570 0.0160 sec/batch\n",
      "Global Step: 68900 Epoch 29/50 Iteration: 68900 Avg. Training loss: 2.8468 0.0142 sec/batch\n",
      "Epoch 30/50 Threshold: 0.0654034721364846 Length of Training words: 2275887\n",
      "Global Step: 69000 Epoch 30/50 Iteration: 69000 Avg. Training loss: 2.8809 0.0074 sec/batch\n",
      "Global Step: 69100 Epoch 30/50 Iteration: 69100 Avg. Training loss: 2.9058 0.0126 sec/batch\n",
      "Global Step: 69200 Epoch 30/50 Iteration: 69200 Avg. Training loss: 2.9044 0.0157 sec/batch\n",
      "Global Step: 69300 Epoch 30/50 Iteration: 69300 Avg. Training loss: 2.8774 0.0135 sec/batch\n",
      "Global Step: 69400 Epoch 30/50 Iteration: 69400 Avg. Training loss: 2.8757 0.0148 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 69500 Epoch 30/50 Iteration: 69500 Avg. Training loss: 2.8781 0.0181 sec/batch\n",
      "Global Step: 69600 Epoch 30/50 Iteration: 69600 Avg. Training loss: 2.8759 0.0168 sec/batch\n",
      "Global Step: 69700 Epoch 30/50 Iteration: 69700 Avg. Training loss: 2.8965 0.0163 sec/batch\n",
      "Global Step: 69800 Epoch 30/50 Iteration: 69800 Avg. Training loss: 2.8986 0.0151 sec/batch\n",
      "Global Step: 69900 Epoch 30/50 Iteration: 69900 Avg. Training loss: 2.8659 0.0150 sec/batch\n",
      "Global Step: 70000 Epoch 30/50 Iteration: 70000 Avg. Training loss: 2.8674 0.0157 sec/batch\n",
      "Global Step: 70100 Epoch 30/50 Iteration: 70100 Avg. Training loss: 2.8952 0.0150 sec/batch\n",
      "Global Step: 70200 Epoch 30/50 Iteration: 70200 Avg. Training loss: 2.8774 0.0138 sec/batch\n",
      "Global Step: 70300 Epoch 30/50 Iteration: 70300 Avg. Training loss: 2.8865 0.0144 sec/batch\n",
      "Global Step: 70400 Epoch 30/50 Iteration: 70400 Avg. Training loss: 2.8862 0.0161 sec/batch\n",
      "Global Step: 70500 Epoch 30/50 Iteration: 70500 Avg. Training loss: 2.8653 0.0143 sec/batch\n",
      "Global Step: 70600 Epoch 30/50 Iteration: 70600 Avg. Training loss: 2.9084 0.0151 sec/batch\n",
      "Global Step: 70700 Epoch 30/50 Iteration: 70700 Avg. Training loss: 2.8830 0.0147 sec/batch\n",
      "Global Step: 70800 Epoch 30/50 Iteration: 70800 Avg. Training loss: 2.9020 0.0159 sec/batch\n",
      "Global Step: 70900 Epoch 30/50 Iteration: 70900 Avg. Training loss: 2.8643 0.0157 sec/batch\n",
      "Global Step: 71000 Epoch 30/50 Iteration: 71000 Avg. Training loss: 2.8571 0.0135 sec/batch\n",
      "Global Step: 71100 Epoch 30/50 Iteration: 71100 Avg. Training loss: 2.8914 0.0143 sec/batch\n",
      "Global Step: 71200 Epoch 30/50 Iteration: 71200 Avg. Training loss: 2.8760 0.0159 sec/batch\n",
      "Epoch 31/50 Threshold: 0.060578662616402365 Length of Training words: 2234747\n",
      "Global Step: 71300 Epoch 31/50 Iteration: 71300 Avg. Training loss: 2.9042 0.0133 sec/batch\n",
      "Global Step: 71400 Epoch 31/50 Iteration: 71400 Avg. Training loss: 2.9262 0.0137 sec/batch\n",
      "Global Step: 71500 Epoch 31/50 Iteration: 71500 Avg. Training loss: 2.9054 0.0148 sec/batch\n",
      "Global Step: 71600 Epoch 31/50 Iteration: 71600 Avg. Training loss: 2.8830 0.0137 sec/batch\n",
      "Global Step: 71700 Epoch 31/50 Iteration: 71700 Avg. Training loss: 2.8938 0.0169 sec/batch\n",
      "Global Step: 71800 Epoch 31/50 Iteration: 71800 Avg. Training loss: 2.8912 0.0160 sec/batch\n",
      "Global Step: 71900 Epoch 31/50 Iteration: 71900 Avg. Training loss: 2.9045 0.0142 sec/batch\n",
      "Global Step: 72000 Epoch 31/50 Iteration: 72000 Avg. Training loss: 2.8956 0.0136 sec/batch\n",
      "Global Step: 72100 Epoch 31/50 Iteration: 72100 Avg. Training loss: 2.9144 0.0149 sec/batch\n",
      "Global Step: 72200 Epoch 31/50 Iteration: 72200 Avg. Training loss: 2.8830 0.0165 sec/batch\n",
      "Global Step: 72300 Epoch 31/50 Iteration: 72300 Avg. Training loss: 2.8844 0.0176 sec/batch\n",
      "Global Step: 72400 Epoch 31/50 Iteration: 72400 Avg. Training loss: 2.8942 0.0157 sec/batch\n",
      "Global Step: 72500 Epoch 31/50 Iteration: 72500 Avg. Training loss: 2.8993 0.0172 sec/batch\n",
      "Global Step: 72600 Epoch 31/50 Iteration: 72600 Avg. Training loss: 2.8914 0.0152 sec/batch\n",
      "Global Step: 72700 Epoch 31/50 Iteration: 72700 Avg. Training loss: 2.9023 0.0141 sec/batch\n",
      "Global Step: 72800 Epoch 31/50 Iteration: 72800 Avg. Training loss: 2.8960 0.0148 sec/batch\n",
      "Global Step: 72900 Epoch 31/50 Iteration: 72900 Avg. Training loss: 2.9102 0.0161 sec/batch\n",
      "Global Step: 73000 Epoch 31/50 Iteration: 73000 Avg. Training loss: 2.9006 0.0173 sec/batch\n",
      "Global Step: 73100 Epoch 31/50 Iteration: 73100 Avg. Training loss: 2.9065 0.0148 sec/batch\n",
      "Global Step: 73200 Epoch 31/50 Iteration: 73200 Avg. Training loss: 2.8709 0.0145 sec/batch\n",
      "Global Step: 73300 Epoch 31/50 Iteration: 73300 Avg. Training loss: 2.8839 0.0138 sec/batch\n",
      "Global Step: 73400 Epoch 31/50 Iteration: 73400 Avg. Training loss: 2.8975 0.0167 sec/batch\n",
      "Epoch 32/50 Threshold: 0.0802312071663469 Length of Training words: 2387534\n",
      "Global Step: 73500 Epoch 32/50 Iteration: 73500 Avg. Training loss: 2.8713 0.0091 sec/batch\n",
      "Global Step: 73600 Epoch 32/50 Iteration: 73600 Avg. Training loss: 2.8711 0.0160 sec/batch\n",
      "Global Step: 73700 Epoch 32/50 Iteration: 73700 Avg. Training loss: 2.8705 0.0147 sec/batch\n",
      "Global Step: 73800 Epoch 32/50 Iteration: 73800 Avg. Training loss: 2.8468 0.0126 sec/batch\n",
      "Global Step: 73900 Epoch 32/50 Iteration: 73900 Avg. Training loss: 2.8437 0.0126 sec/batch\n",
      "Global Step: 74000 Epoch 32/50 Iteration: 74000 Avg. Training loss: 2.8388 0.0149 sec/batch\n",
      "Global Step: 74100 Epoch 32/50 Iteration: 74100 Avg. Training loss: 2.8522 0.0127 sec/batch\n",
      "Global Step: 74200 Epoch 32/50 Iteration: 74200 Avg. Training loss: 2.8525 0.0155 sec/batch\n",
      "Global Step: 74300 Epoch 32/50 Iteration: 74300 Avg. Training loss: 2.8508 0.0142 sec/batch\n",
      "Global Step: 74400 Epoch 32/50 Iteration: 74400 Avg. Training loss: 2.8595 0.0144 sec/batch\n",
      "Global Step: 74500 Epoch 32/50 Iteration: 74500 Avg. Training loss: 2.8337 0.0141 sec/batch\n",
      "Global Step: 74600 Epoch 32/50 Iteration: 74600 Avg. Training loss: 2.8405 0.0152 sec/batch\n",
      "Global Step: 74700 Epoch 32/50 Iteration: 74700 Avg. Training loss: 2.8522 0.0141 sec/batch\n",
      "Global Step: 74800 Epoch 32/50 Iteration: 74800 Avg. Training loss: 2.8450 0.0175 sec/batch\n",
      "Global Step: 74900 Epoch 32/50 Iteration: 74900 Avg. Training loss: 2.8539 0.0170 sec/batch\n",
      "Global Step: 75000 Epoch 32/50 Iteration: 75000 Avg. Training loss: 2.8420 0.0170 sec/batch\n",
      "Global Step: 75100 Epoch 32/50 Iteration: 75100 Avg. Training loss: 2.8333 0.0126 sec/batch\n",
      "Global Step: 75200 Epoch 32/50 Iteration: 75200 Avg. Training loss: 2.8720 0.0178 sec/batch\n",
      "Global Step: 75300 Epoch 32/50 Iteration: 75300 Avg. Training loss: 2.8506 0.0153 sec/batch\n",
      "Global Step: 75400 Epoch 32/50 Iteration: 75400 Avg. Training loss: 2.8680 0.0153 sec/batch\n",
      "Global Step: 75500 Epoch 32/50 Iteration: 75500 Avg. Training loss: 2.8328 0.0161 sec/batch\n",
      "Global Step: 75600 Epoch 32/50 Iteration: 75600 Avg. Training loss: 2.8223 0.0147 sec/batch\n",
      "Global Step: 75700 Epoch 32/50 Iteration: 75700 Avg. Training loss: 2.8545 0.0159 sec/batch\n",
      "Global Step: 75800 Epoch 32/50 Iteration: 75800 Avg. Training loss: 2.8407 0.0147 sec/batch\n",
      "Epoch 33/50 Threshold: 0.06324289916713363 Length of Training words: 2258450\n",
      "Global Step: 75900 Epoch 33/50 Iteration: 75900 Avg. Training loss: 2.8857 0.0120 sec/batch\n",
      "Global Step: 76000 Epoch 33/50 Iteration: 76000 Avg. Training loss: 2.9106 0.0153 sec/batch\n",
      "Global Step: 76100 Epoch 33/50 Iteration: 76100 Avg. Training loss: 2.9095 0.0159 sec/batch\n",
      "Global Step: 76200 Epoch 33/50 Iteration: 76200 Avg. Training loss: 2.8801 0.0162 sec/batch\n",
      "Global Step: 76300 Epoch 33/50 Iteration: 76300 Avg. Training loss: 2.8828 0.0148 sec/batch\n",
      "Global Step: 76400 Epoch 33/50 Iteration: 76400 Avg. Training loss: 2.8858 0.0148 sec/batch\n",
      "Global Step: 76500 Epoch 33/50 Iteration: 76500 Avg. Training loss: 2.8860 0.0133 sec/batch\n",
      "Global Step: 76600 Epoch 33/50 Iteration: 76600 Avg. Training loss: 2.8963 0.0137 sec/batch\n",
      "Global Step: 76700 Epoch 33/50 Iteration: 76700 Avg. Training loss: 2.9017 0.0129 sec/batch\n",
      "Global Step: 76800 Epoch 33/50 Iteration: 76800 Avg. Training loss: 2.8796 0.0135 sec/batch\n",
      "Global Step: 76900 Epoch 33/50 Iteration: 76900 Avg. Training loss: 2.8701 0.0151 sec/batch\n",
      "Global Step: 77000 Epoch 33/50 Iteration: 77000 Avg. Training loss: 2.9061 0.0164 sec/batch\n",
      "Global Step: 77100 Epoch 33/50 Iteration: 77100 Avg. Training loss: 2.8756 0.0163 sec/batch\n",
      "Global Step: 77200 Epoch 33/50 Iteration: 77200 Avg. Training loss: 2.8963 0.0188 sec/batch\n",
      "Global Step: 77300 Epoch 33/50 Iteration: 77300 Avg. Training loss: 2.8865 0.0152 sec/batch\n",
      "Global Step: 77400 Epoch 33/50 Iteration: 77400 Avg. Training loss: 2.8772 0.0142 sec/batch\n",
      "Global Step: 77500 Epoch 33/50 Iteration: 77500 Avg. Training loss: 2.9122 0.0154 sec/batch\n",
      "Global Step: 77600 Epoch 33/50 Iteration: 77600 Avg. Training loss: 2.8915 0.0157 sec/batch\n",
      "Global Step: 77700 Epoch 33/50 Iteration: 77700 Avg. Training loss: 2.9027 0.0142 sec/batch\n",
      "Global Step: 77800 Epoch 33/50 Iteration: 77800 Avg. Training loss: 2.8703 0.0156 sec/batch\n",
      "Global Step: 77900 Epoch 33/50 Iteration: 77900 Avg. Training loss: 2.8723 0.0163 sec/batch\n",
      "Global Step: 78000 Epoch 33/50 Iteration: 78000 Avg. Training loss: 2.8794 0.0150 sec/batch\n",
      "Epoch 34/50 Threshold: 0.07746417489937713 Length of Training words: 2369277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 78100 Epoch 34/50 Iteration: 78100 Avg. Training loss: 2.8833 0.0020 sec/batch\n",
      "Global Step: 78200 Epoch 34/50 Iteration: 78200 Avg. Training loss: 2.8695 0.0169 sec/batch\n",
      "Global Step: 78300 Epoch 34/50 Iteration: 78300 Avg. Training loss: 2.8849 0.0144 sec/batch\n",
      "Global Step: 78400 Epoch 34/50 Iteration: 78400 Avg. Training loss: 2.8621 0.0124 sec/batch\n",
      "Global Step: 78500 Epoch 34/50 Iteration: 78500 Avg. Training loss: 2.8413 0.0143 sec/batch\n",
      "Global Step: 78600 Epoch 34/50 Iteration: 78600 Avg. Training loss: 2.8482 0.0140 sec/batch\n",
      "Global Step: 78700 Epoch 34/50 Iteration: 78700 Avg. Training loss: 2.8491 0.0151 sec/batch\n",
      "Global Step: 78800 Epoch 34/50 Iteration: 78800 Avg. Training loss: 2.8581 0.0144 sec/batch\n",
      "Global Step: 78900 Epoch 34/50 Iteration: 78900 Avg. Training loss: 2.8598 0.0135 sec/batch\n",
      "Global Step: 79000 Epoch 34/50 Iteration: 79000 Avg. Training loss: 2.8737 0.0139 sec/batch\n",
      "Global Step: 79100 Epoch 34/50 Iteration: 79100 Avg. Training loss: 2.8411 0.0155 sec/batch\n",
      "Global Step: 79200 Epoch 34/50 Iteration: 79200 Avg. Training loss: 2.8360 0.0127 sec/batch\n",
      "Global Step: 79300 Epoch 34/50 Iteration: 79300 Avg. Training loss: 2.8640 0.0158 sec/batch\n",
      "Global Step: 79400 Epoch 34/50 Iteration: 79400 Avg. Training loss: 2.8485 0.0161 sec/batch\n",
      "Global Step: 79500 Epoch 34/50 Iteration: 79500 Avg. Training loss: 2.8551 0.0152 sec/batch\n",
      "Global Step: 79600 Epoch 34/50 Iteration: 79600 Avg. Training loss: 2.8634 0.0156 sec/batch\n",
      "Global Step: 79700 Epoch 34/50 Iteration: 79700 Avg. Training loss: 2.8329 0.0141 sec/batch\n",
      "Global Step: 79800 Epoch 34/50 Iteration: 79800 Avg. Training loss: 2.8742 0.0139 sec/batch\n",
      "Global Step: 79900 Epoch 34/50 Iteration: 79900 Avg. Training loss: 2.8585 0.0163 sec/batch\n",
      "Global Step: 80000 Epoch 34/50 Iteration: 80000 Avg. Training loss: 2.8706 0.0178 sec/batch\n",
      "Global Step: 80100 Epoch 34/50 Iteration: 80100 Avg. Training loss: 2.8558 0.0120 sec/batch\n",
      "Global Step: 80200 Epoch 34/50 Iteration: 80200 Avg. Training loss: 2.8265 0.0132 sec/batch\n",
      "Global Step: 80300 Epoch 34/50 Iteration: 80300 Avg. Training loss: 2.8422 0.0160 sec/batch\n",
      "Global Step: 80400 Epoch 34/50 Iteration: 80400 Avg. Training loss: 2.8493 0.0157 sec/batch\n",
      "Epoch 35/50 Threshold: 0.08898776615666429 Length of Training words: 2440799\n",
      "Global Step: 80500 Epoch 35/50 Iteration: 80500 Avg. Training loss: 2.8503 0.0068 sec/batch\n",
      "Global Step: 80600 Epoch 35/50 Iteration: 80600 Avg. Training loss: 2.8414 0.0151 sec/batch\n",
      "Global Step: 80700 Epoch 35/50 Iteration: 80700 Avg. Training loss: 2.8569 0.0150 sec/batch\n",
      "Global Step: 80800 Epoch 35/50 Iteration: 80800 Avg. Training loss: 2.8380 0.0153 sec/batch\n",
      "Global Step: 80900 Epoch 35/50 Iteration: 80900 Avg. Training loss: 2.8241 0.0176 sec/batch\n",
      "Global Step: 81000 Epoch 35/50 Iteration: 81000 Avg. Training loss: 2.8170 0.0160 sec/batch\n",
      "Global Step: 81100 Epoch 35/50 Iteration: 81100 Avg. Training loss: 2.8271 0.0163 sec/batch\n",
      "Global Step: 81200 Epoch 35/50 Iteration: 81200 Avg. Training loss: 2.8338 0.0138 sec/batch\n",
      "Global Step: 81300 Epoch 35/50 Iteration: 81300 Avg. Training loss: 2.8322 0.0136 sec/batch\n",
      "Global Step: 81400 Epoch 35/50 Iteration: 81400 Avg. Training loss: 2.8520 0.0137 sec/batch\n",
      "Global Step: 81500 Epoch 35/50 Iteration: 81500 Avg. Training loss: 2.8154 0.0151 sec/batch\n",
      "Global Step: 81600 Epoch 35/50 Iteration: 81600 Avg. Training loss: 2.8102 0.0130 sec/batch\n",
      "Global Step: 81700 Epoch 35/50 Iteration: 81700 Avg. Training loss: 2.8400 0.0128 sec/batch\n",
      "Global Step: 81800 Epoch 35/50 Iteration: 81800 Avg. Training loss: 2.8253 0.0143 sec/batch\n",
      "Global Step: 81900 Epoch 35/50 Iteration: 81900 Avg. Training loss: 2.8221 0.0161 sec/batch\n",
      "Global Step: 82000 Epoch 35/50 Iteration: 82000 Avg. Training loss: 2.8401 0.0177 sec/batch\n",
      "Global Step: 82100 Epoch 35/50 Iteration: 82100 Avg. Training loss: 2.8228 0.0163 sec/batch\n",
      "Global Step: 82200 Epoch 35/50 Iteration: 82200 Avg. Training loss: 2.8296 0.0148 sec/batch\n",
      "Global Step: 82300 Epoch 35/50 Iteration: 82300 Avg. Training loss: 2.8532 0.0159 sec/batch\n",
      "Global Step: 82400 Epoch 35/50 Iteration: 82400 Avg. Training loss: 2.8319 0.0166 sec/batch\n",
      "Global Step: 82500 Epoch 35/50 Iteration: 82500 Avg. Training loss: 2.8408 0.0160 sec/batch\n",
      "Global Step: 82600 Epoch 35/50 Iteration: 82600 Avg. Training loss: 2.8031 0.0148 sec/batch\n",
      "Global Step: 82700 Epoch 35/50 Iteration: 82700 Avg. Training loss: 2.8184 0.0164 sec/batch\n",
      "Global Step: 82800 Epoch 35/50 Iteration: 82800 Avg. Training loss: 2.8197 0.0151 sec/batch\n",
      "Epoch 36/50 Threshold: 0.07534745402699021 Length of Training words: 2355673\n",
      "Global Step: 82900 Epoch 36/50 Iteration: 82900 Avg. Training loss: 2.8268 0.0005 sec/batch\n",
      "Global Step: 83000 Epoch 36/50 Iteration: 83000 Avg. Training loss: 2.8803 0.0144 sec/batch\n",
      "Global Step: 83100 Epoch 36/50 Iteration: 83100 Avg. Training loss: 2.8871 0.0165 sec/batch\n",
      "Global Step: 83200 Epoch 36/50 Iteration: 83200 Avg. Training loss: 2.8660 0.0155 sec/batch\n",
      "Global Step: 83300 Epoch 36/50 Iteration: 83300 Avg. Training loss: 2.8487 0.0161 sec/batch\n",
      "Global Step: 83400 Epoch 36/50 Iteration: 83400 Avg. Training loss: 2.8498 0.0166 sec/batch\n",
      "Global Step: 83500 Epoch 36/50 Iteration: 83500 Avg. Training loss: 2.8536 0.0141 sec/batch\n",
      "Global Step: 83600 Epoch 36/50 Iteration: 83600 Avg. Training loss: 2.8602 0.0123 sec/batch\n",
      "Global Step: 83700 Epoch 36/50 Iteration: 83700 Avg. Training loss: 2.8686 0.0147 sec/batch\n",
      "Global Step: 83800 Epoch 36/50 Iteration: 83800 Avg. Training loss: 2.8764 0.0137 sec/batch\n",
      "Global Step: 83900 Epoch 36/50 Iteration: 83900 Avg. Training loss: 2.8483 0.0134 sec/batch\n",
      "Global Step: 84000 Epoch 36/50 Iteration: 84000 Avg. Training loss: 2.8404 0.0154 sec/batch\n",
      "Global Step: 84100 Epoch 36/50 Iteration: 84100 Avg. Training loss: 2.8691 0.0132 sec/batch\n",
      "Global Step: 84200 Epoch 36/50 Iteration: 84200 Avg. Training loss: 2.8506 0.0136 sec/batch\n",
      "Global Step: 84300 Epoch 36/50 Iteration: 84300 Avg. Training loss: 2.8566 0.0149 sec/batch\n",
      "Global Step: 84400 Epoch 36/50 Iteration: 84400 Avg. Training loss: 2.8662 0.0173 sec/batch\n",
      "Global Step: 84500 Epoch 36/50 Iteration: 84500 Avg. Training loss: 2.8357 0.0162 sec/batch\n",
      "Global Step: 84600 Epoch 36/50 Iteration: 84600 Avg. Training loss: 2.8759 0.0141 sec/batch\n",
      "Global Step: 84700 Epoch 36/50 Iteration: 84700 Avg. Training loss: 2.8624 0.0146 sec/batch\n",
      "Global Step: 84800 Epoch 36/50 Iteration: 84800 Avg. Training loss: 2.8741 0.0152 sec/batch\n",
      "Global Step: 84900 Epoch 36/50 Iteration: 84900 Avg. Training loss: 2.8591 0.0141 sec/batch\n",
      "Global Step: 85000 Epoch 36/50 Iteration: 85000 Avg. Training loss: 2.8377 0.0164 sec/batch\n",
      "Global Step: 85100 Epoch 36/50 Iteration: 85100 Avg. Training loss: 2.8441 0.0161 sec/batch\n",
      "Global Step: 85200 Epoch 36/50 Iteration: 85200 Avg. Training loss: 2.8583 0.0158 sec/batch\n",
      "Epoch 37/50 Threshold: 0.08809174339175921 Length of Training words: 2435024\n",
      "Global Step: 85300 Epoch 37/50 Iteration: 85300 Avg. Training loss: 2.8461 0.0072 sec/batch\n",
      "Global Step: 85400 Epoch 37/50 Iteration: 85400 Avg. Training loss: 2.8475 0.0144 sec/batch\n",
      "Global Step: 85500 Epoch 37/50 Iteration: 85500 Avg. Training loss: 2.8557 0.0151 sec/batch\n",
      "Global Step: 85600 Epoch 37/50 Iteration: 85600 Avg. Training loss: 2.8360 0.0150 sec/batch\n",
      "Global Step: 85700 Epoch 37/50 Iteration: 85700 Avg. Training loss: 2.8275 0.0140 sec/batch\n",
      "Global Step: 85800 Epoch 37/50 Iteration: 85800 Avg. Training loss: 2.8168 0.0154 sec/batch\n",
      "Global Step: 85900 Epoch 37/50 Iteration: 85900 Avg. Training loss: 2.8360 0.0179 sec/batch\n",
      "Global Step: 86000 Epoch 37/50 Iteration: 86000 Avg. Training loss: 2.8351 0.0162 sec/batch\n",
      "Global Step: 86100 Epoch 37/50 Iteration: 86100 Avg. Training loss: 2.8330 0.0163 sec/batch\n",
      "Global Step: 86200 Epoch 37/50 Iteration: 86200 Avg. Training loss: 2.8584 0.0152 sec/batch\n",
      "Global Step: 86300 Epoch 37/50 Iteration: 86300 Avg. Training loss: 2.8175 0.0149 sec/batch\n",
      "Global Step: 86400 Epoch 37/50 Iteration: 86400 Avg. Training loss: 2.8073 0.0147 sec/batch\n",
      "Global Step: 86500 Epoch 37/50 Iteration: 86500 Avg. Training loss: 2.8441 0.0155 sec/batch\n",
      "Global Step: 86600 Epoch 37/50 Iteration: 86600 Avg. Training loss: 2.8234 0.0148 sec/batch\n",
      "Global Step: 86700 Epoch 37/50 Iteration: 86700 Avg. Training loss: 2.8302 0.0146 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 86800 Epoch 37/50 Iteration: 86800 Avg. Training loss: 2.8378 0.0138 sec/batch\n",
      "Global Step: 86900 Epoch 37/50 Iteration: 86900 Avg. Training loss: 2.8186 0.0152 sec/batch\n",
      "Global Step: 87000 Epoch 37/50 Iteration: 87000 Avg. Training loss: 2.8400 0.0166 sec/batch\n",
      "Global Step: 87100 Epoch 37/50 Iteration: 87100 Avg. Training loss: 2.8483 0.0149 sec/batch\n",
      "Global Step: 87200 Epoch 37/50 Iteration: 87200 Avg. Training loss: 2.8330 0.0185 sec/batch\n",
      "Global Step: 87300 Epoch 37/50 Iteration: 87300 Avg. Training loss: 2.8375 0.0140 sec/batch\n",
      "Global Step: 87400 Epoch 37/50 Iteration: 87400 Avg. Training loss: 2.8104 0.0163 sec/batch\n",
      "Global Step: 87500 Epoch 37/50 Iteration: 87500 Avg. Training loss: 2.8165 0.0159 sec/batch\n",
      "Global Step: 87600 Epoch 37/50 Iteration: 87600 Avg. Training loss: 2.8289 0.0159 sec/batch\n",
      "Epoch 38/50 Threshold: 0.07277438964797873 Length of Training words: 2336721\n",
      "Global Step: 87700 Epoch 38/50 Iteration: 87700 Avg. Training loss: 2.8265 0.0021 sec/batch\n",
      "Global Step: 87800 Epoch 38/50 Iteration: 87800 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 87900 Epoch 38/50 Iteration: 87900 Avg. Training loss: 2.8937 0.0167 sec/batch\n",
      "Global Step: 88000 Epoch 38/50 Iteration: 88000 Avg. Training loss: 2.8696 0.0152 sec/batch\n",
      "Global Step: 88100 Epoch 38/50 Iteration: 88100 Avg. Training loss: 2.8591 0.0154 sec/batch\n",
      "Global Step: 88200 Epoch 38/50 Iteration: 88200 Avg. Training loss: 2.8545 0.0150 sec/batch\n",
      "Global Step: 88300 Epoch 38/50 Iteration: 88300 Avg. Training loss: 2.8603 0.0136 sec/batch\n",
      "Global Step: 88400 Epoch 38/50 Iteration: 88400 Avg. Training loss: 2.8678 0.0148 sec/batch\n",
      "Global Step: 88500 Epoch 38/50 Iteration: 88500 Avg. Training loss: 2.8669 0.0148 sec/batch\n",
      "Global Step: 88600 Epoch 38/50 Iteration: 88600 Avg. Training loss: 2.8888 0.0155 sec/batch\n",
      "Global Step: 88700 Epoch 38/50 Iteration: 88700 Avg. Training loss: 2.8488 0.0144 sec/batch\n",
      "Global Step: 88800 Epoch 38/50 Iteration: 88800 Avg. Training loss: 2.8481 0.0158 sec/batch\n",
      "Global Step: 88900 Epoch 38/50 Iteration: 88900 Avg. Training loss: 2.8802 0.0144 sec/batch\n",
      "Global Step: 89000 Epoch 38/50 Iteration: 89000 Avg. Training loss: 2.8504 0.0139 sec/batch\n",
      "Global Step: 89100 Epoch 38/50 Iteration: 89100 Avg. Training loss: 2.8718 0.0139 sec/batch\n",
      "Global Step: 89200 Epoch 38/50 Iteration: 89200 Avg. Training loss: 2.8640 0.0132 sec/batch\n",
      "Global Step: 89300 Epoch 38/50 Iteration: 89300 Avg. Training loss: 2.8444 0.0161 sec/batch\n",
      "Global Step: 89400 Epoch 38/50 Iteration: 89400 Avg. Training loss: 2.8877 0.0164 sec/batch\n",
      "Global Step: 89500 Epoch 38/50 Iteration: 89500 Avg. Training loss: 2.8663 0.0166 sec/batch\n",
      "Global Step: 89600 Epoch 38/50 Iteration: 89600 Avg. Training loss: 2.8782 0.0177 sec/batch\n",
      "Global Step: 89700 Epoch 38/50 Iteration: 89700 Avg. Training loss: 2.8529 0.0159 sec/batch\n",
      "Global Step: 89800 Epoch 38/50 Iteration: 89800 Avg. Training loss: 2.8370 0.0167 sec/batch\n",
      "Global Step: 89900 Epoch 38/50 Iteration: 89900 Avg. Training loss: 2.8713 0.0162 sec/batch\n",
      "Global Step: 90000 Epoch 38/50 Iteration: 90000 Avg. Training loss: 2.8581 0.0159 sec/batch\n",
      "Epoch 39/50 Threshold: 0.086121294696839 Length of Training words: 2423215\n",
      "Global Step: 90100 Epoch 39/50 Iteration: 90100 Avg. Training loss: 2.8511 0.0120 sec/batch\n",
      "Global Step: 90200 Epoch 39/50 Iteration: 90200 Avg. Training loss: 2.8597 0.0154 sec/batch\n",
      "Global Step: 90300 Epoch 39/50 Iteration: 90300 Avg. Training loss: 2.8542 0.0154 sec/batch\n",
      "Global Step: 90400 Epoch 39/50 Iteration: 90400 Avg. Training loss: 2.8285 0.0158 sec/batch\n",
      "Global Step: 90500 Epoch 39/50 Iteration: 90500 Avg. Training loss: 2.8313 0.0158 sec/batch\n",
      "Global Step: 90600 Epoch 39/50 Iteration: 90600 Avg. Training loss: 2.8206 0.0167 sec/batch\n",
      "Global Step: 90700 Epoch 39/50 Iteration: 90700 Avg. Training loss: 2.8388 0.0134 sec/batch\n",
      "Global Step: 90800 Epoch 39/50 Iteration: 90800 Avg. Training loss: 2.8449 0.0150 sec/batch\n",
      "Global Step: 90900 Epoch 39/50 Iteration: 90900 Avg. Training loss: 2.8452 0.0154 sec/batch\n",
      "Global Step: 91000 Epoch 39/50 Iteration: 91000 Avg. Training loss: 2.8437 0.0156 sec/batch\n",
      "Global Step: 91100 Epoch 39/50 Iteration: 91100 Avg. Training loss: 2.8218 0.0153 sec/batch\n",
      "Global Step: 91200 Epoch 39/50 Iteration: 91200 Avg. Training loss: 2.8275 0.0127 sec/batch\n",
      "Global Step: 91300 Epoch 39/50 Iteration: 91300 Avg. Training loss: 2.8395 0.0145 sec/batch\n",
      "Global Step: 91400 Epoch 39/50 Iteration: 91400 Avg. Training loss: 2.8329 0.0142 sec/batch\n",
      "Global Step: 91500 Epoch 39/50 Iteration: 91500 Avg. Training loss: 2.8406 0.0128 sec/batch\n",
      "Global Step: 91600 Epoch 39/50 Iteration: 91600 Avg. Training loss: 2.8300 0.0147 sec/batch\n",
      "Global Step: 91700 Epoch 39/50 Iteration: 91700 Avg. Training loss: 2.8212 0.0161 sec/batch\n",
      "Global Step: 91800 Epoch 39/50 Iteration: 91800 Avg. Training loss: 2.8601 0.0151 sec/batch\n",
      "Global Step: 91900 Epoch 39/50 Iteration: 91900 Avg. Training loss: 2.8365 0.0163 sec/batch\n",
      "Global Step: 92000 Epoch 39/50 Iteration: 92000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 92100 Epoch 39/50 Iteration: 92100 Avg. Training loss: 2.8259 0.0174 sec/batch\n",
      "Global Step: 92200 Epoch 39/50 Iteration: 92200 Avg. Training loss: 2.8148 0.0173 sec/batch\n",
      "Global Step: 92300 Epoch 39/50 Iteration: 92300 Avg. Training loss: 2.8339 0.0164 sec/batch\n",
      "Global Step: 92400 Epoch 39/50 Iteration: 92400 Avg. Training loss: 2.8303 0.0153 sec/batch\n",
      "Epoch 40/50 Threshold: 0.07775738919778097 Length of Training words: 2370779\n",
      "Global Step: 92500 Epoch 40/50 Iteration: 92500 Avg. Training loss: 2.8442 0.0083 sec/batch\n",
      "Global Step: 92600 Epoch 40/50 Iteration: 92600 Avg. Training loss: 2.8734 0.0159 sec/batch\n",
      "Global Step: 92700 Epoch 40/50 Iteration: 92700 Avg. Training loss: 2.8727 0.0155 sec/batch\n",
      "Global Step: 92800 Epoch 40/50 Iteration: 92800 Avg. Training loss: 2.8527 0.0161 sec/batch\n",
      "Global Step: 92900 Epoch 40/50 Iteration: 92900 Avg. Training loss: 2.8473 0.0153 sec/batch\n",
      "Global Step: 93000 Epoch 40/50 Iteration: 93000 Avg. Training loss: 2.8441 0.0145 sec/batch\n",
      "Global Step: 93100 Epoch 40/50 Iteration: 93100 Avg. Training loss: 2.8565 0.0141 sec/batch\n",
      "Global Step: 93200 Epoch 40/50 Iteration: 93200 Avg. Training loss: 2.8573 0.0162 sec/batch\n",
      "Global Step: 93300 Epoch 40/50 Iteration: 93300 Avg. Training loss: 2.8574 0.0136 sec/batch\n",
      "Global Step: 93400 Epoch 40/50 Iteration: 93400 Avg. Training loss: 2.8654 0.0169 sec/batch\n",
      "Global Step: 93500 Epoch 40/50 Iteration: 93500 Avg. Training loss: 2.8369 0.0162 sec/batch\n",
      "Global Step: 93600 Epoch 40/50 Iteration: 93600 Avg. Training loss: 2.8472 0.0164 sec/batch\n",
      "Global Step: 93700 Epoch 40/50 Iteration: 93700 Avg. Training loss: 2.8550 0.0155 sec/batch\n",
      "Global Step: 93800 Epoch 40/50 Iteration: 93800 Avg. Training loss: 2.8572 0.0164 sec/batch\n",
      "Global Step: 93900 Epoch 40/50 Iteration: 93900 Avg. Training loss: 2.8508 0.0150 sec/batch\n",
      "Global Step: 94000 Epoch 40/50 Iteration: 94000 Avg. Training loss: 2.8506 0.0128 sec/batch\n",
      "Global Step: 94100 Epoch 40/50 Iteration: 94100 Avg. Training loss: 2.8424 0.0131 sec/batch\n",
      "Global Step: 94200 Epoch 40/50 Iteration: 94200 Avg. Training loss: 2.8757 0.0172 sec/batch\n",
      "Global Step: 94300 Epoch 40/50 Iteration: 94300 Avg. Training loss: 2.8537 0.0172 sec/batch\n",
      "Global Step: 94400 Epoch 40/50 Iteration: 94400 Avg. Training loss: 2.8676 0.0167 sec/batch\n",
      "Global Step: 94500 Epoch 40/50 Iteration: 94500 Avg. Training loss: 2.8301 0.0161 sec/batch\n",
      "Global Step: 94600 Epoch 40/50 Iteration: 94600 Avg. Training loss: 2.8309 0.0158 sec/batch\n",
      "Global Step: 94700 Epoch 40/50 Iteration: 94700 Avg. Training loss: 2.8634 0.0155 sec/batch\n",
      "Global Step: 94800 Epoch 40/50 Iteration: 94800 Avg. Training loss: 2.8429 0.0156 sec/batch\n",
      "Epoch 41/50 Threshold: 0.06232496225957763 Length of Training words: 2250332\n",
      "Global Step: 94900 Epoch 41/50 Iteration: 94900 Avg. Training loss: 2.8899 0.0141 sec/batch\n",
      "Global Step: 95000 Epoch 41/50 Iteration: 95000 Avg. Training loss: 2.9321 0.0138 sec/batch\n",
      "Global Step: 95100 Epoch 41/50 Iteration: 95100 Avg. Training loss: 2.9010 0.0171 sec/batch\n",
      "Global Step: 95200 Epoch 41/50 Iteration: 95200 Avg. Training loss: 2.8778 0.0167 sec/batch\n",
      "Global Step: 95300 Epoch 41/50 Iteration: 95300 Avg. Training loss: 2.8830 0.0155 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 95400 Epoch 41/50 Iteration: 95400 Avg. Training loss: 2.8868 0.0145 sec/batch\n",
      "Global Step: 95500 Epoch 41/50 Iteration: 95500 Avg. Training loss: 2.8928 0.0157 sec/batch\n",
      "Global Step: 95600 Epoch 41/50 Iteration: 95600 Avg. Training loss: 2.8940 0.0146 sec/batch\n",
      "Global Step: 95700 Epoch 41/50 Iteration: 95700 Avg. Training loss: 2.9165 0.0161 sec/batch\n",
      "Global Step: 95800 Epoch 41/50 Iteration: 95800 Avg. Training loss: 2.8731 0.0170 sec/batch\n",
      "Global Step: 95900 Epoch 41/50 Iteration: 95900 Avg. Training loss: 2.8816 0.0140 sec/batch\n",
      "Global Step: 96000 Epoch 41/50 Iteration: 96000 Avg. Training loss: 2.8972 0.0143 sec/batch\n",
      "Global Step: 96100 Epoch 41/50 Iteration: 96100 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 96200 Epoch 41/50 Iteration: 96200 Avg. Training loss: 2.8846 0.0137 sec/batch\n",
      "Global Step: 96300 Epoch 41/50 Iteration: 96300 Avg. Training loss: 2.8934 0.0146 sec/batch\n",
      "Global Step: 96400 Epoch 41/50 Iteration: 96400 Avg. Training loss: 2.8802 0.0138 sec/batch\n",
      "Global Step: 96500 Epoch 41/50 Iteration: 96500 Avg. Training loss: 2.9145 0.0160 sec/batch\n",
      "Global Step: 96600 Epoch 41/50 Iteration: 96600 Avg. Training loss: 2.8921 0.0145 sec/batch\n",
      "Global Step: 96700 Epoch 41/50 Iteration: 96700 Avg. Training loss: 2.8998 0.0132 sec/batch\n",
      "Global Step: 96800 Epoch 41/50 Iteration: 96800 Avg. Training loss: 2.8715 0.0166 sec/batch\n",
      "Global Step: 96900 Epoch 41/50 Iteration: 96900 Avg. Training loss: 2.8779 0.0163 sec/batch\n",
      "Global Step: 97000 Epoch 41/50 Iteration: 97000 Avg. Training loss: 2.8849 0.0165 sec/batch\n",
      "Epoch 42/50 Threshold: 0.06905878343510341 Length of Training words: 2305711\n",
      "Global Step: 97100 Epoch 42/50 Iteration: 97100 Avg. Training loss: 2.8886 0.0056 sec/batch\n",
      "Global Step: 97200 Epoch 42/50 Iteration: 97200 Avg. Training loss: 2.8876 0.0161 sec/batch\n",
      "Global Step: 97300 Epoch 42/50 Iteration: 97300 Avg. Training loss: 2.9018 0.0153 sec/batch\n",
      "Global Step: 97400 Epoch 42/50 Iteration: 97400 Avg. Training loss: 2.8790 0.0148 sec/batch\n",
      "Global Step: 97500 Epoch 42/50 Iteration: 97500 Avg. Training loss: 2.8686 0.0143 sec/batch\n",
      "Global Step: 97600 Epoch 42/50 Iteration: 97600 Avg. Training loss: 2.8547 0.0159 sec/batch\n",
      "Global Step: 97700 Epoch 42/50 Iteration: 97700 Avg. Training loss: 2.8821 0.0165 sec/batch\n",
      "Global Step: 97800 Epoch 42/50 Iteration: 97800 Avg. Training loss: 2.8767 0.0151 sec/batch\n",
      "Global Step: 97900 Epoch 42/50 Iteration: 97900 Avg. Training loss: 2.8810 0.0187 sec/batch\n",
      "Global Step: 98000 Epoch 42/50 Iteration: 98000 Avg. Training loss: 2.8814 0.0158 sec/batch\n",
      "Global Step: 98100 Epoch 42/50 Iteration: 98100 Avg. Training loss: 2.8569 0.0162 sec/batch\n",
      "Global Step: 98200 Epoch 42/50 Iteration: 98200 Avg. Training loss: 2.8645 0.0189 sec/batch\n",
      "Global Step: 98300 Epoch 42/50 Iteration: 98300 Avg. Training loss: 2.8741 0.0150 sec/batch\n",
      "Global Step: 98400 Epoch 42/50 Iteration: 98400 Avg. Training loss: 2.8784 0.0151 sec/batch\n",
      "Global Step: 98500 Epoch 42/50 Iteration: 98500 Avg. Training loss: 2.8714 0.0166 sec/batch\n",
      "Global Step: 98600 Epoch 42/50 Iteration: 98600 Avg. Training loss: 2.8792 0.0173 sec/batch\n",
      "Global Step: 98700 Epoch 42/50 Iteration: 98700 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 98800 Epoch 42/50 Iteration: 98800 Avg. Training loss: 2.8923 0.0146 sec/batch\n",
      "Global Step: 98900 Epoch 42/50 Iteration: 98900 Avg. Training loss: 2.8740 0.0138 sec/batch\n",
      "Global Step: 99000 Epoch 42/50 Iteration: 99000 Avg. Training loss: 2.8820 0.0144 sec/batch\n",
      "Global Step: 99100 Epoch 42/50 Iteration: 99100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 99200 Epoch 42/50 Iteration: 99200 Avg. Training loss: 2.8596 0.0159 sec/batch\n",
      "Global Step: 99300 Epoch 42/50 Iteration: 99300 Avg. Training loss: 2.8690 0.0170 sec/batch\n",
      "Epoch 43/50 Threshold: 0.06065749365533461 Length of Training words: 2235476\n",
      "Global Step: 99400 Epoch 43/50 Iteration: 99400 Avg. Training loss: 2.8792 0.0034 sec/batch\n",
      "Global Step: 99500 Epoch 43/50 Iteration: 99500 Avg. Training loss: 2.9161 0.0162 sec/batch\n",
      "Global Step: 99600 Epoch 43/50 Iteration: 99600 Avg. Training loss: 2.9233 0.0153 sec/batch\n",
      "Global Step: 99700 Epoch 43/50 Iteration: 99700 Avg. Training loss: 2.8968 0.0151 sec/batch\n",
      "Global Step: 99800 Epoch 43/50 Iteration: 99800 Avg. Training loss: 2.8904 0.0156 sec/batch\n",
      "Global Step: 99900 Epoch 43/50 Iteration: 99900 Avg. Training loss: 2.8856 0.0143 sec/batch\n",
      "Global Step: 100000 Epoch 43/50 Iteration: 100000 Avg. Training loss: 2.8945 0.0159 sec/batch\n",
      "Global Step: 100100 Epoch 43/50 Iteration: 100100 Avg. Training loss: 2.9092 0.0124 sec/batch\n",
      "Global Step: 100200 Epoch 43/50 Iteration: 100200 Avg. Training loss: 2.9013 0.0134 sec/batch\n",
      "Global Step: 100300 Epoch 43/50 Iteration: 100300 Avg. Training loss: 2.8949 0.0157 sec/batch\n",
      "Global Step: 100400 Epoch 43/50 Iteration: 100400 Avg. Training loss: 2.8842 0.0185 sec/batch\n",
      "Global Step: 100500 Epoch 43/50 Iteration: 100500 Avg. Training loss: 2.8981 0.0157 sec/batch\n",
      "Global Step: 100600 Epoch 43/50 Iteration: 100600 Avg. Training loss: 2.8920 0.0155 sec/batch\n",
      "Global Step: 100700 Epoch 43/50 Iteration: 100700 Avg. Training loss: 2.8917 0.0169 sec/batch\n",
      "Global Step: 100800 Epoch 43/50 Iteration: 100800 Avg. Training loss: 2.9001 0.0136 sec/batch\n",
      "Global Step: 100900 Epoch 43/50 Iteration: 100900 Avg. Training loss: 2.8770 0.0158 sec/batch\n",
      "Global Step: 101000 Epoch 43/50 Iteration: 101000 Avg. Training loss: 2.9147 0.0154 sec/batch\n",
      "Global Step: 101100 Epoch 43/50 Iteration: 101100 Avg. Training loss: 2.8994 0.0128 sec/batch\n",
      "Global Step: 101200 Epoch 43/50 Iteration: 101200 Avg. Training loss: 2.9157 0.0151 sec/batch\n",
      "Global Step: 101300 Epoch 43/50 Iteration: 101300 Avg. Training loss: 2.8816 0.0136 sec/batch\n",
      "Global Step: 101400 Epoch 43/50 Iteration: 101400 Avg. Training loss: 2.8681 0.0128 sec/batch\n",
      "Global Step: 101500 Epoch 43/50 Iteration: 101500 Avg. Training loss: 2.8983 0.0150 sec/batch\n",
      "Global Step: 101600 Epoch 43/50 Iteration: 101600 Avg. Training loss: 2.8918 0.0141 sec/batch\n",
      "Epoch 44/50 Threshold: 0.07064468846220132 Length of Training words: 2319882\n",
      "Global Step: 101700 Epoch 44/50 Iteration: 101700 Avg. Training loss: 2.8802 0.0135 sec/batch\n",
      "Global Step: 101800 Epoch 44/50 Iteration: 101800 Avg. Training loss: 2.9037 0.0139 sec/batch\n",
      "Global Step: 101900 Epoch 44/50 Iteration: 101900 Avg. Training loss: 2.8799 0.0148 sec/batch\n",
      "Global Step: 102000 Epoch 44/50 Iteration: 102000 Avg. Training loss: 2.8559 0.0163 sec/batch\n",
      "Global Step: 102100 Epoch 44/50 Iteration: 102100 Avg. Training loss: 2.8637 0.0165 sec/batch\n",
      "Global Step: 102200 Epoch 44/50 Iteration: 102200 Avg. Training loss: 2.8651 0.0165 sec/batch\n",
      "Global Step: 102300 Epoch 44/50 Iteration: 102300 Avg. Training loss: 2.8706 0.0147 sec/batch\n",
      "Global Step: 102400 Epoch 44/50 Iteration: 102400 Avg. Training loss: 2.8747 0.0145 sec/batch\n",
      "Global Step: 102500 Epoch 44/50 Iteration: 102500 Avg. Training loss: 2.8868 0.0137 sec/batch\n",
      "Global Step: 102600 Epoch 44/50 Iteration: 102600 Avg. Training loss: 2.8564 0.0145 sec/batch\n",
      "Global Step: 102700 Epoch 44/50 Iteration: 102700 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 102800 Epoch 44/50 Iteration: 102800 Avg. Training loss: 2.8858 0.0156 sec/batch\n",
      "Global Step: 102900 Epoch 44/50 Iteration: 102900 Avg. Training loss: 2.8561 0.0139 sec/batch\n",
      "Global Step: 103000 Epoch 44/50 Iteration: 103000 Avg. Training loss: 2.8760 0.0147 sec/batch\n",
      "Global Step: 103100 Epoch 44/50 Iteration: 103100 Avg. Training loss: 2.8720 0.0138 sec/batch\n",
      "Global Step: 103200 Epoch 44/50 Iteration: 103200 Avg. Training loss: 2.8514 0.0139 sec/batch\n",
      "Global Step: 103300 Epoch 44/50 Iteration: 103300 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 103400 Epoch 44/50 Iteration: 103400 Avg. Training loss: 2.8709 0.0161 sec/batch\n",
      "Global Step: 103500 Epoch 44/50 Iteration: 103500 Avg. Training loss: 2.8870 0.0152 sec/batch\n",
      "Global Step: 103600 Epoch 44/50 Iteration: 103600 Avg. Training loss: 2.8601 0.0167 sec/batch\n",
      "Global Step: 103700 Epoch 44/50 Iteration: 103700 Avg. Training loss: 2.8425 0.0152 sec/batch\n",
      "Global Step: 103800 Epoch 44/50 Iteration: 103800 Avg. Training loss: 2.8710 0.0137 sec/batch\n",
      "Global Step: 103900 Epoch 44/50 Iteration: 103900 Avg. Training loss: 2.8641 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Threshold: 0.07593991723833231 Length of Training words: 2358860\n",
      "Global Step: 104000 Epoch 45/50 Iteration: 104000 Avg. Training loss: 2.8658 0.0114 sec/batch\n",
      "Global Step: 104100 Epoch 45/50 Iteration: 104100 Avg. Training loss: 2.8793 0.0146 sec/batch\n",
      "Global Step: 104200 Epoch 45/50 Iteration: 104200 Avg. Training loss: 2.8787 0.0137 sec/batch\n",
      "Global Step: 104300 Epoch 45/50 Iteration: 104300 Avg. Training loss: 2.8509 0.0148 sec/batch\n",
      "Global Step: 104400 Epoch 45/50 Iteration: 104400 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 104500 Epoch 45/50 Iteration: 104500 Avg. Training loss: 2.8496 0.0140 sec/batch\n",
      "Global Step: 104600 Epoch 45/50 Iteration: 104600 Avg. Training loss: 2.8555 0.0157 sec/batch\n",
      "Global Step: 104700 Epoch 45/50 Iteration: 104700 Avg. Training loss: 2.8734 0.0144 sec/batch\n",
      "Global Step: 104800 Epoch 45/50 Iteration: 104800 Avg. Training loss: 2.8605 0.0169 sec/batch\n",
      "Global Step: 104900 Epoch 45/50 Iteration: 104900 Avg. Training loss: 2.8560 0.0176 sec/batch\n",
      "Global Step: 105000 Epoch 45/50 Iteration: 105000 Avg. Training loss: 2.8451 0.0167 sec/batch\n",
      "Global Step: 105100 Epoch 45/50 Iteration: 105100 Avg. Training loss: 2.8596 0.0154 sec/batch\n",
      "Global Step: 105200 Epoch 45/50 Iteration: 105200 Avg. Training loss: 2.8482 0.0153 sec/batch\n",
      "Global Step: 105300 Epoch 45/50 Iteration: 105300 Avg. Training loss: 2.8573 0.0152 sec/batch\n",
      "Global Step: 105400 Epoch 45/50 Iteration: 105400 Avg. Training loss: 2.8552 0.0155 sec/batch\n",
      "Global Step: 105500 Epoch 45/50 Iteration: 105500 Avg. Training loss: 2.8594 0.0152 sec/batch\n",
      "Global Step: 105600 Epoch 45/50 Iteration: 105600 Avg. Training loss: 2.8578 0.0136 sec/batch\n",
      "Global Step: 105700 Epoch 45/50 Iteration: 105700 Avg. Training loss: 2.8719 0.0152 sec/batch\n",
      "Global Step: 105800 Epoch 45/50 Iteration: 105800 Avg. Training loss: 2.8567 0.0153 sec/batch\n",
      "Global Step: 105900 Epoch 45/50 Iteration: 105900 Avg. Training loss: 2.8654 0.0128 sec/batch\n",
      "Global Step: 106000 Epoch 45/50 Iteration: 106000 Avg. Training loss: 2.8337 0.0138 sec/batch\n",
      "Global Step: 106100 Epoch 45/50 Iteration: 106100 Avg. Training loss: 2.8447 0.0169 sec/batch\n",
      "Global Step: 106200 Epoch 45/50 Iteration: 106200 Avg. Training loss: 2.8509 0.0156 sec/batch\n",
      "Epoch 46/50 Threshold: 0.07458249463143053 Length of Training words: 2350270\n",
      "Global Step: 106300 Epoch 46/50 Iteration: 106300 Avg. Training loss: 2.8559 0.0024 sec/batch\n",
      "Global Step: 106400 Epoch 46/50 Iteration: 106400 Avg. Training loss: 2.8692 0.0153 sec/batch\n",
      "Global Step: 106500 Epoch 46/50 Iteration: 106500 Avg. Training loss: 2.8916 0.0146 sec/batch\n",
      "Global Step: 106600 Epoch 46/50 Iteration: 106600 Avg. Training loss: 2.8670 0.0152 sec/batch\n",
      "Global Step: 106700 Epoch 46/50 Iteration: 106700 Avg. Training loss: 2.8505 0.0148 sec/batch\n",
      "Global Step: 106800 Epoch 46/50 Iteration: 106800 Avg. Training loss: 2.8512 0.0154 sec/batch\n",
      "Global Step: 106900 Epoch 46/50 Iteration: 106900 Avg. Training loss: 2.8534 0.0144 sec/batch\n",
      "Global Step: 107000 Epoch 46/50 Iteration: 107000 Avg. Training loss: 2.8648 0.0134 sec/batch\n",
      "Global Step: 107100 Epoch 46/50 Iteration: 107100 Avg. Training loss: 2.8622 0.0183 sec/batch\n",
      "Global Step: 107200 Epoch 46/50 Iteration: 107200 Avg. Training loss: 2.8821 0.0146 sec/batch\n",
      "Global Step: 107300 Epoch 46/50 Iteration: 107300 Avg. Training loss: 2.8442 0.0166 sec/batch\n",
      "Global Step: 107400 Epoch 46/50 Iteration: 107400 Avg. Training loss: 2.8414 0.0156 sec/batch\n",
      "Global Step: 107500 Epoch 46/50 Iteration: 107500 Avg. Training loss: 2.8783 0.0160 sec/batch\n",
      "Global Step: 107600 Epoch 46/50 Iteration: 107600 Avg. Training loss: 2.8440 0.0163 sec/batch\n",
      "Global Step: 107700 Epoch 46/50 Iteration: 107700 Avg. Training loss: 2.8680 0.0140 sec/batch\n",
      "Global Step: 107800 Epoch 46/50 Iteration: 107800 Avg. Training loss: 2.8608 0.0161 sec/batch\n",
      "Global Step: 107900 Epoch 46/50 Iteration: 107900 Avg. Training loss: 2.8416 0.0155 sec/batch\n",
      "Global Step: 108000 Epoch 46/50 Iteration: 108000 Avg. Training loss: 2.8843 0.0150 sec/batch\n",
      "Global Step: 108100 Epoch 46/50 Iteration: 108100 Avg. Training loss: 2.8579 0.0149 sec/batch\n",
      "Global Step: 108200 Epoch 46/50 Iteration: 108200 Avg. Training loss: 2.8778 0.0148 sec/batch\n",
      "Global Step: 108300 Epoch 46/50 Iteration: 108300 Avg. Training loss: 2.8508 0.0152 sec/batch\n",
      "Global Step: 108400 Epoch 46/50 Iteration: 108400 Avg. Training loss: 2.8374 0.0154 sec/batch\n",
      "Global Step: 108500 Epoch 46/50 Iteration: 108500 Avg. Training loss: 2.8620 0.0161 sec/batch\n",
      "Global Step: 108600 Epoch 46/50 Iteration: 108600 Avg. Training loss: 2.8523 0.0139 sec/batch\n",
      "Epoch 47/50 Threshold: 0.07293145990513991 Length of Training words: 2337022\n",
      "Global Step: 108700 Epoch 47/50 Iteration: 108700 Avg. Training loss: 2.8708 0.0092 sec/batch\n",
      "Global Step: 108800 Epoch 47/50 Iteration: 108800 Avg. Training loss: 2.8870 0.0129 sec/batch\n",
      "Global Step: 108900 Epoch 47/50 Iteration: 108900 Avg. Training loss: 2.8835 0.0147 sec/batch\n",
      "Global Step: 109000 Epoch 47/50 Iteration: 109000 Avg. Training loss: 2.8593 0.0136 sec/batch\n",
      "Global Step: 109100 Epoch 47/50 Iteration: 109100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 109200 Epoch 47/50 Iteration: 109200 Avg. Training loss: 2.8550 0.0148 sec/batch\n",
      "Global Step: 109300 Epoch 47/50 Iteration: 109300 Avg. Training loss: 2.8643 0.0150 sec/batch\n",
      "Global Step: 109400 Epoch 47/50 Iteration: 109400 Avg. Training loss: 2.8806 0.0133 sec/batch\n",
      "Global Step: 109500 Epoch 47/50 Iteration: 109500 Avg. Training loss: 2.8665 0.0158 sec/batch\n",
      "Global Step: 109600 Epoch 47/50 Iteration: 109600 Avg. Training loss: 2.8622 0.0177 sec/batch\n",
      "Global Step: 109700 Epoch 47/50 Iteration: 109700 Avg. Training loss: 2.8536 0.0176 sec/batch\n",
      "Global Step: 109800 Epoch 47/50 Iteration: 109800 Avg. Training loss: 2.8700 0.0164 sec/batch\n",
      "Global Step: 109900 Epoch 47/50 Iteration: 109900 Avg. Training loss: 2.8559 0.0151 sec/batch\n",
      "Global Step: 110000 Epoch 47/50 Iteration: 110000 Avg. Training loss: 2.8598 0.0162 sec/batch\n",
      "Global Step: 110100 Epoch 47/50 Iteration: 110100 Avg. Training loss: 2.8601 0.0161 sec/batch\n",
      "Global Step: 110200 Epoch 47/50 Iteration: 110200 Avg. Training loss: 2.8660 0.0161 sec/batch\n",
      "Global Step: 110300 Epoch 47/50 Iteration: 110300 Avg. Training loss: 2.8643 0.0162 sec/batch\n",
      "Global Step: 110400 Epoch 47/50 Iteration: 110400 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 110500 Epoch 47/50 Iteration: 110500 Avg. Training loss: 2.8634 0.0154 sec/batch\n",
      "Global Step: 110600 Epoch 47/50 Iteration: 110600 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 110700 Epoch 47/50 Iteration: 110700 Avg. Training loss: 2.8442 0.0165 sec/batch\n",
      "Global Step: 110800 Epoch 47/50 Iteration: 110800 Avg. Training loss: 2.8501 0.0166 sec/batch\n",
      "Global Step: 110900 Epoch 47/50 Iteration: 110900 Avg. Training loss: 2.8555 0.0148 sec/batch\n",
      "Epoch 48/50 Threshold: 0.06985650474361509 Length of Training words: 2312860\n",
      "Global Step: 111000 Epoch 48/50 Iteration: 111000 Avg. Training loss: 2.8629 0.0047 sec/batch\n",
      "Global Step: 111100 Epoch 48/50 Iteration: 111100 Avg. Training loss: 2.8838 0.0139 sec/batch\n",
      "Global Step: 111200 Epoch 48/50 Iteration: 111200 Avg. Training loss: 2.9036 0.0141 sec/batch\n",
      "Global Step: 111300 Epoch 48/50 Iteration: 111300 Avg. Training loss: 2.8771 0.0161 sec/batch\n",
      "Global Step: 111400 Epoch 48/50 Iteration: 111400 Avg. Training loss: 2.8649 0.0138 sec/batch\n",
      "Global Step: 111500 Epoch 48/50 Iteration: 111500 Avg. Training loss: 2.8544 0.0125 sec/batch\n",
      "Global Step: 111600 Epoch 48/50 Iteration: 111600 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 111700 Epoch 48/50 Iteration: 111700 Avg. Training loss: 2.8774 0.0144 sec/batch\n",
      "Global Step: 111800 Epoch 48/50 Iteration: 111800 Avg. Training loss: 2.8724 0.0117 sec/batch\n",
      "Global Step: 111900 Epoch 48/50 Iteration: 111900 Avg. Training loss: 2.8840 0.0141 sec/batch\n",
      "Global Step: 112000 Epoch 48/50 Iteration: 112000 Avg. Training loss: 2.8562 0.0156 sec/batch\n",
      "Global Step: 112100 Epoch 48/50 Iteration: 112100 Avg. Training loss: 2.8637 0.0178 sec/batch\n",
      "Global Step: 112200 Epoch 48/50 Iteration: 112200 Avg. Training loss: 2.8711 0.0175 sec/batch\n",
      "Global Step: 112300 Epoch 48/50 Iteration: 112300 Avg. Training loss: 2.8781 0.0157 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 112400 Epoch 48/50 Iteration: 112400 Avg. Training loss: 2.8706 0.0165 sec/batch\n",
      "Global Step: 112500 Epoch 48/50 Iteration: 112500 Avg. Training loss: 2.8753 0.0185 sec/batch\n",
      "Global Step: 112600 Epoch 48/50 Iteration: 112600 Avg. Training loss: 2.8575 0.0160 sec/batch\n",
      "Global Step: 112700 Epoch 48/50 Iteration: 112700 Avg. Training loss: 2.8922 0.0139 sec/batch\n",
      "Global Step: 112800 Epoch 48/50 Iteration: 112800 Avg. Training loss: 2.8744 0.0159 sec/batch\n",
      "Global Step: 112900 Epoch 48/50 Iteration: 112900 Avg. Training loss: 2.8827 0.0152 sec/batch\n",
      "Global Step: 113000 Epoch 48/50 Iteration: 113000 Avg. Training loss: 2.8446 0.0156 sec/batch\n",
      "Global Step: 113100 Epoch 48/50 Iteration: 113100 Avg. Training loss: 2.8607 0.0175 sec/batch\n",
      "Global Step: 113200 Epoch 48/50 Iteration: 113200 Avg. Training loss: 2.8686 0.0163 sec/batch\n",
      "Epoch 49/50 Threshold: 0.07556541599744111 Length of Training words: 2356231\n",
      "Global Step: 113300 Epoch 49/50 Iteration: 113300 Avg. Training loss: 2.8701 0.0023 sec/batch\n",
      "Global Step: 113400 Epoch 49/50 Iteration: 113400 Avg. Training loss: 2.8662 0.0152 sec/batch\n",
      "Global Step: 113500 Epoch 49/50 Iteration: 113500 Avg. Training loss: 2.8920 0.0169 sec/batch\n",
      "Global Step: 113600 Epoch 49/50 Iteration: 113600 Avg. Training loss: 2.8648 0.0153 sec/batch\n",
      "Global Step: 113700 Epoch 49/50 Iteration: 113700 Avg. Training loss: 2.8487 0.0147 sec/batch\n",
      "Global Step: 113800 Epoch 49/50 Iteration: 113800 Avg. Training loss: 2.8506 0.0156 sec/batch\n",
      "Global Step: 113900 Epoch 49/50 Iteration: 113900 Avg. Training loss: 2.8528 0.0130 sec/batch\n",
      "Global Step: 114000 Epoch 49/50 Iteration: 114000 Avg. Training loss: 2.8622 0.0158 sec/batch\n",
      "Global Step: 114100 Epoch 49/50 Iteration: 114100 Avg. Training loss: 2.8607 0.0137 sec/batch\n",
      "Global Step: 114200 Epoch 49/50 Iteration: 114200 Avg. Training loss: 2.8839 0.0116 sec/batch\n",
      "Global Step: 114300 Epoch 49/50 Iteration: 114300 Avg. Training loss: 2.8428 0.0129 sec/batch\n",
      "Global Step: 114400 Epoch 49/50 Iteration: 114400 Avg. Training loss: 2.8368 0.0153 sec/batch\n",
      "Global Step: 114500 Epoch 49/50 Iteration: 114500 Avg. Training loss: 2.8732 0.0148 sec/batch\n",
      "Global Step: 114600 Epoch 49/50 Iteration: 114600 Avg. Training loss: 2.8440 0.0137 sec/batch\n",
      "Global Step: 114700 Epoch 49/50 Iteration: 114700 Avg. Training loss: 2.8673 0.0145 sec/batch\n",
      "Global Step: 114800 Epoch 49/50 Iteration: 114800 Avg. Training loss: 2.8568 0.0178 sec/batch\n",
      "Global Step: 114900 Epoch 49/50 Iteration: 114900 Avg. Training loss: 2.8395 0.0161 sec/batch\n",
      "Global Step: 115000 Epoch 49/50 Iteration: 115000 Avg. Training loss: 2.8809 0.0165 sec/batch\n",
      "Global Step: 115100 Epoch 49/50 Iteration: 115100 Avg. Training loss: 2.8554 0.0174 sec/batch\n",
      "Global Step: 115200 Epoch 49/50 Iteration: 115200 Avg. Training loss: 2.8734 0.0160 sec/batch\n",
      "Global Step: 115300 Epoch 49/50 Iteration: 115300 Avg. Training loss: 2.8501 0.0163 sec/batch\n",
      "Global Step: 115400 Epoch 49/50 Iteration: 115400 Avg. Training loss: 2.8378 0.0137 sec/batch\n",
      "Global Step: 115500 Epoch 49/50 Iteration: 115500 Avg. Training loss: 2.8551 0.0174 sec/batch\n",
      "Global Step: 115600 Epoch 49/50 Iteration: 115600 Avg. Training loss: 2.8550 0.0152 sec/batch\n",
      "Epoch 50/50 Threshold: 0.0823238264796941 Length of Training words: 2399910\n",
      "Global Step: 115700 Epoch 50/50 Iteration: 115700 Avg. Training loss: 2.8504 0.0103 sec/batch\n",
      "Global Step: 115800 Epoch 50/50 Iteration: 115800 Avg. Training loss: 2.8617 0.0149 sec/batch\n",
      "Global Step: 115900 Epoch 50/50 Iteration: 115900 Avg. Training loss: 2.8669 0.0154 sec/batch\n",
      "Global Step: 116000 Epoch 50/50 Iteration: 116000 Avg. Training loss: 2.8363 0.0146 sec/batch\n",
      "Global Step: 116100 Epoch 50/50 Iteration: 116100 Avg. Training loss: 2.8398 0.0156 sec/batch\n",
      "Global Step: 116200 Epoch 50/50 Iteration: 116200 Avg. Training loss: 2.8349 0.0153 sec/batch\n",
      "Global Step: 116300 Epoch 50/50 Iteration: 116300 Avg. Training loss: 2.8455 0.0153 sec/batch\n",
      "Global Step: 116400 Epoch 50/50 Iteration: 116400 Avg. Training loss: 2.8501 0.0135 sec/batch\n",
      "Global Step: 116500 Epoch 50/50 Iteration: 116500 Avg. Training loss: 2.8486 0.0170 sec/batch\n",
      "Global Step: 116600 Epoch 50/50 Iteration: 116600 Avg. Training loss: 2.8564 0.0153 sec/batch\n",
      "Global Step: 116700 Epoch 50/50 Iteration: 116700 Avg. Training loss: 2.8292 0.0132 sec/batch\n",
      "Global Step: 116800 Epoch 50/50 Iteration: 116800 Avg. Training loss: 2.8331 0.0137 sec/batch\n",
      "Global Step: 116900 Epoch 50/50 Iteration: 116900 Avg. Training loss: 2.8493 0.0138 sec/batch\n",
      "Global Step: 117000 Epoch 50/50 Iteration: 117000 Avg. Training loss: 2.8371 0.0149 sec/batch\n",
      "Global Step: 117100 Epoch 50/50 Iteration: 117100 Avg. Training loss: 2.8497 0.0154 sec/batch\n",
      "Global Step: 117200 Epoch 50/50 Iteration: 117200 Avg. Training loss: 2.8351 0.0131 sec/batch\n",
      "Global Step: 117300 Epoch 50/50 Iteration: 117300 Avg. Training loss: 2.8299 0.0124 sec/batch\n",
      "Global Step: 117400 Epoch 50/50 Iteration: 117400 Avg. Training loss: 2.8669 0.0193 sec/batch\n",
      "Global Step: 117500 Epoch 50/50 Iteration: 117500 Avg. Training loss: 2.8450 0.0135 sec/batch\n",
      "Global Step: 117600 Epoch 50/50 Iteration: 117600 Avg. Training loss: 2.8595 0.0148 sec/batch\n",
      "Global Step: 117700 Epoch 50/50 Iteration: 117700 Avg. Training loss: 2.8310 0.0169 sec/batch\n",
      "Global Step: 117800 Epoch 50/50 Iteration: 117800 Avg. Training loss: 2.8190 0.0164 sec/batch\n",
      "Global Step: 117900 Epoch 50/50 Iteration: 117900 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Global Step: 118000 Epoch 50/50 Iteration: 118000 Avg. Training loss: 2.8372 0.0135 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/pos/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/pos5/pos.ckpt\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos5'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB14AAAcMCAYAAAAHCCGfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIcErAAAAAAAAQEOCVwAAAAAAAICG/j8HMdlqGjE/GAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2aa43cabe0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 902,
       "width": 943
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
