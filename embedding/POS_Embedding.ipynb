{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.05, 0.08)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Threshold: 0.044622917170810975 Length of Training words: 2054860\n",
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 6.3688 0.0290 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 4.7142 0.0160 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 3.8906 0.0185 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 3.4649 0.0181 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 3.2752 0.0166 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 3.1614 0.0141 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 3.1091 0.0194 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 3.0812 0.0168 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 3.0223 0.0195 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 3.0149 0.0178 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 3.0031 0.0164 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 3.0049 0.0183 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 2.9894 0.0184 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 2.9679 0.0163 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 3.0012 0.0158 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 2.9768 0.0164 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 2.9873 0.0175 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 2.9468 0.0168 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 2.9510 0.0210 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 2.9573 0.0167 sec/batch\n",
      "Epoch 2/50 Threshold: 0.0719227696404879 Length of Training words: 2329427\n",
      "Global Step: 2100 Epoch 2/50 Iteration: 2100 Avg. Training loss: 2.9356 0.0074 sec/batch\n",
      "Global Step: 2200 Epoch 2/50 Iteration: 2200 Avg. Training loss: 2.8984 0.0147 sec/batch\n",
      "Global Step: 2300 Epoch 2/50 Iteration: 2300 Avg. Training loss: 2.8913 0.0134 sec/batch\n",
      "Global Step: 2400 Epoch 2/50 Iteration: 2400 Avg. Training loss: 2.8801 0.0200 sec/batch\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 2.8690 0.0158 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 2.8656 0.0180 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 2.8769 0.0151 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 2.8803 0.0172 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 2.8782 0.0160 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 2.8781 0.0166 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 2.8552 0.0158 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 2.8636 0.0150 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 2.8737 0.0157 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 2.8781 0.0183 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 2.8718 0.0173 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 2.8768 0.0205 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 2.8625 0.0170 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 2.8948 0.0200 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 2.8720 0.0171 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 2.8827 0.0173 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 2.8451 0.0168 sec/batch\n",
      "Global Step: 4200 Epoch 2/50 Iteration: 4200 Avg. Training loss: 2.8581 0.0176 sec/batch\n",
      "Global Step: 4300 Epoch 2/50 Iteration: 4300 Avg. Training loss: 2.8641 0.0195 sec/batch\n",
      "Epoch 3/50 Threshold: 0.07300470054910907 Length of Training words: 2337264\n",
      "Global Step: 4400 Epoch 3/50 Iteration: 4400 Avg. Training loss: 2.8687 0.0035 sec/batch\n",
      "Global Step: 4500 Epoch 3/50 Iteration: 4500 Avg. Training loss: 2.8758 0.0145 sec/batch\n",
      "Global Step: 4600 Epoch 3/50 Iteration: 4600 Avg. Training loss: 2.9004 0.0184 sec/batch\n",
      "Global Step: 4700 Epoch 3/50 Iteration: 4700 Avg. Training loss: 2.8764 0.0203 sec/batch\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 2.8607 0.0172 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 2.8585 0.0161 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 2.8638 0.0158 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 2.8722 0.0149 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 2.8698 0.0169 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 2.8942 0.0164 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 2.8459 0.0197 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 2.8516 0.0181 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 2.8793 0.0203 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 2.8520 0.0155 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 2.8735 0.0150 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 2.8664 0.0181 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 2.8481 0.0174 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 2.8896 0.0160 sec/batch\n",
      "Global Step: 6200 Epoch 3/50 Iteration: 6200 Avg. Training loss: 2.8680 0.0182 sec/batch\n",
      "Global Step: 6300 Epoch 3/50 Iteration: 6300 Avg. Training loss: 2.8856 0.0167 sec/batch\n",
      "Global Step: 6400 Epoch 3/50 Iteration: 6400 Avg. Training loss: 2.8494 0.0182 sec/batch\n",
      "Global Step: 6500 Epoch 3/50 Iteration: 6500 Avg. Training loss: 2.8395 0.0171 sec/batch\n",
      "Global Step: 6600 Epoch 3/50 Iteration: 6600 Avg. Training loss: 2.8754 0.0187 sec/batch\n",
      "Global Step: 6700 Epoch 3/50 Iteration: 6700 Avg. Training loss: 2.8590 0.0177 sec/batch\n",
      "Epoch 4/50 Threshold: 0.05146883671173269 Length of Training words: 2141850\n",
      "Global Step: 6800 Epoch 4/50 Iteration: 6800 Avg. Training loss: 2.9164 0.0115 sec/batch\n",
      "Global Step: 6900 Epoch 4/50 Iteration: 6900 Avg. Training loss: 2.9622 0.0154 sec/batch\n",
      "Global Step: 7000 Epoch 4/50 Iteration: 7000 Avg. Training loss: 2.9374 0.0126 sec/batch\n",
      "Global Step: 7100 Epoch 4/50 Iteration: 7100 Avg. Training loss: 2.9160 0.0146 sec/batch\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 2.9165 0.0142 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 2.9321 0.0160 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 2.9255 0.0184 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 2.9324 0.0175 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 2.9319 0.0159 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 2.9146 0.0219 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 2.9243 0.0173 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 2.9223 0.0141 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 2.9281 0.0179 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 2.9296 0.0170 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 2.9097 0.0188 sec/batch\n",
      "Global Step: 8300 Epoch 4/50 Iteration: 8300 Avg. Training loss: 2.9446 0.0164 sec/batch\n",
      "Global Step: 8400 Epoch 4/50 Iteration: 8400 Avg. Training loss: 2.9337 0.0149 sec/batch\n",
      "Global Step: 8500 Epoch 4/50 Iteration: 8500 Avg. Training loss: 2.9393 0.0159 sec/batch\n",
      "Global Step: 8600 Epoch 4/50 Iteration: 8600 Avg. Training loss: 2.9048 0.0134 sec/batch\n",
      "Global Step: 8700 Epoch 4/50 Iteration: 8700 Avg. Training loss: 2.9144 0.0133 sec/batch\n",
      "Global Step: 8800 Epoch 4/50 Iteration: 8800 Avg. Training loss: 2.9197 0.0158 sec/batch\n",
      "Epoch 5/50 Threshold: 0.0461214770859633 Length of Training words: 2075181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 8900 Epoch 5/50 Iteration: 8900 Avg. Training loss: 2.9409 0.0058 sec/batch\n",
      "Global Step: 9000 Epoch 5/50 Iteration: 9000 Avg. Training loss: 2.9642 0.0135 sec/batch\n",
      "Global Step: 9100 Epoch 5/50 Iteration: 9100 Avg. Training loss: 2.9695 0.0163 sec/batch\n",
      "Global Step: 9200 Epoch 5/50 Iteration: 9200 Avg. Training loss: 2.9397 0.0173 sec/batch\n",
      "Global Step: 9300 Epoch 5/50 Iteration: 9300 Avg. Training loss: 2.9393 0.0160 sec/batch\n",
      "Global Step: 9400 Epoch 5/50 Iteration: 9400 Avg. Training loss: 2.9384 0.0169 sec/batch\n",
      "Global Step: 9500 Epoch 5/50 Iteration: 9500 Avg. Training loss: 2.9518 0.0175 sec/batch\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 2.9466 0.0195 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 2.9555 0.0164 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 2.9248 0.0190 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 2.9572 0.0233 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 2.9410 0.0180 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 2.9470 0.0190 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 2.9506 0.0166 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 2.9295 0.0177 sec/batch\n",
      "Global Step: 10400 Epoch 5/50 Iteration: 10400 Avg. Training loss: 2.9695 0.0164 sec/batch\n",
      "Global Step: 10500 Epoch 5/50 Iteration: 10500 Avg. Training loss: 2.9518 0.0194 sec/batch\n",
      "Global Step: 10600 Epoch 5/50 Iteration: 10600 Avg. Training loss: 2.9506 0.0175 sec/batch\n",
      "Global Step: 10700 Epoch 5/50 Iteration: 10700 Avg. Training loss: 2.9297 0.0156 sec/batch\n",
      "Global Step: 10800 Epoch 5/50 Iteration: 10800 Avg. Training loss: 2.9302 0.0155 sec/batch\n",
      "Global Step: 10900 Epoch 5/50 Iteration: 10900 Avg. Training loss: 2.9467 0.0165 sec/batch\n",
      "Epoch 6/50 Threshold: 0.056001474022455824 Length of Training words: 2192227\n",
      "Global Step: 11000 Epoch 6/50 Iteration: 11000 Avg. Training loss: 2.9267 0.0097 sec/batch\n",
      "Global Step: 11100 Epoch 6/50 Iteration: 11100 Avg. Training loss: 2.9285 0.0160 sec/batch\n",
      "Global Step: 11200 Epoch 6/50 Iteration: 11200 Avg. Training loss: 2.9297 0.0164 sec/batch\n",
      "Global Step: 11300 Epoch 6/50 Iteration: 11300 Avg. Training loss: 2.8963 0.0152 sec/batch\n",
      "Global Step: 11400 Epoch 6/50 Iteration: 11400 Avg. Training loss: 2.9087 0.0163 sec/batch\n",
      "Global Step: 11500 Epoch 6/50 Iteration: 11500 Avg. Training loss: 2.9063 0.0141 sec/batch\n",
      "Global Step: 11600 Epoch 6/50 Iteration: 11600 Avg. Training loss: 2.9148 0.0131 sec/batch\n",
      "Global Step: 11700 Epoch 6/50 Iteration: 11700 Avg. Training loss: 2.9131 0.0169 sec/batch\n",
      "Global Step: 11800 Epoch 6/50 Iteration: 11800 Avg. Training loss: 2.9337 0.0193 sec/batch\n",
      "Global Step: 11900 Epoch 6/50 Iteration: 11900 Avg. Training loss: 2.8910 0.0163 sec/batch\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 2.9051 0.0141 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 2.9073 0.0170 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 2.9146 0.0159 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 2.9050 0.0165 sec/batch\n",
      "Global Step: 12400 Epoch 6/50 Iteration: 12400 Avg. Training loss: 2.9140 0.0145 sec/batch\n",
      "Global Step: 12500 Epoch 6/50 Iteration: 12500 Avg. Training loss: 2.9088 0.0151 sec/batch\n",
      "Global Step: 12600 Epoch 6/50 Iteration: 12600 Avg. Training loss: 2.9255 0.0155 sec/batch\n",
      "Global Step: 12700 Epoch 6/50 Iteration: 12700 Avg. Training loss: 2.9221 0.0188 sec/batch\n",
      "Global Step: 12800 Epoch 6/50 Iteration: 12800 Avg. Training loss: 2.9135 0.0177 sec/batch\n",
      "Global Step: 12900 Epoch 6/50 Iteration: 12900 Avg. Training loss: 2.8879 0.0181 sec/batch\n",
      "Global Step: 13000 Epoch 6/50 Iteration: 13000 Avg. Training loss: 2.9060 0.0201 sec/batch\n",
      "Global Step: 13100 Epoch 6/50 Iteration: 13100 Avg. Training loss: 2.9063 0.0200 sec/batch\n",
      "Epoch 7/50 Threshold: 0.06130474364872056 Length of Training words: 2241014\n",
      "Global Step: 13200 Epoch 7/50 Iteration: 13200 Avg. Training loss: 2.9089 0.0140 sec/batch\n",
      "Global Step: 13300 Epoch 7/50 Iteration: 13300 Avg. Training loss: 2.9173 0.0184 sec/batch\n",
      "Global Step: 13400 Epoch 7/50 Iteration: 13400 Avg. Training loss: 2.9136 0.0199 sec/batch\n",
      "Global Step: 13500 Epoch 7/50 Iteration: 13500 Avg. Training loss: 2.8836 0.0211 sec/batch\n",
      "Global Step: 13600 Epoch 7/50 Iteration: 13600 Avg. Training loss: 2.8912 0.0185 sec/batch\n",
      "Global Step: 13700 Epoch 7/50 Iteration: 13700 Avg. Training loss: 2.8924 0.0192 sec/batch\n",
      "Global Step: 13800 Epoch 7/50 Iteration: 13800 Avg. Training loss: 2.8973 0.0188 sec/batch\n",
      "Global Step: 13900 Epoch 7/50 Iteration: 13900 Avg. Training loss: 2.8989 0.0175 sec/batch\n",
      "Global Step: 14000 Epoch 7/50 Iteration: 14000 Avg. Training loss: 2.9187 0.0157 sec/batch\n",
      "Global Step: 14100 Epoch 7/50 Iteration: 14100 Avg. Training loss: 2.8800 0.0175 sec/batch\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 2.8841 0.0167 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 2.9056 0.0165 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 2.8914 0.0153 sec/batch\n",
      "Global Step: 14500 Epoch 7/50 Iteration: 14500 Avg. Training loss: 2.8999 0.0190 sec/batch\n",
      "Global Step: 14600 Epoch 7/50 Iteration: 14600 Avg. Training loss: 2.8964 0.0206 sec/batch\n",
      "Global Step: 14700 Epoch 7/50 Iteration: 14700 Avg. Training loss: 2.8828 0.0187 sec/batch\n",
      "Global Step: 14800 Epoch 7/50 Iteration: 14800 Avg. Training loss: 2.9194 0.0155 sec/batch\n",
      "Global Step: 14900 Epoch 7/50 Iteration: 14900 Avg. Training loss: 2.9022 0.0159 sec/batch\n",
      "Global Step: 15000 Epoch 7/50 Iteration: 15000 Avg. Training loss: 2.9050 0.0155 sec/batch\n",
      "Global Step: 15100 Epoch 7/50 Iteration: 15100 Avg. Training loss: 2.8732 0.0139 sec/batch\n",
      "Global Step: 15200 Epoch 7/50 Iteration: 15200 Avg. Training loss: 2.8803 0.0143 sec/batch\n",
      "Global Step: 15300 Epoch 7/50 Iteration: 15300 Avg. Training loss: 2.8899 0.0215 sec/batch\n",
      "Epoch 8/50 Threshold: 0.04906450104505607 Length of Training words: 2113607\n",
      "Global Step: 15400 Epoch 8/50 Iteration: 15400 Avg. Training loss: 2.9085 0.0059 sec/batch\n",
      "Global Step: 15500 Epoch 8/50 Iteration: 15500 Avg. Training loss: 2.9557 0.0164 sec/batch\n",
      "Global Step: 15600 Epoch 8/50 Iteration: 15600 Avg. Training loss: 2.9567 0.0165 sec/batch\n",
      "Global Step: 15700 Epoch 8/50 Iteration: 15700 Avg. Training loss: 2.9328 0.0160 sec/batch\n",
      "Global Step: 15800 Epoch 8/50 Iteration: 15800 Avg. Training loss: 2.9299 0.0163 sec/batch\n",
      "Global Step: 15900 Epoch 8/50 Iteration: 15900 Avg. Training loss: 2.9281 0.0168 sec/batch\n",
      "Global Step: 16000 Epoch 8/50 Iteration: 16000 Avg. Training loss: 2.9328 0.0206 sec/batch\n",
      "Global Step: 16100 Epoch 8/50 Iteration: 16100 Avg. Training loss: 2.9380 0.0208 sec/batch\n",
      "Global Step: 16200 Epoch 8/50 Iteration: 16200 Avg. Training loss: 2.9570 0.0219 sec/batch\n",
      "Global Step: 16300 Epoch 8/50 Iteration: 16300 Avg. Training loss: 2.9119 0.0186 sec/batch\n",
      "Global Step: 16400 Epoch 8/50 Iteration: 16400 Avg. Training loss: 2.9274 0.0185 sec/batch\n",
      "Global Step: 16500 Epoch 8/50 Iteration: 16500 Avg. Training loss: 2.9295 0.0178 sec/batch\n",
      "Global Step: 16600 Epoch 8/50 Iteration: 16600 Avg. Training loss: 2.9403 0.0159 sec/batch\n",
      "Global Step: 16700 Epoch 8/50 Iteration: 16700 Avg. Training loss: 2.9316 0.0168 sec/batch\n",
      "Global Step: 16800 Epoch 8/50 Iteration: 16800 Avg. Training loss: 2.9273 0.0148 sec/batch\n",
      "Global Step: 16900 Epoch 8/50 Iteration: 16900 Avg. Training loss: 2.9499 0.0190 sec/batch\n",
      "Global Step: 17000 Epoch 8/50 Iteration: 17000 Avg. Training loss: 2.9371 0.0207 sec/batch\n",
      "Global Step: 17100 Epoch 8/50 Iteration: 17100 Avg. Training loss: 2.9536 0.0189 sec/batch\n",
      "Global Step: 17200 Epoch 8/50 Iteration: 17200 Avg. Training loss: 2.9152 0.0185 sec/batch\n",
      "Global Step: 17300 Epoch 8/50 Iteration: 17300 Avg. Training loss: 2.9159 0.0197 sec/batch\n",
      "Global Step: 17400 Epoch 8/50 Iteration: 17400 Avg. Training loss: 2.9263 0.0167 sec/batch\n",
      "Epoch 9/50 Threshold: 0.0784940785554667 Length of Training words: 2375185\n",
      "Global Step: 17500 Epoch 9/50 Iteration: 17500 Avg. Training loss: 2.9250 0.0024 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17600 Epoch 9/50 Iteration: 17600 Avg. Training loss: 2.8605 0.0160 sec/batch\n",
      "Global Step: 17700 Epoch 9/50 Iteration: 17700 Avg. Training loss: 2.8844 0.0149 sec/batch\n",
      "Global Step: 17800 Epoch 9/50 Iteration: 17800 Avg. Training loss: 2.8572 0.0164 sec/batch\n",
      "Global Step: 17900 Epoch 9/50 Iteration: 17900 Avg. Training loss: 2.8452 0.0181 sec/batch\n",
      "Global Step: 18000 Epoch 9/50 Iteration: 18000 Avg. Training loss: 2.8478 0.0173 sec/batch\n",
      "Global Step: 18100 Epoch 9/50 Iteration: 18100 Avg. Training loss: 2.8486 0.0187 sec/batch\n",
      "Global Step: 18200 Epoch 9/50 Iteration: 18200 Avg. Training loss: 2.8590 0.0157 sec/batch\n",
      "Global Step: 18300 Epoch 9/50 Iteration: 18300 Avg. Training loss: 2.8543 0.0152 sec/batch\n",
      "Global Step: 18400 Epoch 9/50 Iteration: 18400 Avg. Training loss: 2.8754 0.0182 sec/batch\n",
      "Global Step: 18500 Epoch 9/50 Iteration: 18500 Avg. Training loss: 2.8407 0.0174 sec/batch\n",
      "Global Step: 18600 Epoch 9/50 Iteration: 18600 Avg. Training loss: 2.8330 0.0156 sec/batch\n",
      "Global Step: 18700 Epoch 9/50 Iteration: 18700 Avg. Training loss: 2.8663 0.0187 sec/batch\n",
      "Global Step: 18800 Epoch 9/50 Iteration: 18800 Avg. Training loss: 2.8443 0.0181 sec/batch\n",
      "Global Step: 18900 Epoch 9/50 Iteration: 18900 Avg. Training loss: 2.8553 0.0188 sec/batch\n",
      "Global Step: 19000 Epoch 9/50 Iteration: 19000 Avg. Training loss: 2.8601 0.0180 sec/batch\n",
      "Global Step: 19100 Epoch 9/50 Iteration: 19100 Avg. Training loss: 2.8325 0.0182 sec/batch\n",
      "Global Step: 19200 Epoch 9/50 Iteration: 19200 Avg. Training loss: 2.8747 0.0177 sec/batch\n",
      "Global Step: 19300 Epoch 9/50 Iteration: 19300 Avg. Training loss: 2.8533 0.0167 sec/batch\n",
      "Global Step: 19400 Epoch 9/50 Iteration: 19400 Avg. Training loss: 2.8730 0.0161 sec/batch\n",
      "Global Step: 19500 Epoch 9/50 Iteration: 19500 Avg. Training loss: 2.8571 0.0164 sec/batch\n",
      "Global Step: 19600 Epoch 9/50 Iteration: 19600 Avg. Training loss: 2.8303 0.0155 sec/batch\n",
      "Global Step: 19700 Epoch 9/50 Iteration: 19700 Avg. Training loss: 2.8385 0.0197 sec/batch\n",
      "Global Step: 19800 Epoch 9/50 Iteration: 19800 Avg. Training loss: 2.8473 0.0159 sec/batch\n",
      "Epoch 10/50 Threshold: 0.05129761489882203 Length of Training words: 2139473\n",
      "Global Step: 19900 Epoch 10/50 Iteration: 19900 Avg. Training loss: 2.8917 0.0062 sec/batch\n",
      "Global Step: 20000 Epoch 10/50 Iteration: 20000 Avg. Training loss: 2.9464 0.0188 sec/batch\n",
      "Global Step: 20100 Epoch 10/50 Iteration: 20100 Avg. Training loss: 2.9458 0.0131 sec/batch\n",
      "Global Step: 20200 Epoch 10/50 Iteration: 20200 Avg. Training loss: 2.9199 0.0146 sec/batch\n",
      "Global Step: 20300 Epoch 10/50 Iteration: 20300 Avg. Training loss: 2.9160 0.0199 sec/batch\n",
      "Global Step: 20400 Epoch 10/50 Iteration: 20400 Avg. Training loss: 2.9215 0.0189 sec/batch\n",
      "Global Step: 20500 Epoch 10/50 Iteration: 20500 Avg. Training loss: 2.9264 0.0157 sec/batch\n",
      "Global Step: 20600 Epoch 10/50 Iteration: 20600 Avg. Training loss: 2.9285 0.0153 sec/batch\n",
      "Global Step: 20700 Epoch 10/50 Iteration: 20700 Avg. Training loss: 2.9462 0.0145 sec/batch\n",
      "Global Step: 20800 Epoch 10/50 Iteration: 20800 Avg. Training loss: 2.9038 0.0174 sec/batch\n",
      "Global Step: 20900 Epoch 10/50 Iteration: 20900 Avg. Training loss: 2.9204 0.0149 sec/batch\n",
      "Global Step: 21000 Epoch 10/50 Iteration: 21000 Avg. Training loss: 2.9204 0.0135 sec/batch\n",
      "Global Step: 21100 Epoch 10/50 Iteration: 21100 Avg. Training loss: 2.9312 0.0159 sec/batch\n",
      "Global Step: 21200 Epoch 10/50 Iteration: 21200 Avg. Training loss: 2.9246 0.0192 sec/batch\n",
      "Global Step: 21300 Epoch 10/50 Iteration: 21300 Avg. Training loss: 2.9250 0.0177 sec/batch\n",
      "Global Step: 21400 Epoch 10/50 Iteration: 21400 Avg. Training loss: 2.9341 0.0181 sec/batch\n",
      "Global Step: 21500 Epoch 10/50 Iteration: 21500 Avg. Training loss: 2.9301 0.0160 sec/batch\n",
      "Global Step: 21600 Epoch 10/50 Iteration: 21600 Avg. Training loss: 2.9437 0.0176 sec/batch\n",
      "Global Step: 21700 Epoch 10/50 Iteration: 21700 Avg. Training loss: 2.9155 0.0155 sec/batch\n",
      "Global Step: 21800 Epoch 10/50 Iteration: 21800 Avg. Training loss: 2.9004 0.0190 sec/batch\n",
      "Global Step: 21900 Epoch 10/50 Iteration: 21900 Avg. Training loss: 2.9226 0.0185 sec/batch\n",
      "Epoch 11/50 Threshold: 0.07300227559120946 Length of Training words: 2338676\n",
      "Global Step: 22000 Epoch 11/50 Iteration: 22000 Avg. Training loss: 2.9219 0.0005 sec/batch\n",
      "Global Step: 22100 Epoch 11/50 Iteration: 22100 Avg. Training loss: 2.8877 0.0177 sec/batch\n",
      "Global Step: 22200 Epoch 11/50 Iteration: 22200 Avg. Training loss: 2.8926 0.0174 sec/batch\n",
      "Global Step: 22300 Epoch 11/50 Iteration: 22300 Avg. Training loss: 2.8745 0.0166 sec/batch\n",
      "Global Step: 22400 Epoch 11/50 Iteration: 22400 Avg. Training loss: 2.8548 0.0156 sec/batch\n",
      "Global Step: 22500 Epoch 11/50 Iteration: 22500 Avg. Training loss: 2.8579 0.0168 sec/batch\n",
      "Global Step: 22600 Epoch 11/50 Iteration: 22600 Avg. Training loss: 2.8605 0.0170 sec/batch\n",
      "Global Step: 22700 Epoch 11/50 Iteration: 22700 Avg. Training loss: 2.8686 0.0162 sec/batch\n",
      "Global Step: 22800 Epoch 11/50 Iteration: 22800 Avg. Training loss: 2.8677 0.0155 sec/batch\n",
      "Global Step: 22900 Epoch 11/50 Iteration: 22900 Avg. Training loss: 2.8856 0.0149 sec/batch\n",
      "Global Step: 23000 Epoch 11/50 Iteration: 23000 Avg. Training loss: 2.8526 0.0198 sec/batch\n",
      "Global Step: 23100 Epoch 11/50 Iteration: 23100 Avg. Training loss: 2.8442 0.0210 sec/batch\n",
      "Global Step: 23200 Epoch 11/50 Iteration: 23200 Avg. Training loss: 2.8828 0.0144 sec/batch\n",
      "Global Step: 23300 Epoch 11/50 Iteration: 23300 Avg. Training loss: 2.8527 0.0164 sec/batch\n",
      "Global Step: 23400 Epoch 11/50 Iteration: 23400 Avg. Training loss: 2.8711 0.0180 sec/batch\n",
      "Global Step: 23500 Epoch 11/50 Iteration: 23500 Avg. Training loss: 2.8659 0.0132 sec/batch\n",
      "Global Step: 23600 Epoch 11/50 Iteration: 23600 Avg. Training loss: 2.8475 0.0162 sec/batch\n",
      "Global Step: 23700 Epoch 11/50 Iteration: 23700 Avg. Training loss: 2.8849 0.0199 sec/batch\n",
      "Global Step: 23800 Epoch 11/50 Iteration: 23800 Avg. Training loss: 2.8663 0.0198 sec/batch\n",
      "Global Step: 23900 Epoch 11/50 Iteration: 23900 Avg. Training loss: 2.8846 0.0171 sec/batch\n",
      "Global Step: 24000 Epoch 11/50 Iteration: 24000 Avg. Training loss: 2.8554 0.0159 sec/batch\n",
      "Global Step: 24100 Epoch 11/50 Iteration: 24100 Avg. Training loss: 2.8412 0.0154 sec/batch\n",
      "Global Step: 24200 Epoch 11/50 Iteration: 24200 Avg. Training loss: 2.8663 0.0168 sec/batch\n",
      "Global Step: 24300 Epoch 11/50 Iteration: 24300 Avg. Training loss: 2.8595 0.0153 sec/batch\n",
      "Epoch 12/50 Threshold: 0.040450185270247505 Length of Training words: 1999922\n",
      "Global Step: 24400 Epoch 12/50 Iteration: 24400 Avg. Training loss: 2.9353 0.0106 sec/batch\n",
      "Global Step: 24500 Epoch 12/50 Iteration: 24500 Avg. Training loss: 2.9957 0.0177 sec/batch\n",
      "Global Step: 24600 Epoch 12/50 Iteration: 24600 Avg. Training loss: 2.9693 0.0164 sec/batch\n",
      "Global Step: 24700 Epoch 12/50 Iteration: 24700 Avg. Training loss: 2.9633 0.0160 sec/batch\n",
      "Global Step: 24800 Epoch 12/50 Iteration: 24800 Avg. Training loss: 2.9574 0.0188 sec/batch\n",
      "Global Step: 24900 Epoch 12/50 Iteration: 24900 Avg. Training loss: 2.9708 0.0175 sec/batch\n",
      "Global Step: 25000 Epoch 12/50 Iteration: 25000 Avg. Training loss: 2.9836 0.0184 sec/batch\n",
      "Global Step: 25100 Epoch 12/50 Iteration: 25100 Avg. Training loss: 2.9692 0.0176 sec/batch\n",
      "Global Step: 25200 Epoch 12/50 Iteration: 25200 Avg. Training loss: 2.9554 0.0179 sec/batch\n",
      "Global Step: 25300 Epoch 12/50 Iteration: 25300 Avg. Training loss: 2.9563 0.0152 sec/batch\n",
      "Global Step: 25400 Epoch 12/50 Iteration: 25400 Avg. Training loss: 2.9661 0.0192 sec/batch\n",
      "Global Step: 25500 Epoch 12/50 Iteration: 25500 Avg. Training loss: 2.9683 0.0176 sec/batch\n",
      "Global Step: 25600 Epoch 12/50 Iteration: 25600 Avg. Training loss: 2.9656 0.0184 sec/batch\n",
      "Global Step: 25700 Epoch 12/50 Iteration: 25700 Avg. Training loss: 2.9553 0.0206 sec/batch\n",
      "Global Step: 25800 Epoch 12/50 Iteration: 25800 Avg. Training loss: 2.9889 0.0187 sec/batch\n",
      "Global Step: 25900 Epoch 12/50 Iteration: 25900 Avg. Training loss: 2.9703 0.0170 sec/batch\n",
      "Global Step: 26000 Epoch 12/50 Iteration: 26000 Avg. Training loss: 2.9773 0.0188 sec/batch\n",
      "Global Step: 26100 Epoch 12/50 Iteration: 26100 Avg. Training loss: 2.9472 0.0166 sec/batch\n",
      "Global Step: 26200 Epoch 12/50 Iteration: 26200 Avg. Training loss: 2.9549 0.0191 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 26300 Epoch 12/50 Iteration: 26300 Avg. Training loss: 2.9687 0.0172 sec/batch\n",
      "Epoch 13/50 Threshold: 0.07522506631461154 Length of Training words: 2354123\n",
      "Global Step: 26400 Epoch 13/50 Iteration: 26400 Avg. Training loss: 2.9006 0.0119 sec/batch\n",
      "Global Step: 26500 Epoch 13/50 Iteration: 26500 Avg. Training loss: 2.8804 0.0214 sec/batch\n",
      "Global Step: 26600 Epoch 13/50 Iteration: 26600 Avg. Training loss: 2.8793 0.0200 sec/batch\n",
      "Global Step: 26700 Epoch 13/50 Iteration: 26700 Avg. Training loss: 2.8527 0.0175 sec/batch\n",
      "Global Step: 26800 Epoch 13/50 Iteration: 26800 Avg. Training loss: 2.8570 0.0203 sec/batch\n",
      "Global Step: 26900 Epoch 13/50 Iteration: 26900 Avg. Training loss: 2.8510 0.0191 sec/batch\n",
      "Global Step: 27000 Epoch 13/50 Iteration: 27000 Avg. Training loss: 2.8605 0.0217 sec/batch\n",
      "Global Step: 27100 Epoch 13/50 Iteration: 27100 Avg. Training loss: 2.8735 0.0159 sec/batch\n",
      "Global Step: 27200 Epoch 13/50 Iteration: 27200 Avg. Training loss: 2.8624 0.0161 sec/batch\n",
      "Global Step: 27300 Epoch 13/50 Iteration: 27300 Avg. Training loss: 2.8639 0.0136 sec/batch\n",
      "Global Step: 27400 Epoch 13/50 Iteration: 27400 Avg. Training loss: 2.8395 0.0151 sec/batch\n",
      "Global Step: 27500 Epoch 13/50 Iteration: 27500 Avg. Training loss: 2.8610 0.0194 sec/batch\n",
      "Global Step: 27600 Epoch 13/50 Iteration: 27600 Avg. Training loss: 2.8545 0.0186 sec/batch\n",
      "Global Step: 27700 Epoch 13/50 Iteration: 27700 Avg. Training loss: 2.8647 0.0189 sec/batch\n",
      "Global Step: 27800 Epoch 13/50 Iteration: 27800 Avg. Training loss: 2.8594 0.0171 sec/batch\n",
      "Global Step: 27900 Epoch 13/50 Iteration: 27900 Avg. Training loss: 2.8640 0.0177 sec/batch\n",
      "Global Step: 28000 Epoch 13/50 Iteration: 28000 Avg. Training loss: 2.8561 0.0170 sec/batch\n",
      "Global Step: 28100 Epoch 13/50 Iteration: 28100 Avg. Training loss: 2.8828 0.0205 sec/batch\n",
      "Global Step: 28200 Epoch 13/50 Iteration: 28200 Avg. Training loss: 2.8631 0.0175 sec/batch\n",
      "Global Step: 28300 Epoch 13/50 Iteration: 28300 Avg. Training loss: 2.8682 0.0195 sec/batch\n",
      "Global Step: 28400 Epoch 13/50 Iteration: 28400 Avg. Training loss: 2.8350 0.0176 sec/batch\n",
      "Global Step: 28500 Epoch 13/50 Iteration: 28500 Avg. Training loss: 2.8485 0.0180 sec/batch\n",
      "Global Step: 28600 Epoch 13/50 Iteration: 28600 Avg. Training loss: 2.8515 0.0180 sec/batch\n",
      "Epoch 14/50 Threshold: 0.06991912759086406 Length of Training words: 2313274\n",
      "Global Step: 28700 Epoch 14/50 Iteration: 28700 Avg. Training loss: 2.8546 0.0023 sec/batch\n",
      "Global Step: 28800 Epoch 14/50 Iteration: 28800 Avg. Training loss: 2.8900 0.0196 sec/batch\n",
      "Global Step: 28900 Epoch 14/50 Iteration: 28900 Avg. Training loss: 2.9030 0.0169 sec/batch\n",
      "Global Step: 29000 Epoch 14/50 Iteration: 29000 Avg. Training loss: 2.8803 0.0192 sec/batch\n",
      "Global Step: 29100 Epoch 14/50 Iteration: 29100 Avg. Training loss: 2.8657 0.0178 sec/batch\n",
      "Global Step: 29200 Epoch 14/50 Iteration: 29200 Avg. Training loss: 2.8612 0.0169 sec/batch\n",
      "Global Step: 29300 Epoch 14/50 Iteration: 29300 Avg. Training loss: 2.8712 0.0191 sec/batch\n",
      "Global Step: 29400 Epoch 14/50 Iteration: 29400 Avg. Training loss: 2.8804 0.0179 sec/batch\n",
      "Global Step: 29500 Epoch 14/50 Iteration: 29500 Avg. Training loss: 2.8721 0.0151 sec/batch\n",
      "Global Step: 29600 Epoch 14/50 Iteration: 29600 Avg. Training loss: 2.8961 0.0199 sec/batch\n",
      "Global Step: 29700 Epoch 14/50 Iteration: 29700 Avg. Training loss: 2.8540 0.0169 sec/batch\n",
      "Global Step: 29800 Epoch 14/50 Iteration: 29800 Avg. Training loss: 2.8631 0.0205 sec/batch\n",
      "Global Step: 29900 Epoch 14/50 Iteration: 29900 Avg. Training loss: 2.8804 0.0199 sec/batch\n",
      "Global Step: 30000 Epoch 14/50 Iteration: 30000 Avg. Training loss: 2.8676 0.0177 sec/batch\n",
      "Global Step: 30100 Epoch 14/50 Iteration: 30100 Avg. Training loss: 2.8773 0.0206 sec/batch\n",
      "Global Step: 30200 Epoch 14/50 Iteration: 30200 Avg. Training loss: 2.8681 0.0183 sec/batch\n",
      "Global Step: 30300 Epoch 14/50 Iteration: 30300 Avg. Training loss: 2.8635 0.0156 sec/batch\n",
      "Global Step: 30400 Epoch 14/50 Iteration: 30400 Avg. Training loss: 2.8916 0.0163 sec/batch\n",
      "Global Step: 30500 Epoch 14/50 Iteration: 30500 Avg. Training loss: 2.8737 0.0175 sec/batch\n",
      "Global Step: 30600 Epoch 14/50 Iteration: 30600 Avg. Training loss: 2.8875 0.0174 sec/batch\n",
      "Global Step: 30700 Epoch 14/50 Iteration: 30700 Avg. Training loss: 2.8524 0.0166 sec/batch\n",
      "Global Step: 30800 Epoch 14/50 Iteration: 30800 Avg. Training loss: 2.8536 0.0146 sec/batch\n",
      "Global Step: 30900 Epoch 14/50 Iteration: 30900 Avg. Training loss: 2.8658 0.0155 sec/batch\n",
      "Global Step: 31000 Epoch 14/50 Iteration: 31000 Avg. Training loss: 2.8715 0.0162 sec/batch\n",
      "Epoch 15/50 Threshold: 0.06747449043131928 Length of Training words: 2292809\n",
      "Global Step: 31100 Epoch 15/50 Iteration: 31100 Avg. Training loss: 2.8990 0.0165 sec/batch\n",
      "Global Step: 31200 Epoch 15/50 Iteration: 31200 Avg. Training loss: 2.9064 0.0187 sec/batch\n",
      "Global Step: 31300 Epoch 15/50 Iteration: 31300 Avg. Training loss: 2.8855 0.0139 sec/batch\n",
      "Global Step: 31400 Epoch 15/50 Iteration: 31400 Avg. Training loss: 2.8631 0.0212 sec/batch\n",
      "Global Step: 31500 Epoch 15/50 Iteration: 31500 Avg. Training loss: 2.8773 0.0173 sec/batch\n",
      "Global Step: 31600 Epoch 15/50 Iteration: 31600 Avg. Training loss: 2.8738 0.0186 sec/batch\n",
      "Global Step: 31700 Epoch 15/50 Iteration: 31700 Avg. Training loss: 2.8820 0.0200 sec/batch\n",
      "Global Step: 31800 Epoch 15/50 Iteration: 31800 Avg. Training loss: 2.8826 0.0189 sec/batch\n",
      "Global Step: 31900 Epoch 15/50 Iteration: 31900 Avg. Training loss: 2.9039 0.0164 sec/batch\n",
      "Global Step: 32000 Epoch 15/50 Iteration: 32000 Avg. Training loss: 2.8611 0.0143 sec/batch\n",
      "Global Step: 32100 Epoch 15/50 Iteration: 32100 Avg. Training loss: 2.8696 0.0221 sec/batch\n",
      "Global Step: 32200 Epoch 15/50 Iteration: 32200 Avg. Training loss: 2.8884 0.0212 sec/batch\n",
      "Global Step: 32300 Epoch 15/50 Iteration: 32300 Avg. Training loss: 2.8697 0.0175 sec/batch\n",
      "Global Step: 32400 Epoch 15/50 Iteration: 32400 Avg. Training loss: 2.8868 0.0200 sec/batch\n",
      "Global Step: 32500 Epoch 15/50 Iteration: 32500 Avg. Training loss: 2.8748 0.0184 sec/batch\n",
      "Global Step: 32600 Epoch 15/50 Iteration: 32600 Avg. Training loss: 2.8688 0.0190 sec/batch\n",
      "Global Step: 32700 Epoch 15/50 Iteration: 32700 Avg. Training loss: 2.9001 0.0180 sec/batch\n",
      "Global Step: 32800 Epoch 15/50 Iteration: 32800 Avg. Training loss: 2.8796 0.0187 sec/batch\n",
      "Global Step: 32900 Epoch 15/50 Iteration: 32900 Avg. Training loss: 2.8941 0.0166 sec/batch\n",
      "Global Step: 33000 Epoch 15/50 Iteration: 33000 Avg. Training loss: 2.8585 0.0182 sec/batch\n",
      "Global Step: 33100 Epoch 15/50 Iteration: 33100 Avg. Training loss: 2.8611 0.0170 sec/batch\n",
      "Global Step: 33200 Epoch 15/50 Iteration: 33200 Avg. Training loss: 2.8705 0.0201 sec/batch\n",
      "Epoch 16/50 Threshold: 0.06416927774719366 Length of Training words: 2265186\n",
      "Global Step: 33300 Epoch 16/50 Iteration: 33300 Avg. Training loss: 2.8777 0.0014 sec/batch\n",
      "Global Step: 33400 Epoch 16/50 Iteration: 33400 Avg. Training loss: 2.9082 0.0152 sec/batch\n",
      "Global Step: 33500 Epoch 16/50 Iteration: 33500 Avg. Training loss: 2.9170 0.0208 sec/batch\n",
      "Global Step: 33600 Epoch 16/50 Iteration: 33600 Avg. Training loss: 2.8958 0.0151 sec/batch\n",
      "Global Step: 33700 Epoch 16/50 Iteration: 33700 Avg. Training loss: 2.8815 0.0160 sec/batch\n",
      "Global Step: 33800 Epoch 16/50 Iteration: 33800 Avg. Training loss: 2.8764 0.0168 sec/batch\n",
      "Global Step: 33900 Epoch 16/50 Iteration: 33900 Avg. Training loss: 2.8929 0.0192 sec/batch\n",
      "Global Step: 34000 Epoch 16/50 Iteration: 34000 Avg. Training loss: 2.8908 0.0175 sec/batch\n",
      "Global Step: 34100 Epoch 16/50 Iteration: 34100 Avg. Training loss: 2.8869 0.0163 sec/batch\n",
      "Global Step: 34200 Epoch 16/50 Iteration: 34200 Avg. Training loss: 2.9029 0.0182 sec/batch\n",
      "Global Step: 34300 Epoch 16/50 Iteration: 34300 Avg. Training loss: 2.8732 0.0139 sec/batch\n",
      "Global Step: 34400 Epoch 16/50 Iteration: 34400 Avg. Training loss: 2.8760 0.0177 sec/batch\n",
      "Global Step: 34500 Epoch 16/50 Iteration: 34500 Avg. Training loss: 2.8879 0.0158 sec/batch\n",
      "Global Step: 34600 Epoch 16/50 Iteration: 34600 Avg. Training loss: 2.8905 0.0175 sec/batch\n",
      "Global Step: 34700 Epoch 16/50 Iteration: 34700 Avg. Training loss: 2.8845 0.0195 sec/batch\n",
      "Global Step: 34800 Epoch 16/50 Iteration: 34800 Avg. Training loss: 2.8935 0.0194 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 34900 Epoch 16/50 Iteration: 34900 Avg. Training loss: 2.8863 0.0161 sec/batch\n",
      "Global Step: 35000 Epoch 16/50 Iteration: 35000 Avg. Training loss: 2.9045 0.0189 sec/batch\n",
      "Global Step: 35100 Epoch 16/50 Iteration: 35100 Avg. Training loss: 2.8882 0.0191 sec/batch\n",
      "Global Step: 35200 Epoch 16/50 Iteration: 35200 Avg. Training loss: 2.8954 0.0164 sec/batch\n",
      "Global Step: 35300 Epoch 16/50 Iteration: 35300 Avg. Training loss: 2.8666 0.0192 sec/batch\n",
      "Global Step: 35400 Epoch 16/50 Iteration: 35400 Avg. Training loss: 2.8748 0.0203 sec/batch\n",
      "Global Step: 35500 Epoch 16/50 Iteration: 35500 Avg. Training loss: 2.8795 0.0164 sec/batch\n",
      "Epoch 17/50 Threshold: 0.049982691132391004 Length of Training words: 2123372\n",
      "Global Step: 35600 Epoch 17/50 Iteration: 35600 Avg. Training loss: 2.9127 0.0080 sec/batch\n",
      "Global Step: 35700 Epoch 17/50 Iteration: 35700 Avg. Training loss: 2.9513 0.0151 sec/batch\n",
      "Global Step: 35800 Epoch 17/50 Iteration: 35800 Avg. Training loss: 2.9482 0.0216 sec/batch\n",
      "Global Step: 35900 Epoch 17/50 Iteration: 35900 Avg. Training loss: 2.9256 0.0186 sec/batch\n",
      "Global Step: 36000 Epoch 17/50 Iteration: 36000 Avg. Training loss: 2.9220 0.0146 sec/batch\n",
      "Global Step: 36100 Epoch 17/50 Iteration: 36100 Avg. Training loss: 2.9251 0.0194 sec/batch\n",
      "Global Step: 36200 Epoch 17/50 Iteration: 36200 Avg. Training loss: 2.9357 0.0143 sec/batch\n",
      "Global Step: 36300 Epoch 17/50 Iteration: 36300 Avg. Training loss: 2.9313 0.0128 sec/batch\n",
      "Global Step: 36400 Epoch 17/50 Iteration: 36400 Avg. Training loss: 2.9482 0.0147 sec/batch\n",
      "Global Step: 36500 Epoch 17/50 Iteration: 36500 Avg. Training loss: 2.9137 0.0178 sec/batch\n",
      "Global Step: 36600 Epoch 17/50 Iteration: 36600 Avg. Training loss: 2.9185 0.0173 sec/batch\n",
      "Global Step: 36700 Epoch 17/50 Iteration: 36700 Avg. Training loss: 2.9310 0.0149 sec/batch\n",
      "Global Step: 36800 Epoch 17/50 Iteration: 36800 Avg. Training loss: 2.9359 0.0149 sec/batch\n",
      "Global Step: 36900 Epoch 17/50 Iteration: 36900 Avg. Training loss: 2.9336 0.0143 sec/batch\n",
      "Global Step: 37000 Epoch 17/50 Iteration: 37000 Avg. Training loss: 2.9174 0.0130 sec/batch\n",
      "Global Step: 37100 Epoch 17/50 Iteration: 37100 Avg. Training loss: 2.9519 0.0126 sec/batch\n",
      "Global Step: 37200 Epoch 17/50 Iteration: 37200 Avg. Training loss: 2.9330 0.0152 sec/batch\n",
      "Global Step: 37300 Epoch 17/50 Iteration: 37300 Avg. Training loss: 2.9490 0.0135 sec/batch\n",
      "Global Step: 37400 Epoch 17/50 Iteration: 37400 Avg. Training loss: 2.9079 0.0124 sec/batch\n",
      "Global Step: 37500 Epoch 17/50 Iteration: 37500 Avg. Training loss: 2.9145 0.0162 sec/batch\n",
      "Global Step: 37600 Epoch 17/50 Iteration: 37600 Avg. Training loss: 2.9247 0.0185 sec/batch\n",
      "Epoch 18/50 Threshold: 0.07074387497540083 Length of Training words: 2320191\n",
      "Global Step: 37700 Epoch 18/50 Iteration: 37700 Avg. Training loss: 2.9228 0.0033 sec/batch\n",
      "Global Step: 37800 Epoch 18/50 Iteration: 37800 Avg. Training loss: 2.8780 0.0172 sec/batch\n",
      "Global Step: 37900 Epoch 18/50 Iteration: 37900 Avg. Training loss: 2.9007 0.0179 sec/batch\n",
      "Global Step: 38000 Epoch 18/50 Iteration: 38000 Avg. Training loss: 2.8793 0.0140 sec/batch\n",
      "Global Step: 38100 Epoch 18/50 Iteration: 38100 Avg. Training loss: 2.8600 0.0149 sec/batch\n",
      "Global Step: 38200 Epoch 18/50 Iteration: 38200 Avg. Training loss: 2.8579 0.0149 sec/batch\n",
      "Global Step: 38300 Epoch 18/50 Iteration: 38300 Avg. Training loss: 2.8732 0.0177 sec/batch\n",
      "Global Step: 38400 Epoch 18/50 Iteration: 38400 Avg. Training loss: 2.8752 0.0147 sec/batch\n",
      "Global Step: 38500 Epoch 18/50 Iteration: 38500 Avg. Training loss: 2.8669 0.0162 sec/batch\n",
      "Global Step: 38600 Epoch 18/50 Iteration: 38600 Avg. Training loss: 2.8916 0.0155 sec/batch\n",
      "Global Step: 38700 Epoch 18/50 Iteration: 38700 Avg. Training loss: 2.8543 0.0153 sec/batch\n",
      "Global Step: 38800 Epoch 18/50 Iteration: 38800 Avg. Training loss: 2.8587 0.0132 sec/batch\n",
      "Global Step: 38900 Epoch 18/50 Iteration: 38900 Avg. Training loss: 2.8794 0.0138 sec/batch\n",
      "Global Step: 39000 Epoch 18/50 Iteration: 39000 Avg. Training loss: 2.8663 0.0140 sec/batch\n",
      "Global Step: 39100 Epoch 18/50 Iteration: 39100 Avg. Training loss: 2.8741 0.0144 sec/batch\n",
      "Global Step: 39200 Epoch 18/50 Iteration: 39200 Avg. Training loss: 2.8679 0.0155 sec/batch\n",
      "Global Step: 39300 Epoch 18/50 Iteration: 39300 Avg. Training loss: 2.8592 0.0170 sec/batch\n",
      "Global Step: 39400 Epoch 18/50 Iteration: 39400 Avg. Training loss: 2.8901 0.0152 sec/batch\n",
      "Global Step: 39500 Epoch 18/50 Iteration: 39500 Avg. Training loss: 2.8698 0.0153 sec/batch\n",
      "Global Step: 39600 Epoch 18/50 Iteration: 39600 Avg. Training loss: 2.8873 0.0165 sec/batch\n",
      "Global Step: 39700 Epoch 18/50 Iteration: 39700 Avg. Training loss: 2.8493 0.0163 sec/batch\n",
      "Global Step: 39800 Epoch 18/50 Iteration: 39800 Avg. Training loss: 2.8476 0.0176 sec/batch\n",
      "Global Step: 39900 Epoch 18/50 Iteration: 39900 Avg. Training loss: 2.8621 0.0193 sec/batch\n",
      "Global Step: 40000 Epoch 18/50 Iteration: 40000 Avg. Training loss: 2.8676 0.0191 sec/batch\n",
      "Epoch 19/50 Threshold: 0.04572142571806319 Length of Training words: 2070116\n",
      "Global Step: 40100 Epoch 19/50 Iteration: 40100 Avg. Training loss: 2.9658 0.0202 sec/batch\n",
      "Global Step: 40200 Epoch 19/50 Iteration: 40200 Avg. Training loss: 2.9667 0.0168 sec/batch\n",
      "Global Step: 40300 Epoch 19/50 Iteration: 40300 Avg. Training loss: 2.9495 0.0158 sec/batch\n",
      "Global Step: 40400 Epoch 19/50 Iteration: 40400 Avg. Training loss: 2.9365 0.0164 sec/batch\n",
      "Global Step: 40500 Epoch 19/50 Iteration: 40500 Avg. Training loss: 2.9421 0.0237 sec/batch\n",
      "Global Step: 40600 Epoch 19/50 Iteration: 40600 Avg. Training loss: 2.9418 0.0151 sec/batch\n",
      "Global Step: 40700 Epoch 19/50 Iteration: 40700 Avg. Training loss: 2.9549 0.0138 sec/batch\n",
      "Global Step: 40800 Epoch 19/50 Iteration: 40800 Avg. Training loss: 2.9558 0.0124 sec/batch\n",
      "Global Step: 40900 Epoch 19/50 Iteration: 40900 Avg. Training loss: 2.9375 0.0140 sec/batch\n",
      "Global Step: 41000 Epoch 19/50 Iteration: 41000 Avg. Training loss: 2.9340 0.0162 sec/batch\n",
      "Global Step: 41100 Epoch 19/50 Iteration: 41100 Avg. Training loss: 2.9465 0.0165 sec/batch\n",
      "Global Step: 41200 Epoch 19/50 Iteration: 41200 Avg. Training loss: 2.9478 0.0184 sec/batch\n",
      "Global Step: 41300 Epoch 19/50 Iteration: 41300 Avg. Training loss: 2.9436 0.0157 sec/batch\n",
      "Global Step: 41400 Epoch 19/50 Iteration: 41400 Avg. Training loss: 2.9423 0.0150 sec/batch\n",
      "Global Step: 41500 Epoch 19/50 Iteration: 41500 Avg. Training loss: 2.9582 0.0195 sec/batch\n",
      "Global Step: 41600 Epoch 19/50 Iteration: 41600 Avg. Training loss: 2.9499 0.0197 sec/batch\n",
      "Global Step: 41700 Epoch 19/50 Iteration: 41700 Avg. Training loss: 2.9662 0.0183 sec/batch\n",
      "Global Step: 41800 Epoch 19/50 Iteration: 41800 Avg. Training loss: 2.9235 0.0209 sec/batch\n",
      "Global Step: 41900 Epoch 19/50 Iteration: 41900 Avg. Training loss: 2.9316 0.0155 sec/batch\n",
      "Global Step: 42000 Epoch 19/50 Iteration: 42000 Avg. Training loss: 2.9418 0.0185 sec/batch\n",
      "Epoch 20/50 Threshold: 0.061878602571714236 Length of Training words: 2245507\n",
      "Global Step: 42100 Epoch 20/50 Iteration: 42100 Avg. Training loss: 2.9269 0.0057 sec/batch\n",
      "Global Step: 42200 Epoch 20/50 Iteration: 42200 Avg. Training loss: 2.9118 0.0168 sec/batch\n",
      "Global Step: 42300 Epoch 20/50 Iteration: 42300 Avg. Training loss: 2.9231 0.0152 sec/batch\n",
      "Global Step: 42400 Epoch 20/50 Iteration: 42400 Avg. Training loss: 2.8956 0.0140 sec/batch\n",
      "Global Step: 42500 Epoch 20/50 Iteration: 42500 Avg. Training loss: 2.8889 0.0136 sec/batch\n",
      "Global Step: 42600 Epoch 20/50 Iteration: 42600 Avg. Training loss: 2.8845 0.0134 sec/batch\n",
      "Global Step: 42700 Epoch 20/50 Iteration: 42700 Avg. Training loss: 2.8934 0.0125 sec/batch\n",
      "Global Step: 42800 Epoch 20/50 Iteration: 42800 Avg. Training loss: 2.9049 0.0138 sec/batch\n",
      "Global Step: 42900 Epoch 20/50 Iteration: 42900 Avg. Training loss: 2.8969 0.0125 sec/batch\n",
      "Global Step: 43000 Epoch 20/50 Iteration: 43000 Avg. Training loss: 2.8924 0.0152 sec/batch\n",
      "Global Step: 43100 Epoch 20/50 Iteration: 43100 Avg. Training loss: 2.8827 0.0199 sec/batch\n",
      "Global Step: 43200 Epoch 20/50 Iteration: 43200 Avg. Training loss: 2.8942 0.0171 sec/batch\n",
      "Global Step: 43300 Epoch 20/50 Iteration: 43300 Avg. Training loss: 2.8896 0.0173 sec/batch\n",
      "Global Step: 43400 Epoch 20/50 Iteration: 43400 Avg. Training loss: 2.8905 0.0174 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 43500 Epoch 20/50 Iteration: 43500 Avg. Training loss: 2.9008 0.0171 sec/batch\n",
      "Global Step: 43600 Epoch 20/50 Iteration: 43600 Avg. Training loss: 2.8727 0.0160 sec/batch\n",
      "Global Step: 43700 Epoch 20/50 Iteration: 43700 Avg. Training loss: 2.9148 0.0164 sec/batch\n",
      "Global Step: 43800 Epoch 20/50 Iteration: 43800 Avg. Training loss: 2.8931 0.0144 sec/batch\n",
      "Global Step: 43900 Epoch 20/50 Iteration: 43900 Avg. Training loss: 2.9114 0.0132 sec/batch\n",
      "Global Step: 44000 Epoch 20/50 Iteration: 44000 Avg. Training loss: 2.8853 0.0163 sec/batch\n",
      "Global Step: 44100 Epoch 20/50 Iteration: 44100 Avg. Training loss: 2.8669 0.0125 sec/batch\n",
      "Global Step: 44200 Epoch 20/50 Iteration: 44200 Avg. Training loss: 2.8966 0.0138 sec/batch\n",
      "Global Step: 44300 Epoch 20/50 Iteration: 44300 Avg. Training loss: 2.8883 0.0160 sec/batch\n",
      "Epoch 21/50 Threshold: 0.06576353879619048 Length of Training words: 2278037\n",
      "Global Step: 44400 Epoch 21/50 Iteration: 44400 Avg. Training loss: 2.8910 0.0150 sec/batch\n",
      "Global Step: 44500 Epoch 21/50 Iteration: 44500 Avg. Training loss: 2.9190 0.0141 sec/batch\n",
      "Global Step: 44600 Epoch 21/50 Iteration: 44600 Avg. Training loss: 2.8955 0.0148 sec/batch\n",
      "Global Step: 44700 Epoch 21/50 Iteration: 44700 Avg. Training loss: 2.8686 0.0145 sec/batch\n",
      "Global Step: 44800 Epoch 21/50 Iteration: 44800 Avg. Training loss: 2.8795 0.0146 sec/batch\n",
      "Global Step: 44900 Epoch 21/50 Iteration: 44900 Avg. Training loss: 2.8790 0.0150 sec/batch\n",
      "Global Step: 45000 Epoch 21/50 Iteration: 45000 Avg. Training loss: 2.8853 0.0161 sec/batch\n",
      "Global Step: 45100 Epoch 21/50 Iteration: 45100 Avg. Training loss: 2.8866 0.0176 sec/batch\n",
      "Global Step: 45200 Epoch 21/50 Iteration: 45200 Avg. Training loss: 2.9056 0.0170 sec/batch\n",
      "Global Step: 45300 Epoch 21/50 Iteration: 45300 Avg. Training loss: 2.8716 0.0149 sec/batch\n",
      "Global Step: 45400 Epoch 21/50 Iteration: 45400 Avg. Training loss: 2.8680 0.0150 sec/batch\n",
      "Global Step: 45500 Epoch 21/50 Iteration: 45500 Avg. Training loss: 2.8995 0.0167 sec/batch\n",
      "Global Step: 45600 Epoch 21/50 Iteration: 45600 Avg. Training loss: 2.8696 0.0194 sec/batch\n",
      "Global Step: 45700 Epoch 21/50 Iteration: 45700 Avg. Training loss: 2.8906 0.0165 sec/batch\n",
      "Global Step: 45800 Epoch 21/50 Iteration: 45800 Avg. Training loss: 2.8786 0.0176 sec/batch\n",
      "Global Step: 45900 Epoch 21/50 Iteration: 45900 Avg. Training loss: 2.8748 0.0153 sec/batch\n",
      "Global Step: 46000 Epoch 21/50 Iteration: 46000 Avg. Training loss: 2.9059 0.0186 sec/batch\n",
      "Global Step: 46100 Epoch 21/50 Iteration: 46100 Avg. Training loss: 2.8829 0.0188 sec/batch\n",
      "Global Step: 46200 Epoch 21/50 Iteration: 46200 Avg. Training loss: 2.8984 0.0162 sec/batch\n",
      "Global Step: 46300 Epoch 21/50 Iteration: 46300 Avg. Training loss: 2.8643 0.0167 sec/batch\n",
      "Global Step: 46400 Epoch 21/50 Iteration: 46400 Avg. Training loss: 2.8637 0.0166 sec/batch\n",
      "Global Step: 46500 Epoch 21/50 Iteration: 46500 Avg. Training loss: 2.8747 0.0165 sec/batch\n",
      "Epoch 22/50 Threshold: 0.040604641527411375 Length of Training words: 2001234\n",
      "Global Step: 46600 Epoch 22/50 Iteration: 46600 Avg. Training loss: 2.8882 0.0011 sec/batch\n",
      "Global Step: 46700 Epoch 22/50 Iteration: 46700 Avg. Training loss: 2.9829 0.0144 sec/batch\n",
      "Global Step: 46800 Epoch 22/50 Iteration: 46800 Avg. Training loss: 2.9961 0.0159 sec/batch\n",
      "Global Step: 46900 Epoch 22/50 Iteration: 46900 Avg. Training loss: 2.9626 0.0152 sec/batch\n",
      "Global Step: 47000 Epoch 22/50 Iteration: 47000 Avg. Training loss: 2.9673 0.0142 sec/batch\n",
      "Global Step: 47100 Epoch 22/50 Iteration: 47100 Avg. Training loss: 2.9580 0.0125 sec/batch\n",
      "Global Step: 47200 Epoch 22/50 Iteration: 47200 Avg. Training loss: 2.9660 0.0132 sec/batch\n",
      "Global Step: 47300 Epoch 22/50 Iteration: 47300 Avg. Training loss: 2.9666 0.0140 sec/batch\n",
      "Global Step: 47400 Epoch 22/50 Iteration: 47400 Avg. Training loss: 2.9783 0.0182 sec/batch\n",
      "Global Step: 47500 Epoch 22/50 Iteration: 47500 Avg. Training loss: 2.9485 0.0141 sec/batch\n",
      "Global Step: 47600 Epoch 22/50 Iteration: 47600 Avg. Training loss: 2.9707 0.0167 sec/batch\n",
      "Global Step: 47700 Epoch 22/50 Iteration: 47700 Avg. Training loss: 2.9634 0.0149 sec/batch\n",
      "Global Step: 47800 Epoch 22/50 Iteration: 47800 Avg. Training loss: 2.9710 0.0168 sec/batch\n",
      "Global Step: 47900 Epoch 22/50 Iteration: 47900 Avg. Training loss: 2.9651 0.0176 sec/batch\n",
      "Global Step: 48000 Epoch 22/50 Iteration: 48000 Avg. Training loss: 2.9576 0.0179 sec/batch\n",
      "Global Step: 48100 Epoch 22/50 Iteration: 48100 Avg. Training loss: 2.9866 0.0181 sec/batch\n",
      "Global Step: 48200 Epoch 22/50 Iteration: 48200 Avg. Training loss: 2.9718 0.0181 sec/batch\n",
      "Global Step: 48300 Epoch 22/50 Iteration: 48300 Avg. Training loss: 2.9602 0.0137 sec/batch\n",
      "Global Step: 48400 Epoch 22/50 Iteration: 48400 Avg. Training loss: 2.9422 0.0136 sec/batch\n",
      "Global Step: 48500 Epoch 22/50 Iteration: 48500 Avg. Training loss: 2.9631 0.0157 sec/batch\n",
      "Epoch 23/50 Threshold: 0.06819497408722314 Length of Training words: 2299146\n",
      "Global Step: 48600 Epoch 23/50 Iteration: 48600 Avg. Training loss: 2.9546 0.0009 sec/batch\n",
      "Global Step: 48700 Epoch 23/50 Iteration: 48700 Avg. Training loss: 2.8977 0.0161 sec/batch\n",
      "Global Step: 48800 Epoch 23/50 Iteration: 48800 Avg. Training loss: 2.9062 0.0154 sec/batch\n",
      "Global Step: 48900 Epoch 23/50 Iteration: 48900 Avg. Training loss: 2.8826 0.0153 sec/batch\n",
      "Global Step: 49000 Epoch 23/50 Iteration: 49000 Avg. Training loss: 2.8720 0.0148 sec/batch\n",
      "Global Step: 49100 Epoch 23/50 Iteration: 49100 Avg. Training loss: 2.8704 0.0190 sec/batch\n",
      "Global Step: 49200 Epoch 23/50 Iteration: 49200 Avg. Training loss: 2.8717 0.0175 sec/batch\n",
      "Global Step: 49300 Epoch 23/50 Iteration: 49300 Avg. Training loss: 2.8818 0.0164 sec/batch\n",
      "Global Step: 49400 Epoch 23/50 Iteration: 49400 Avg. Training loss: 2.8802 0.0204 sec/batch\n",
      "Global Step: 49500 Epoch 23/50 Iteration: 49500 Avg. Training loss: 2.8994 0.0141 sec/batch\n",
      "Global Step: 49600 Epoch 23/50 Iteration: 49600 Avg. Training loss: 2.8592 0.0166 sec/batch\n",
      "Global Step: 49700 Epoch 23/50 Iteration: 49700 Avg. Training loss: 2.8692 0.0180 sec/batch\n",
      "Global Step: 49800 Epoch 23/50 Iteration: 49800 Avg. Training loss: 2.8837 0.0158 sec/batch\n",
      "Global Step: 49900 Epoch 23/50 Iteration: 49900 Avg. Training loss: 2.8704 0.0147 sec/batch\n",
      "Global Step: 50000 Epoch 23/50 Iteration: 50000 Avg. Training loss: 2.8801 0.0179 sec/batch\n",
      "Global Step: 50100 Epoch 23/50 Iteration: 50100 Avg. Training loss: 2.8724 0.0223 sec/batch\n",
      "Global Step: 50200 Epoch 23/50 Iteration: 50200 Avg. Training loss: 2.8693 0.0165 sec/batch\n",
      "Global Step: 50300 Epoch 23/50 Iteration: 50300 Avg. Training loss: 2.8963 0.0146 sec/batch\n",
      "Global Step: 50400 Epoch 23/50 Iteration: 50400 Avg. Training loss: 2.8778 0.0182 sec/batch\n",
      "Global Step: 50500 Epoch 23/50 Iteration: 50500 Avg. Training loss: 2.8947 0.0121 sec/batch\n",
      "Global Step: 50600 Epoch 23/50 Iteration: 50600 Avg. Training loss: 2.8564 0.0133 sec/batch\n",
      "Global Step: 50700 Epoch 23/50 Iteration: 50700 Avg. Training loss: 2.8582 0.0139 sec/batch\n",
      "Global Step: 50800 Epoch 23/50 Iteration: 50800 Avg. Training loss: 2.8670 0.0168 sec/batch\n",
      "Epoch 24/50 Threshold: 0.06591749734089662 Length of Training words: 2280725\n",
      "Global Step: 50900 Epoch 24/50 Iteration: 50900 Avg. Training loss: 2.8727 0.0012 sec/batch\n",
      "Global Step: 51000 Epoch 24/50 Iteration: 51000 Avg. Training loss: 2.9039 0.0138 sec/batch\n",
      "Global Step: 51100 Epoch 24/50 Iteration: 51100 Avg. Training loss: 2.9110 0.0154 sec/batch\n",
      "Global Step: 51200 Epoch 24/50 Iteration: 51200 Avg. Training loss: 2.8888 0.0174 sec/batch\n",
      "Global Step: 51300 Epoch 24/50 Iteration: 51300 Avg. Training loss: 2.8765 0.0137 sec/batch\n",
      "Global Step: 51400 Epoch 24/50 Iteration: 51400 Avg. Training loss: 2.8726 0.0145 sec/batch\n",
      "Global Step: 51500 Epoch 24/50 Iteration: 51500 Avg. Training loss: 2.8833 0.0127 sec/batch\n",
      "Global Step: 51600 Epoch 24/50 Iteration: 51600 Avg. Training loss: 2.8881 0.0141 sec/batch\n",
      "Global Step: 51700 Epoch 24/50 Iteration: 51700 Avg. Training loss: 2.8785 0.0156 sec/batch\n",
      "Global Step: 51800 Epoch 24/50 Iteration: 51800 Avg. Training loss: 2.9039 0.0175 sec/batch\n",
      "Global Step: 51900 Epoch 24/50 Iteration: 51900 Avg. Training loss: 2.8646 0.0183 sec/batch\n",
      "Global Step: 52000 Epoch 24/50 Iteration: 52000 Avg. Training loss: 2.8780 0.0129 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 52100 Epoch 24/50 Iteration: 52100 Avg. Training loss: 2.8830 0.0154 sec/batch\n",
      "Global Step: 52200 Epoch 24/50 Iteration: 52200 Avg. Training loss: 2.8850 0.0164 sec/batch\n",
      "Global Step: 52300 Epoch 24/50 Iteration: 52300 Avg. Training loss: 2.8776 0.0195 sec/batch\n",
      "Global Step: 52400 Epoch 24/50 Iteration: 52400 Avg. Training loss: 2.8848 0.0186 sec/batch\n",
      "Global Step: 52500 Epoch 24/50 Iteration: 52500 Avg. Training loss: 2.8711 0.0180 sec/batch\n",
      "Global Step: 52600 Epoch 24/50 Iteration: 52600 Avg. Training loss: 2.9069 0.0155 sec/batch\n",
      "Global Step: 52700 Epoch 24/50 Iteration: 52700 Avg. Training loss: 2.8821 0.0157 sec/batch\n",
      "Global Step: 52800 Epoch 24/50 Iteration: 52800 Avg. Training loss: 2.8903 0.0174 sec/batch\n",
      "Global Step: 52900 Epoch 24/50 Iteration: 52900 Avg. Training loss: 2.8609 0.0161 sec/batch\n",
      "Global Step: 53000 Epoch 24/50 Iteration: 53000 Avg. Training loss: 2.8688 0.0179 sec/batch\n",
      "Global Step: 53100 Epoch 24/50 Iteration: 53100 Avg. Training loss: 2.8770 0.0165 sec/batch\n",
      "Epoch 25/50 Threshold: 0.07124153558946027 Length of Training words: 2324110\n",
      "Global Step: 53200 Epoch 25/50 Iteration: 53200 Avg. Training loss: 2.8780 0.0034 sec/batch\n",
      "Global Step: 53300 Epoch 25/50 Iteration: 53300 Avg. Training loss: 2.8800 0.0142 sec/batch\n",
      "Global Step: 53400 Epoch 25/50 Iteration: 53400 Avg. Training loss: 2.9030 0.0154 sec/batch\n",
      "Global Step: 53500 Epoch 25/50 Iteration: 53500 Avg. Training loss: 2.8772 0.0163 sec/batch\n",
      "Global Step: 53600 Epoch 25/50 Iteration: 53600 Avg. Training loss: 2.8632 0.0164 sec/batch\n",
      "Global Step: 53700 Epoch 25/50 Iteration: 53700 Avg. Training loss: 2.8522 0.0186 sec/batch\n",
      "Global Step: 53800 Epoch 25/50 Iteration: 53800 Avg. Training loss: 2.8774 0.0183 sec/batch\n",
      "Global Step: 53900 Epoch 25/50 Iteration: 53900 Avg. Training loss: 2.8713 0.0139 sec/batch\n",
      "Global Step: 54000 Epoch 25/50 Iteration: 54000 Avg. Training loss: 2.8686 0.0132 sec/batch\n",
      "Global Step: 54100 Epoch 25/50 Iteration: 54100 Avg. Training loss: 2.8877 0.0101 sec/batch\n",
      "Global Step: 54200 Epoch 25/50 Iteration: 54200 Avg. Training loss: 2.8502 0.0141 sec/batch\n",
      "Global Step: 54300 Epoch 25/50 Iteration: 54300 Avg. Training loss: 2.8624 0.0174 sec/batch\n",
      "Global Step: 54400 Epoch 25/50 Iteration: 54400 Avg. Training loss: 2.8688 0.0171 sec/batch\n",
      "Global Step: 54500 Epoch 25/50 Iteration: 54500 Avg. Training loss: 2.8722 0.0161 sec/batch\n",
      "Global Step: 54600 Epoch 25/50 Iteration: 54600 Avg. Training loss: 2.8646 0.0177 sec/batch\n",
      "Global Step: 54700 Epoch 25/50 Iteration: 54700 Avg. Training loss: 2.8747 0.0139 sec/batch\n",
      "Global Step: 54800 Epoch 25/50 Iteration: 54800 Avg. Training loss: 2.8523 0.0150 sec/batch\n",
      "Global Step: 54900 Epoch 25/50 Iteration: 54900 Avg. Training loss: 2.8947 0.0160 sec/batch\n",
      "Global Step: 55000 Epoch 25/50 Iteration: 55000 Avg. Training loss: 2.8695 0.0152 sec/batch\n",
      "Global Step: 55100 Epoch 25/50 Iteration: 55100 Avg. Training loss: 2.8840 0.0155 sec/batch\n",
      "Global Step: 55200 Epoch 25/50 Iteration: 55200 Avg. Training loss: 2.8481 0.0183 sec/batch\n",
      "Global Step: 55300 Epoch 25/50 Iteration: 55300 Avg. Training loss: 2.8494 0.0145 sec/batch\n",
      "Global Step: 55400 Epoch 25/50 Iteration: 55400 Avg. Training loss: 2.8586 0.0146 sec/batch\n",
      "Epoch 26/50 Threshold: 0.06381515429289862 Length of Training words: 2261791\n",
      "Global Step: 55500 Epoch 26/50 Iteration: 55500 Avg. Training loss: 2.8660 0.0004 sec/batch\n",
      "Global Step: 55600 Epoch 26/50 Iteration: 55600 Avg. Training loss: 2.9101 0.0177 sec/batch\n",
      "Global Step: 55700 Epoch 26/50 Iteration: 55700 Avg. Training loss: 2.9141 0.0151 sec/batch\n",
      "Global Step: 55800 Epoch 26/50 Iteration: 55800 Avg. Training loss: 2.8931 0.0182 sec/batch\n",
      "Global Step: 55900 Epoch 26/50 Iteration: 55900 Avg. Training loss: 2.8832 0.0167 sec/batch\n",
      "Global Step: 56000 Epoch 26/50 Iteration: 56000 Avg. Training loss: 2.8779 0.0164 sec/batch\n",
      "Global Step: 56100 Epoch 26/50 Iteration: 56100 Avg. Training loss: 2.8880 0.0164 sec/batch\n",
      "Global Step: 56200 Epoch 26/50 Iteration: 56200 Avg. Training loss: 2.8938 0.0164 sec/batch\n",
      "Global Step: 56300 Epoch 26/50 Iteration: 56300 Avg. Training loss: 2.8835 0.0183 sec/batch\n",
      "Global Step: 56400 Epoch 26/50 Iteration: 56400 Avg. Training loss: 2.9057 0.0187 sec/batch\n",
      "Global Step: 56500 Epoch 26/50 Iteration: 56500 Avg. Training loss: 2.8748 0.0178 sec/batch\n",
      "Global Step: 56600 Epoch 26/50 Iteration: 56600 Avg. Training loss: 2.8793 0.0193 sec/batch\n",
      "Global Step: 56700 Epoch 26/50 Iteration: 56700 Avg. Training loss: 2.8855 0.0209 sec/batch\n",
      "Global Step: 56800 Epoch 26/50 Iteration: 56800 Avg. Training loss: 2.8931 0.0198 sec/batch\n",
      "Global Step: 56900 Epoch 26/50 Iteration: 56900 Avg. Training loss: 2.8872 0.0172 sec/batch\n",
      "Global Step: 57000 Epoch 26/50 Iteration: 57000 Avg. Training loss: 2.8906 0.0155 sec/batch\n",
      "Global Step: 57100 Epoch 26/50 Iteration: 57100 Avg. Training loss: 2.8861 0.0163 sec/batch\n",
      "Global Step: 57200 Epoch 26/50 Iteration: 57200 Avg. Training loss: 2.9047 0.0187 sec/batch\n",
      "Global Step: 57300 Epoch 26/50 Iteration: 57300 Avg. Training loss: 2.8885 0.0175 sec/batch\n",
      "Global Step: 57400 Epoch 26/50 Iteration: 57400 Avg. Training loss: 2.8927 0.0178 sec/batch\n",
      "Global Step: 57500 Epoch 26/50 Iteration: 57500 Avg. Training loss: 2.8703 0.0163 sec/batch\n",
      "Global Step: 57600 Epoch 26/50 Iteration: 57600 Avg. Training loss: 2.8733 0.0147 sec/batch\n",
      "Global Step: 57700 Epoch 26/50 Iteration: 57700 Avg. Training loss: 2.8815 0.0191 sec/batch\n",
      "Epoch 27/50 Threshold: 0.05531688109305566 Length of Training words: 2184814\n",
      "Global Step: 57800 Epoch 27/50 Iteration: 57800 Avg. Training loss: 2.9059 0.0060 sec/batch\n",
      "Global Step: 57900 Epoch 27/50 Iteration: 57900 Avg. Training loss: 2.9310 0.0197 sec/batch\n",
      "Global Step: 58000 Epoch 27/50 Iteration: 58000 Avg. Training loss: 2.9337 0.0176 sec/batch\n",
      "Global Step: 58100 Epoch 27/50 Iteration: 58100 Avg. Training loss: 2.9087 0.0168 sec/batch\n",
      "Global Step: 58200 Epoch 27/50 Iteration: 58200 Avg. Training loss: 2.9045 0.0161 sec/batch\n",
      "Global Step: 58300 Epoch 27/50 Iteration: 58300 Avg. Training loss: 2.9089 0.0153 sec/batch\n",
      "Global Step: 58400 Epoch 27/50 Iteration: 58400 Avg. Training loss: 2.9060 0.0142 sec/batch\n",
      "Global Step: 58500 Epoch 27/50 Iteration: 58500 Avg. Training loss: 2.9224 0.0167 sec/batch\n",
      "Global Step: 58600 Epoch 27/50 Iteration: 58600 Avg. Training loss: 2.9259 0.0146 sec/batch\n",
      "Global Step: 58700 Epoch 27/50 Iteration: 58700 Avg. Training loss: 2.9040 0.0163 sec/batch\n",
      "Global Step: 58800 Epoch 27/50 Iteration: 58800 Avg. Training loss: 2.8955 0.0159 sec/batch\n",
      "Global Step: 58900 Epoch 27/50 Iteration: 58900 Avg. Training loss: 2.9257 0.0143 sec/batch\n",
      "Global Step: 59000 Epoch 27/50 Iteration: 59000 Avg. Training loss: 2.9054 0.0165 sec/batch\n",
      "Global Step: 59100 Epoch 27/50 Iteration: 59100 Avg. Training loss: 2.9115 0.0155 sec/batch\n",
      "Global Step: 59200 Epoch 27/50 Iteration: 59200 Avg. Training loss: 2.9143 0.0152 sec/batch\n",
      "Global Step: 59300 Epoch 27/50 Iteration: 59300 Avg. Training loss: 2.9041 0.0164 sec/batch\n",
      "Global Step: 59400 Epoch 27/50 Iteration: 59400 Avg. Training loss: 2.9346 0.0181 sec/batch\n",
      "Global Step: 59500 Epoch 27/50 Iteration: 59500 Avg. Training loss: 2.9126 0.0180 sec/batch\n",
      "Global Step: 59600 Epoch 27/50 Iteration: 59600 Avg. Training loss: 2.9216 0.0143 sec/batch\n",
      "Global Step: 59700 Epoch 27/50 Iteration: 59700 Avg. Training loss: 2.8898 0.0131 sec/batch\n",
      "Global Step: 59800 Epoch 27/50 Iteration: 59800 Avg. Training loss: 2.8968 0.0162 sec/batch\n",
      "Global Step: 59900 Epoch 27/50 Iteration: 59900 Avg. Training loss: 2.9107 0.0165 sec/batch\n",
      "Epoch 28/50 Threshold: 0.04761855868358424 Length of Training words: 2093205\n",
      "Global Step: 60000 Epoch 28/50 Iteration: 60000 Avg. Training loss: 2.9305 0.0089 sec/batch\n",
      "Global Step: 60100 Epoch 28/50 Iteration: 60100 Avg. Training loss: 2.9610 0.0152 sec/batch\n",
      "Global Step: 60200 Epoch 28/50 Iteration: 60200 Avg. Training loss: 2.9581 0.0153 sec/batch\n",
      "Global Step: 60300 Epoch 28/50 Iteration: 60300 Avg. Training loss: 2.9277 0.0174 sec/batch\n",
      "Global Step: 60400 Epoch 28/50 Iteration: 60400 Avg. Training loss: 2.9347 0.0180 sec/batch\n",
      "Global Step: 60500 Epoch 28/50 Iteration: 60500 Avg. Training loss: 2.9333 0.0141 sec/batch\n",
      "Global Step: 60600 Epoch 28/50 Iteration: 60600 Avg. Training loss: 2.9427 0.0163 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 60700 Epoch 28/50 Iteration: 60700 Avg. Training loss: 2.9449 0.0156 sec/batch\n",
      "Global Step: 60800 Epoch 28/50 Iteration: 60800 Avg. Training loss: 2.9434 0.0176 sec/batch\n",
      "Global Step: 60900 Epoch 28/50 Iteration: 60900 Avg. Training loss: 2.9295 0.0157 sec/batch\n",
      "Global Step: 61000 Epoch 28/50 Iteration: 61000 Avg. Training loss: 2.9385 0.0152 sec/batch\n",
      "Global Step: 61100 Epoch 28/50 Iteration: 61100 Avg. Training loss: 2.9350 0.0144 sec/batch\n",
      "Global Step: 61200 Epoch 28/50 Iteration: 61200 Avg. Training loss: 2.9436 0.0154 sec/batch\n",
      "Global Step: 61300 Epoch 28/50 Iteration: 61300 Avg. Training loss: 2.9388 0.0167 sec/batch\n",
      "Global Step: 61400 Epoch 28/50 Iteration: 61400 Avg. Training loss: 2.9279 0.0171 sec/batch\n",
      "Global Step: 61500 Epoch 28/50 Iteration: 61500 Avg. Training loss: 2.9606 0.0176 sec/batch\n",
      "Global Step: 61600 Epoch 28/50 Iteration: 61600 Avg. Training loss: 2.9428 0.0151 sec/batch\n",
      "Global Step: 61700 Epoch 28/50 Iteration: 61700 Avg. Training loss: 2.9424 0.0160 sec/batch\n",
      "Global Step: 61800 Epoch 28/50 Iteration: 61800 Avg. Training loss: 2.9237 0.0155 sec/batch\n",
      "Global Step: 61900 Epoch 28/50 Iteration: 61900 Avg. Training loss: 2.9274 0.0139 sec/batch\n",
      "Global Step: 62000 Epoch 28/50 Iteration: 62000 Avg. Training loss: 2.9410 0.0133 sec/batch\n",
      "Epoch 29/50 Threshold: 0.05664707315334713 Length of Training words: 2198736\n",
      "Global Step: 62100 Epoch 29/50 Iteration: 62100 Avg. Training loss: 2.9241 0.0097 sec/batch\n",
      "Global Step: 62200 Epoch 29/50 Iteration: 62200 Avg. Training loss: 2.9268 0.0157 sec/batch\n",
      "Global Step: 62300 Epoch 29/50 Iteration: 62300 Avg. Training loss: 2.9268 0.0165 sec/batch\n",
      "Global Step: 62400 Epoch 29/50 Iteration: 62400 Avg. Training loss: 2.8967 0.0136 sec/batch\n",
      "Global Step: 62500 Epoch 29/50 Iteration: 62500 Avg. Training loss: 2.9080 0.0152 sec/batch\n",
      "Global Step: 62600 Epoch 29/50 Iteration: 62600 Avg. Training loss: 2.9009 0.0159 sec/batch\n",
      "Global Step: 62700 Epoch 29/50 Iteration: 62700 Avg. Training loss: 2.9143 0.0182 sec/batch\n",
      "Global Step: 62800 Epoch 29/50 Iteration: 62800 Avg. Training loss: 2.9060 0.0176 sec/batch\n",
      "Global Step: 62900 Epoch 29/50 Iteration: 62900 Avg. Training loss: 2.9296 0.0190 sec/batch\n",
      "Global Step: 63000 Epoch 29/50 Iteration: 63000 Avg. Training loss: 2.8878 0.0169 sec/batch\n",
      "Global Step: 63100 Epoch 29/50 Iteration: 63100 Avg. Training loss: 2.9010 0.0149 sec/batch\n",
      "Global Step: 63200 Epoch 29/50 Iteration: 63200 Avg. Training loss: 2.9081 0.0147 sec/batch\n",
      "Global Step: 63300 Epoch 29/50 Iteration: 63300 Avg. Training loss: 2.9091 0.0201 sec/batch\n",
      "Global Step: 63400 Epoch 29/50 Iteration: 63400 Avg. Training loss: 2.9033 0.0175 sec/batch\n",
      "Global Step: 63500 Epoch 29/50 Iteration: 63500 Avg. Training loss: 2.9118 0.0152 sec/batch\n",
      "Global Step: 63600 Epoch 29/50 Iteration: 63600 Avg. Training loss: 2.9089 0.0178 sec/batch\n",
      "Global Step: 63700 Epoch 29/50 Iteration: 63700 Avg. Training loss: 2.9212 0.0176 sec/batch\n",
      "Global Step: 63800 Epoch 29/50 Iteration: 63800 Avg. Training loss: 2.9127 0.0172 sec/batch\n",
      "Global Step: 63900 Epoch 29/50 Iteration: 63900 Avg. Training loss: 2.9115 0.0191 sec/batch\n",
      "Global Step: 64000 Epoch 29/50 Iteration: 64000 Avg. Training loss: 2.8847 0.0183 sec/batch\n",
      "Global Step: 64100 Epoch 29/50 Iteration: 64100 Avg. Training loss: 2.8989 0.0192 sec/batch\n",
      "Global Step: 64200 Epoch 29/50 Iteration: 64200 Avg. Training loss: 2.9044 0.0181 sec/batch\n",
      "Epoch 30/50 Threshold: 0.0696478998206196 Length of Training words: 2311461\n",
      "Global Step: 64300 Epoch 30/50 Iteration: 64300 Avg. Training loss: 2.8920 0.0101 sec/batch\n",
      "Global Step: 64400 Epoch 30/50 Iteration: 64400 Avg. Training loss: 2.8918 0.0153 sec/batch\n",
      "Global Step: 64500 Epoch 30/50 Iteration: 64500 Avg. Training loss: 2.8914 0.0148 sec/batch\n",
      "Global Step: 64600 Epoch 30/50 Iteration: 64600 Avg. Training loss: 2.8660 0.0138 sec/batch\n",
      "Global Step: 64700 Epoch 30/50 Iteration: 64700 Avg. Training loss: 2.8661 0.0146 sec/batch\n",
      "Global Step: 64800 Epoch 30/50 Iteration: 64800 Avg. Training loss: 2.8655 0.0169 sec/batch\n",
      "Global Step: 64900 Epoch 30/50 Iteration: 64900 Avg. Training loss: 2.8703 0.0148 sec/batch\n",
      "Global Step: 65000 Epoch 30/50 Iteration: 65000 Avg. Training loss: 2.8923 0.0165 sec/batch\n",
      "Global Step: 65100 Epoch 30/50 Iteration: 65100 Avg. Training loss: 2.8741 0.0138 sec/batch\n",
      "Global Step: 65200 Epoch 30/50 Iteration: 65200 Avg. Training loss: 2.8610 0.0142 sec/batch\n",
      "Global Step: 65300 Epoch 30/50 Iteration: 65300 Avg. Training loss: 2.8636 0.0169 sec/batch\n",
      "Global Step: 65400 Epoch 30/50 Iteration: 65400 Avg. Training loss: 2.8752 0.0185 sec/batch\n",
      "Global Step: 65500 Epoch 30/50 Iteration: 65500 Avg. Training loss: 2.8693 0.0180 sec/batch\n",
      "Global Step: 65600 Epoch 30/50 Iteration: 65600 Avg. Training loss: 2.8635 0.0174 sec/batch\n",
      "Global Step: 65700 Epoch 30/50 Iteration: 65700 Avg. Training loss: 2.8841 0.0144 sec/batch\n",
      "Global Step: 65800 Epoch 30/50 Iteration: 65800 Avg. Training loss: 2.8575 0.0171 sec/batch\n",
      "Global Step: 65900 Epoch 30/50 Iteration: 65900 Avg. Training loss: 2.8850 0.0205 sec/batch\n",
      "Global Step: 66000 Epoch 30/50 Iteration: 66000 Avg. Training loss: 2.8773 0.0178 sec/batch\n",
      "Global Step: 66100 Epoch 30/50 Iteration: 66100 Avg. Training loss: 2.8880 0.0164 sec/batch\n",
      "Global Step: 66200 Epoch 30/50 Iteration: 66200 Avg. Training loss: 2.8709 0.0187 sec/batch\n",
      "Global Step: 66300 Epoch 30/50 Iteration: 66300 Avg. Training loss: 2.8502 0.0176 sec/batch\n",
      "Global Step: 66400 Epoch 30/50 Iteration: 66400 Avg. Training loss: 2.8641 0.0147 sec/batch\n",
      "Global Step: 66500 Epoch 30/50 Iteration: 66500 Avg. Training loss: 2.8730 0.0158 sec/batch\n",
      "Epoch 31/50 Threshold: 0.07179336368747122 Length of Training words: 2328517\n",
      "Global Step: 66600 Epoch 31/50 Iteration: 66600 Avg. Training loss: 2.8701 0.0095 sec/batch\n",
      "Global Step: 66700 Epoch 31/50 Iteration: 66700 Avg. Training loss: 2.8902 0.0155 sec/batch\n",
      "Global Step: 66800 Epoch 31/50 Iteration: 66800 Avg. Training loss: 2.8917 0.0153 sec/batch\n",
      "Global Step: 66900 Epoch 31/50 Iteration: 66900 Avg. Training loss: 2.8582 0.0190 sec/batch\n",
      "Global Step: 67000 Epoch 31/50 Iteration: 67000 Avg. Training loss: 2.8654 0.0181 sec/batch\n",
      "Global Step: 67100 Epoch 31/50 Iteration: 67100 Avg. Training loss: 2.8546 0.0172 sec/batch\n",
      "Global Step: 67200 Epoch 31/50 Iteration: 67200 Avg. Training loss: 2.8687 0.0169 sec/batch\n",
      "Global Step: 67300 Epoch 31/50 Iteration: 67300 Avg. Training loss: 2.8783 0.0143 sec/batch\n",
      "Global Step: 67400 Epoch 31/50 Iteration: 67400 Avg. Training loss: 2.8715 0.0170 sec/batch\n",
      "Global Step: 67500 Epoch 31/50 Iteration: 67500 Avg. Training loss: 2.8733 0.0156 sec/batch\n",
      "Global Step: 67600 Epoch 31/50 Iteration: 67600 Avg. Training loss: 2.8487 0.0129 sec/batch\n",
      "Global Step: 67700 Epoch 31/50 Iteration: 67700 Avg. Training loss: 2.8683 0.0158 sec/batch\n",
      "Global Step: 67800 Epoch 31/50 Iteration: 67800 Avg. Training loss: 2.8616 0.0134 sec/batch\n",
      "Global Step: 67900 Epoch 31/50 Iteration: 67900 Avg. Training loss: 2.8701 0.0147 sec/batch\n",
      "Global Step: 68000 Epoch 31/50 Iteration: 68000 Avg. Training loss: 2.8632 0.0139 sec/batch\n",
      "Global Step: 68100 Epoch 31/50 Iteration: 68100 Avg. Training loss: 2.8716 0.0142 sec/batch\n",
      "Global Step: 68200 Epoch 31/50 Iteration: 68200 Avg. Training loss: 2.8640 0.0158 sec/batch\n",
      "Global Step: 68300 Epoch 31/50 Iteration: 68300 Avg. Training loss: 2.8839 0.0157 sec/batch\n",
      "Global Step: 68400 Epoch 31/50 Iteration: 68400 Avg. Training loss: 2.8680 0.0163 sec/batch\n",
      "Global Step: 68500 Epoch 31/50 Iteration: 68500 Avg. Training loss: 2.8753 0.0189 sec/batch\n",
      "Global Step: 68600 Epoch 31/50 Iteration: 68600 Avg. Training loss: 2.8448 0.0157 sec/batch\n",
      "Global Step: 68700 Epoch 31/50 Iteration: 68700 Avg. Training loss: 2.8511 0.0175 sec/batch\n",
      "Global Step: 68800 Epoch 31/50 Iteration: 68800 Avg. Training loss: 2.8601 0.0161 sec/batch\n",
      "Epoch 32/50 Threshold: 0.05873743345724779 Length of Training words: 2217588\n",
      "Global Step: 68900 Epoch 32/50 Iteration: 68900 Avg. Training loss: 2.8755 0.0044 sec/batch\n",
      "Global Step: 69000 Epoch 32/50 Iteration: 69000 Avg. Training loss: 2.9178 0.0163 sec/batch\n",
      "Global Step: 69100 Epoch 32/50 Iteration: 69100 Avg. Training loss: 2.9299 0.0144 sec/batch\n",
      "Global Step: 69200 Epoch 32/50 Iteration: 69200 Avg. Training loss: 2.9035 0.0143 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 69300 Epoch 32/50 Iteration: 69300 Avg. Training loss: 2.8950 0.0191 sec/batch\n",
      "Global Step: 69400 Epoch 32/50 Iteration: 69400 Avg. Training loss: 2.8913 0.0175 sec/batch\n",
      "Global Step: 69500 Epoch 32/50 Iteration: 69500 Avg. Training loss: 2.9051 0.0157 sec/batch\n",
      "Global Step: 69600 Epoch 32/50 Iteration: 69600 Avg. Training loss: 2.9153 0.0155 sec/batch\n",
      "Global Step: 69700 Epoch 32/50 Iteration: 69700 Avg. Training loss: 2.9078 0.0148 sec/batch\n",
      "Global Step: 69800 Epoch 32/50 Iteration: 69800 Avg. Training loss: 2.8929 0.0135 sec/batch\n",
      "Global Step: 69900 Epoch 32/50 Iteration: 69900 Avg. Training loss: 2.8960 0.0170 sec/batch\n",
      "Global Step: 70000 Epoch 32/50 Iteration: 70000 Avg. Training loss: 2.9036 0.0156 sec/batch\n",
      "Global Step: 70100 Epoch 32/50 Iteration: 70100 Avg. Training loss: 2.8952 0.0165 sec/batch\n",
      "Global Step: 70200 Epoch 32/50 Iteration: 70200 Avg. Training loss: 2.9034 0.0142 sec/batch\n",
      "Global Step: 70300 Epoch 32/50 Iteration: 70300 Avg. Training loss: 2.9047 0.0166 sec/batch\n",
      "Global Step: 70400 Epoch 32/50 Iteration: 70400 Avg. Training loss: 2.8879 0.0185 sec/batch\n",
      "Global Step: 70500 Epoch 32/50 Iteration: 70500 Avg. Training loss: 2.9240 0.0120 sec/batch\n",
      "Global Step: 70600 Epoch 32/50 Iteration: 70600 Avg. Training loss: 2.9004 0.0122 sec/batch\n",
      "Global Step: 70700 Epoch 32/50 Iteration: 70700 Avg. Training loss: 2.9238 0.0137 sec/batch\n",
      "Global Step: 70800 Epoch 32/50 Iteration: 70800 Avg. Training loss: 2.8848 0.0157 sec/batch\n",
      "Global Step: 70900 Epoch 32/50 Iteration: 70900 Avg. Training loss: 2.8841 0.0162 sec/batch\n",
      "Global Step: 71000 Epoch 32/50 Iteration: 71000 Avg. Training loss: 2.8930 0.0164 sec/batch\n",
      "Epoch 33/50 Threshold: 0.052261286034779746 Length of Training words: 2150700\n",
      "Global Step: 71100 Epoch 33/50 Iteration: 71100 Avg. Training loss: 2.9030 0.0015 sec/batch\n",
      "Global Step: 71200 Epoch 33/50 Iteration: 71200 Avg. Training loss: 2.9366 0.0194 sec/batch\n",
      "Global Step: 71300 Epoch 33/50 Iteration: 71300 Avg. Training loss: 2.9506 0.0180 sec/batch\n",
      "Global Step: 71400 Epoch 33/50 Iteration: 71400 Avg. Training loss: 2.9262 0.0172 sec/batch\n",
      "Global Step: 71500 Epoch 33/50 Iteration: 71500 Avg. Training loss: 2.9173 0.0176 sec/batch\n",
      "Global Step: 71600 Epoch 33/50 Iteration: 71600 Avg. Training loss: 2.9112 0.0148 sec/batch\n",
      "Global Step: 71700 Epoch 33/50 Iteration: 71700 Avg. Training loss: 2.9213 0.0148 sec/batch\n",
      "Global Step: 71800 Epoch 33/50 Iteration: 71800 Avg. Training loss: 2.9377 0.0154 sec/batch\n",
      "Global Step: 71900 Epoch 33/50 Iteration: 71900 Avg. Training loss: 2.9262 0.0171 sec/batch\n",
      "Global Step: 72000 Epoch 33/50 Iteration: 72000 Avg. Training loss: 2.9146 0.0151 sec/batch\n",
      "Global Step: 72100 Epoch 33/50 Iteration: 72100 Avg. Training loss: 2.9048 0.0157 sec/batch\n",
      "Global Step: 72200 Epoch 33/50 Iteration: 72200 Avg. Training loss: 2.9378 0.0167 sec/batch\n",
      "Global Step: 72300 Epoch 33/50 Iteration: 72300 Avg. Training loss: 2.9093 0.0194 sec/batch\n",
      "Global Step: 72400 Epoch 33/50 Iteration: 72400 Avg. Training loss: 2.9292 0.0143 sec/batch\n",
      "Global Step: 72500 Epoch 33/50 Iteration: 72500 Avg. Training loss: 2.9180 0.0140 sec/batch\n",
      "Global Step: 72600 Epoch 33/50 Iteration: 72600 Avg. Training loss: 2.9099 0.0164 sec/batch\n",
      "Global Step: 72700 Epoch 33/50 Iteration: 72700 Avg. Training loss: 2.9471 0.0155 sec/batch\n",
      "Global Step: 72800 Epoch 33/50 Iteration: 72800 Avg. Training loss: 2.9226 0.0177 sec/batch\n",
      "Global Step: 72900 Epoch 33/50 Iteration: 72900 Avg. Training loss: 2.9273 0.0157 sec/batch\n",
      "Global Step: 73000 Epoch 33/50 Iteration: 73000 Avg. Training loss: 2.9064 0.0177 sec/batch\n",
      "Global Step: 73100 Epoch 33/50 Iteration: 73100 Avg. Training loss: 2.9076 0.0173 sec/batch\n",
      "Global Step: 73200 Epoch 33/50 Iteration: 73200 Avg. Training loss: 2.9250 0.0144 sec/batch\n",
      "Epoch 34/50 Threshold: 0.07534667894549421 Length of Training words: 2355807\n",
      "Global Step: 73300 Epoch 34/50 Iteration: 73300 Avg. Training loss: 2.8879 0.0105 sec/batch\n",
      "Global Step: 73400 Epoch 34/50 Iteration: 73400 Avg. Training loss: 2.8768 0.0147 sec/batch\n",
      "Global Step: 73500 Epoch 34/50 Iteration: 73500 Avg. Training loss: 2.8832 0.0169 sec/batch\n",
      "Global Step: 73600 Epoch 34/50 Iteration: 73600 Avg. Training loss: 2.8492 0.0168 sec/batch\n",
      "Global Step: 73700 Epoch 34/50 Iteration: 73700 Avg. Training loss: 2.8555 0.0166 sec/batch\n",
      "Global Step: 73800 Epoch 34/50 Iteration: 73800 Avg. Training loss: 2.8471 0.0177 sec/batch\n",
      "Global Step: 73900 Epoch 34/50 Iteration: 73900 Avg. Training loss: 2.8591 0.0142 sec/batch\n",
      "Global Step: 74000 Epoch 34/50 Iteration: 74000 Avg. Training loss: 2.8690 0.0175 sec/batch\n",
      "Global Step: 74100 Epoch 34/50 Iteration: 74100 Avg. Training loss: 2.8655 0.0172 sec/batch\n",
      "Global Step: 74200 Epoch 34/50 Iteration: 74200 Avg. Training loss: 2.8618 0.0153 sec/batch\n",
      "Global Step: 74300 Epoch 34/50 Iteration: 74300 Avg. Training loss: 2.8426 0.0174 sec/batch\n",
      "Global Step: 74400 Epoch 34/50 Iteration: 74400 Avg. Training loss: 2.8503 0.0138 sec/batch\n",
      "Global Step: 74500 Epoch 34/50 Iteration: 74500 Avg. Training loss: 2.8582 0.0171 sec/batch\n",
      "Global Step: 74600 Epoch 34/50 Iteration: 74600 Avg. Training loss: 2.8648 0.0183 sec/batch\n",
      "Global Step: 74700 Epoch 34/50 Iteration: 74700 Avg. Training loss: 2.8582 0.0172 sec/batch\n",
      "Global Step: 74800 Epoch 34/50 Iteration: 74800 Avg. Training loss: 2.8646 0.0164 sec/batch\n",
      "Global Step: 74900 Epoch 34/50 Iteration: 74900 Avg. Training loss: 2.8433 0.0127 sec/batch\n",
      "Global Step: 75000 Epoch 34/50 Iteration: 75000 Avg. Training loss: 2.8826 0.0168 sec/batch\n",
      "Global Step: 75100 Epoch 34/50 Iteration: 75100 Avg. Training loss: 2.8598 0.0161 sec/batch\n",
      "Global Step: 75200 Epoch 34/50 Iteration: 75200 Avg. Training loss: 2.8687 0.0143 sec/batch\n",
      "Global Step: 75300 Epoch 34/50 Iteration: 75300 Avg. Training loss: 2.8324 0.0229 sec/batch\n",
      "Global Step: 75400 Epoch 34/50 Iteration: 75400 Avg. Training loss: 2.8441 0.0189 sec/batch\n",
      "Global Step: 75500 Epoch 34/50 Iteration: 75500 Avg. Training loss: 2.8455 0.0145 sec/batch\n",
      "Epoch 35/50 Threshold: 0.04374003192777994 Length of Training words: 2043816\n",
      "Global Step: 75600 Epoch 35/50 Iteration: 75600 Avg. Training loss: 2.8610 0.0009 sec/batch\n",
      "Global Step: 75700 Epoch 35/50 Iteration: 75700 Avg. Training loss: 2.9733 0.0165 sec/batch\n",
      "Global Step: 75800 Epoch 35/50 Iteration: 75800 Avg. Training loss: 2.9844 0.0160 sec/batch\n",
      "Global Step: 75900 Epoch 35/50 Iteration: 75900 Avg. Training loss: 2.9531 0.0130 sec/batch\n",
      "Global Step: 76000 Epoch 35/50 Iteration: 76000 Avg. Training loss: 2.9485 0.0172 sec/batch\n",
      "Global Step: 76100 Epoch 35/50 Iteration: 76100 Avg. Training loss: 2.9463 0.0148 sec/batch\n",
      "Global Step: 76200 Epoch 35/50 Iteration: 76200 Avg. Training loss: 2.9487 0.0141 sec/batch\n",
      "Global Step: 76300 Epoch 35/50 Iteration: 76300 Avg. Training loss: 2.9598 0.0178 sec/batch\n",
      "Global Step: 76400 Epoch 35/50 Iteration: 76400 Avg. Training loss: 2.9724 0.0151 sec/batch\n",
      "Global Step: 76500 Epoch 35/50 Iteration: 76500 Avg. Training loss: 2.9357 0.0176 sec/batch\n",
      "Global Step: 76600 Epoch 35/50 Iteration: 76600 Avg. Training loss: 2.9424 0.0182 sec/batch\n",
      "Global Step: 76700 Epoch 35/50 Iteration: 76700 Avg. Training loss: 2.9609 0.0197 sec/batch\n",
      "Global Step: 76800 Epoch 35/50 Iteration: 76800 Avg. Training loss: 2.9449 0.0185 sec/batch\n",
      "Global Step: 76900 Epoch 35/50 Iteration: 76900 Avg. Training loss: 2.9616 0.0174 sec/batch\n",
      "Global Step: 77000 Epoch 35/50 Iteration: 77000 Avg. Training loss: 2.9381 0.0171 sec/batch\n",
      "Global Step: 77100 Epoch 35/50 Iteration: 77100 Avg. Training loss: 2.9706 0.0152 sec/batch\n",
      "Global Step: 77200 Epoch 35/50 Iteration: 77200 Avg. Training loss: 2.9568 0.0146 sec/batch\n",
      "Global Step: 77300 Epoch 35/50 Iteration: 77300 Avg. Training loss: 2.9682 0.0190 sec/batch\n",
      "Global Step: 77400 Epoch 35/50 Iteration: 77400 Avg. Training loss: 2.9311 0.0143 sec/batch\n",
      "Global Step: 77500 Epoch 35/50 Iteration: 77500 Avg. Training loss: 2.9406 0.0159 sec/batch\n",
      "Global Step: 77600 Epoch 35/50 Iteration: 77600 Avg. Training loss: 2.9539 0.0138 sec/batch\n",
      "Epoch 36/50 Threshold: 0.05626833438411996 Length of Training words: 2194460\n",
      "Global Step: 77700 Epoch 36/50 Iteration: 77700 Avg. Training loss: 2.9322 0.0090 sec/batch\n",
      "Global Step: 77800 Epoch 36/50 Iteration: 77800 Avg. Training loss: 2.9283 0.0169 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 77900 Epoch 36/50 Iteration: 77900 Avg. Training loss: 2.9273 0.0136 sec/batch\n",
      "Global Step: 78000 Epoch 36/50 Iteration: 78000 Avg. Training loss: 2.8979 0.0134 sec/batch\n",
      "Global Step: 78100 Epoch 36/50 Iteration: 78100 Avg. Training loss: 2.9065 0.0141 sec/batch\n",
      "Global Step: 78200 Epoch 36/50 Iteration: 78200 Avg. Training loss: 2.9023 0.0123 sec/batch\n",
      "Global Step: 78300 Epoch 36/50 Iteration: 78300 Avg. Training loss: 2.9123 0.0150 sec/batch\n",
      "Global Step: 78400 Epoch 36/50 Iteration: 78400 Avg. Training loss: 2.9106 0.0212 sec/batch\n",
      "Global Step: 78500 Epoch 36/50 Iteration: 78500 Avg. Training loss: 2.9304 0.0187 sec/batch\n",
      "Global Step: 78600 Epoch 36/50 Iteration: 78600 Avg. Training loss: 2.8884 0.0186 sec/batch\n",
      "Global Step: 78700 Epoch 36/50 Iteration: 78700 Avg. Training loss: 2.9021 0.0154 sec/batch\n",
      "Global Step: 78800 Epoch 36/50 Iteration: 78800 Avg. Training loss: 2.9083 0.0155 sec/batch\n",
      "Global Step: 78900 Epoch 36/50 Iteration: 78900 Avg. Training loss: 2.9098 0.0149 sec/batch\n",
      "Global Step: 79000 Epoch 36/50 Iteration: 79000 Avg. Training loss: 2.9039 0.0144 sec/batch\n",
      "Global Step: 79100 Epoch 36/50 Iteration: 79100 Avg. Training loss: 2.9108 0.0149 sec/batch\n",
      "Global Step: 79200 Epoch 36/50 Iteration: 79200 Avg. Training loss: 2.9097 0.0169 sec/batch\n",
      "Global Step: 79300 Epoch 36/50 Iteration: 79300 Avg. Training loss: 2.9191 0.0131 sec/batch\n",
      "Global Step: 79400 Epoch 36/50 Iteration: 79400 Avg. Training loss: 2.9170 0.0166 sec/batch\n",
      "Global Step: 79500 Epoch 36/50 Iteration: 79500 Avg. Training loss: 2.9146 0.0178 sec/batch\n",
      "Global Step: 79600 Epoch 36/50 Iteration: 79600 Avg. Training loss: 2.8861 0.0173 sec/batch\n",
      "Global Step: 79700 Epoch 36/50 Iteration: 79700 Avg. Training loss: 2.9027 0.0180 sec/batch\n",
      "Global Step: 79800 Epoch 36/50 Iteration: 79800 Avg. Training loss: 2.9058 0.0147 sec/batch\n",
      "Epoch 37/50 Threshold: 0.06393583698436207 Length of Training words: 2264201\n",
      "Global Step: 79900 Epoch 37/50 Iteration: 79900 Avg. Training loss: 2.8999 0.0104 sec/batch\n",
      "Global Step: 80000 Epoch 37/50 Iteration: 80000 Avg. Training loss: 2.9086 0.0165 sec/batch\n",
      "Global Step: 80100 Epoch 37/50 Iteration: 80100 Avg. Training loss: 2.9056 0.0129 sec/batch\n",
      "Global Step: 80200 Epoch 37/50 Iteration: 80200 Avg. Training loss: 2.8809 0.0137 sec/batch\n",
      "Global Step: 80300 Epoch 37/50 Iteration: 80300 Avg. Training loss: 2.8800 0.0146 sec/batch\n",
      "Global Step: 80400 Epoch 37/50 Iteration: 80400 Avg. Training loss: 2.8848 0.0153 sec/batch\n",
      "Global Step: 80500 Epoch 37/50 Iteration: 80500 Avg. Training loss: 2.8829 0.0166 sec/batch\n",
      "Global Step: 80600 Epoch 37/50 Iteration: 80600 Avg. Training loss: 2.8980 0.0155 sec/batch\n",
      "Global Step: 80700 Epoch 37/50 Iteration: 80700 Avg. Training loss: 2.8991 0.0158 sec/batch\n",
      "Global Step: 80800 Epoch 37/50 Iteration: 80800 Avg. Training loss: 2.8769 0.0167 sec/batch\n",
      "Global Step: 80900 Epoch 37/50 Iteration: 80900 Avg. Training loss: 2.8670 0.0152 sec/batch\n",
      "Global Step: 81000 Epoch 37/50 Iteration: 81000 Avg. Training loss: 2.9026 0.0144 sec/batch\n",
      "Global Step: 81100 Epoch 37/50 Iteration: 81100 Avg. Training loss: 2.8744 0.0141 sec/batch\n",
      "Global Step: 81200 Epoch 37/50 Iteration: 81200 Avg. Training loss: 2.8962 0.0183 sec/batch\n",
      "Global Step: 81300 Epoch 37/50 Iteration: 81300 Avg. Training loss: 2.8883 0.0167 sec/batch\n",
      "Global Step: 81400 Epoch 37/50 Iteration: 81400 Avg. Training loss: 2.8683 0.0170 sec/batch\n",
      "Global Step: 81500 Epoch 37/50 Iteration: 81500 Avg. Training loss: 2.9060 0.0148 sec/batch\n",
      "Global Step: 81600 Epoch 37/50 Iteration: 81600 Avg. Training loss: 2.8937 0.0168 sec/batch\n",
      "Global Step: 81700 Epoch 37/50 Iteration: 81700 Avg. Training loss: 2.9046 0.0160 sec/batch\n",
      "Global Step: 81800 Epoch 37/50 Iteration: 81800 Avg. Training loss: 2.8686 0.0162 sec/batch\n",
      "Global Step: 81900 Epoch 37/50 Iteration: 81900 Avg. Training loss: 2.8650 0.0148 sec/batch\n",
      "Global Step: 82000 Epoch 37/50 Iteration: 82000 Avg. Training loss: 2.8815 0.0193 sec/batch\n",
      "Epoch 38/50 Threshold: 0.05788771820140076 Length of Training words: 2211078\n",
      "Global Step: 82100 Epoch 38/50 Iteration: 82100 Avg. Training loss: 2.8866 0.0007 sec/batch\n",
      "Global Step: 82200 Epoch 38/50 Iteration: 82200 Avg. Training loss: 2.9235 0.0141 sec/batch\n",
      "Global Step: 82300 Epoch 38/50 Iteration: 82300 Avg. Training loss: 2.9322 0.0140 sec/batch\n",
      "Global Step: 82400 Epoch 38/50 Iteration: 82400 Avg. Training loss: 2.9094 0.0166 sec/batch\n",
      "Global Step: 82500 Epoch 38/50 Iteration: 82500 Avg. Training loss: 2.8986 0.0147 sec/batch\n",
      "Global Step: 82600 Epoch 38/50 Iteration: 82600 Avg. Training loss: 2.8830 0.0160 sec/batch\n",
      "Global Step: 82700 Epoch 38/50 Iteration: 82700 Avg. Training loss: 2.9177 0.0143 sec/batch\n",
      "Global Step: 82800 Epoch 38/50 Iteration: 82800 Avg. Training loss: 2.9039 0.0168 sec/batch\n",
      "Global Step: 82900 Epoch 38/50 Iteration: 82900 Avg. Training loss: 2.9117 0.0155 sec/batch\n",
      "Global Step: 83000 Epoch 38/50 Iteration: 83000 Avg. Training loss: 2.9066 0.0158 sec/batch\n",
      "Global Step: 83100 Epoch 38/50 Iteration: 83100 Avg. Training loss: 2.8895 0.0153 sec/batch\n",
      "Global Step: 83200 Epoch 38/50 Iteration: 83200 Avg. Training loss: 2.9090 0.0127 sec/batch\n",
      "Global Step: 83300 Epoch 38/50 Iteration: 83300 Avg. Training loss: 2.8983 0.0137 sec/batch\n",
      "Global Step: 83400 Epoch 38/50 Iteration: 83400 Avg. Training loss: 2.8947 0.0169 sec/batch\n",
      "Global Step: 83500 Epoch 38/50 Iteration: 83500 Avg. Training loss: 2.9141 0.0183 sec/batch\n",
      "Global Step: 83600 Epoch 38/50 Iteration: 83600 Avg. Training loss: 2.8869 0.0165 sec/batch\n",
      "Global Step: 83700 Epoch 38/50 Iteration: 83700 Avg. Training loss: 2.9225 0.0148 sec/batch\n",
      "Global Step: 83800 Epoch 38/50 Iteration: 83800 Avg. Training loss: 2.9032 0.0234 sec/batch\n",
      "Global Step: 83900 Epoch 38/50 Iteration: 83900 Avg. Training loss: 2.9246 0.0185 sec/batch\n",
      "Global Step: 84000 Epoch 38/50 Iteration: 84000 Avg. Training loss: 2.8929 0.0204 sec/batch\n",
      "Global Step: 84100 Epoch 38/50 Iteration: 84100 Avg. Training loss: 2.8764 0.0192 sec/batch\n",
      "Global Step: 84200 Epoch 38/50 Iteration: 84200 Avg. Training loss: 2.9073 0.0178 sec/batch\n",
      "Global Step: 84300 Epoch 38/50 Iteration: 84300 Avg. Training loss: 2.9006 0.0148 sec/batch\n",
      "Epoch 39/50 Threshold: 0.051211233562116674 Length of Training words: 2138659\n",
      "Global Step: 84400 Epoch 39/50 Iteration: 84400 Avg. Training loss: 2.9401 0.0138 sec/batch\n",
      "Global Step: 84500 Epoch 39/50 Iteration: 84500 Avg. Training loss: 2.9474 0.0123 sec/batch\n",
      "Global Step: 84600 Epoch 39/50 Iteration: 84600 Avg. Training loss: 2.9330 0.0159 sec/batch\n",
      "Global Step: 84700 Epoch 39/50 Iteration: 84700 Avg. Training loss: 2.9193 0.0161 sec/batch\n",
      "Global Step: 84800 Epoch 39/50 Iteration: 84800 Avg. Training loss: 2.9076 0.0152 sec/batch\n",
      "Global Step: 84900 Epoch 39/50 Iteration: 84900 Avg. Training loss: 2.9340 0.0177 sec/batch\n",
      "Global Step: 85000 Epoch 39/50 Iteration: 85000 Avg. Training loss: 2.9324 0.0169 sec/batch\n",
      "Global Step: 85100 Epoch 39/50 Iteration: 85100 Avg. Training loss: 2.9346 0.0158 sec/batch\n",
      "Global Step: 85200 Epoch 39/50 Iteration: 85200 Avg. Training loss: 2.9184 0.0154 sec/batch\n",
      "Global Step: 85300 Epoch 39/50 Iteration: 85300 Avg. Training loss: 2.9161 0.0147 sec/batch\n",
      "Global Step: 85400 Epoch 39/50 Iteration: 85400 Avg. Training loss: 2.9279 0.0161 sec/batch\n",
      "Global Step: 85500 Epoch 39/50 Iteration: 85500 Avg. Training loss: 2.9193 0.0168 sec/batch\n",
      "Global Step: 85600 Epoch 39/50 Iteration: 85600 Avg. Training loss: 2.9331 0.0151 sec/batch\n",
      "Global Step: 85700 Epoch 39/50 Iteration: 85700 Avg. Training loss: 2.9235 0.0184 sec/batch\n",
      "Global Step: 85800 Epoch 39/50 Iteration: 85800 Avg. Training loss: 2.9209 0.0190 sec/batch\n",
      "Global Step: 85900 Epoch 39/50 Iteration: 85900 Avg. Training loss: 2.9457 0.0188 sec/batch\n",
      "Global Step: 86000 Epoch 39/50 Iteration: 86000 Avg. Training loss: 2.9290 0.0155 sec/batch\n",
      "Global Step: 86100 Epoch 39/50 Iteration: 86100 Avg. Training loss: 2.9267 0.0168 sec/batch\n",
      "Global Step: 86200 Epoch 39/50 Iteration: 86200 Avg. Training loss: 2.9112 0.0191 sec/batch\n",
      "Global Step: 86300 Epoch 39/50 Iteration: 86300 Avg. Training loss: 2.9108 0.0175 sec/batch\n",
      "Global Step: 86400 Epoch 39/50 Iteration: 86400 Avg. Training loss: 2.9256 0.0147 sec/batch\n",
      "Epoch 40/50 Threshold: 0.07592367925337153 Length of Training words: 2359327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 86500 Epoch 40/50 Iteration: 86500 Avg. Training loss: 2.8871 0.0081 sec/batch\n",
      "Global Step: 86600 Epoch 40/50 Iteration: 86600 Avg. Training loss: 2.8778 0.0150 sec/batch\n",
      "Global Step: 86700 Epoch 40/50 Iteration: 86700 Avg. Training loss: 2.8786 0.0176 sec/batch\n",
      "Global Step: 86800 Epoch 40/50 Iteration: 86800 Avg. Training loss: 2.8524 0.0152 sec/batch\n",
      "Global Step: 86900 Epoch 40/50 Iteration: 86900 Avg. Training loss: 2.8522 0.0174 sec/batch\n",
      "Global Step: 87000 Epoch 40/50 Iteration: 87000 Avg. Training loss: 2.8490 0.0190 sec/batch\n",
      "Global Step: 87100 Epoch 40/50 Iteration: 87100 Avg. Training loss: 2.8582 0.0175 sec/batch\n",
      "Global Step: 87200 Epoch 40/50 Iteration: 87200 Avg. Training loss: 2.8645 0.0182 sec/batch\n",
      "Global Step: 87300 Epoch 40/50 Iteration: 87300 Avg. Training loss: 2.8665 0.0132 sec/batch\n",
      "Global Step: 87400 Epoch 40/50 Iteration: 87400 Avg. Training loss: 2.8636 0.0122 sec/batch\n",
      "Global Step: 87500 Epoch 40/50 Iteration: 87500 Avg. Training loss: 2.8403 0.0150 sec/batch\n",
      "Global Step: 87600 Epoch 40/50 Iteration: 87600 Avg. Training loss: 2.8478 0.0161 sec/batch\n",
      "Global Step: 87700 Epoch 40/50 Iteration: 87700 Avg. Training loss: 2.8609 0.0147 sec/batch\n",
      "Global Step: 87800 Epoch 40/50 Iteration: 87800 Avg. Training loss: 2.8620 0.0148 sec/batch\n",
      "Global Step: 87900 Epoch 40/50 Iteration: 87900 Avg. Training loss: 2.8558 0.0159 sec/batch\n",
      "Global Step: 88000 Epoch 40/50 Iteration: 88000 Avg. Training loss: 2.8594 0.0156 sec/batch\n",
      "Global Step: 88100 Epoch 40/50 Iteration: 88100 Avg. Training loss: 2.8409 0.0160 sec/batch\n",
      "Global Step: 88200 Epoch 40/50 Iteration: 88200 Avg. Training loss: 2.8826 0.0153 sec/batch\n",
      "Global Step: 88300 Epoch 40/50 Iteration: 88300 Avg. Training loss: 2.8572 0.0146 sec/batch\n",
      "Global Step: 88400 Epoch 40/50 Iteration: 88400 Avg. Training loss: 2.8743 0.0166 sec/batch\n",
      "Global Step: 88500 Epoch 40/50 Iteration: 88500 Avg. Training loss: 2.8364 0.0173 sec/batch\n",
      "Global Step: 88600 Epoch 40/50 Iteration: 88600 Avg. Training loss: 2.8377 0.0125 sec/batch\n",
      "Global Step: 88700 Epoch 40/50 Iteration: 88700 Avg. Training loss: 2.8509 0.0208 sec/batch\n",
      "Global Step: 88800 Epoch 40/50 Iteration: 88800 Avg. Training loss: 2.8549 0.0157 sec/batch\n",
      "Epoch 41/50 Threshold: 0.06910040164384194 Length of Training words: 2307037\n",
      "Global Step: 88900 Epoch 41/50 Iteration: 88900 Avg. Training loss: 2.8883 0.0141 sec/batch\n",
      "Global Step: 89000 Epoch 41/50 Iteration: 89000 Avg. Training loss: 2.9036 0.0179 sec/batch\n",
      "Global Step: 89100 Epoch 41/50 Iteration: 89100 Avg. Training loss: 2.8809 0.0200 sec/batch\n",
      "Global Step: 89200 Epoch 41/50 Iteration: 89200 Avg. Training loss: 2.8658 0.0159 sec/batch\n",
      "Global Step: 89300 Epoch 41/50 Iteration: 89300 Avg. Training loss: 2.8648 0.0160 sec/batch\n",
      "Global Step: 89400 Epoch 41/50 Iteration: 89400 Avg. Training loss: 2.8670 0.0181 sec/batch\n",
      "Global Step: 89500 Epoch 41/50 Iteration: 89500 Avg. Training loss: 2.8780 0.0178 sec/batch\n",
      "Global Step: 89600 Epoch 41/50 Iteration: 89600 Avg. Training loss: 2.8740 0.0157 sec/batch\n",
      "Global Step: 89700 Epoch 41/50 Iteration: 89700 Avg. Training loss: 2.8951 0.0151 sec/batch\n",
      "Global Step: 89800 Epoch 41/50 Iteration: 89800 Avg. Training loss: 2.8598 0.0188 sec/batch\n",
      "Global Step: 89900 Epoch 41/50 Iteration: 89900 Avg. Training loss: 2.8535 0.0173 sec/batch\n",
      "Global Step: 90000 Epoch 41/50 Iteration: 90000 Avg. Training loss: 2.8899 0.0159 sec/batch\n",
      "Global Step: 90100 Epoch 41/50 Iteration: 90100 Avg. Training loss: 2.8566 0.0154 sec/batch\n",
      "Global Step: 90200 Epoch 41/50 Iteration: 90200 Avg. Training loss: 2.8817 0.0138 sec/batch\n",
      "Global Step: 90300 Epoch 41/50 Iteration: 90300 Avg. Training loss: 2.8738 0.0161 sec/batch\n",
      "Global Step: 90400 Epoch 41/50 Iteration: 90400 Avg. Training loss: 2.8547 0.0190 sec/batch\n",
      "Global Step: 90500 Epoch 41/50 Iteration: 90500 Avg. Training loss: 2.8952 0.0175 sec/batch\n",
      "Global Step: 90600 Epoch 41/50 Iteration: 90600 Avg. Training loss: 2.8735 0.0147 sec/batch\n",
      "Global Step: 90700 Epoch 41/50 Iteration: 90700 Avg. Training loss: 2.8952 0.0170 sec/batch\n",
      "Global Step: 90800 Epoch 41/50 Iteration: 90800 Avg. Training loss: 2.8557 0.0237 sec/batch\n",
      "Global Step: 90900 Epoch 41/50 Iteration: 90900 Avg. Training loss: 2.8510 0.0173 sec/batch\n",
      "Global Step: 91000 Epoch 41/50 Iteration: 91000 Avg. Training loss: 2.8791 0.0150 sec/batch\n",
      "Global Step: 91100 Epoch 41/50 Iteration: 91100 Avg. Training loss: 2.8675 0.0137 sec/batch\n",
      "Epoch 42/50 Threshold: 0.05095992031399647 Length of Training words: 2136391\n",
      "Global Step: 91200 Epoch 42/50 Iteration: 91200 Avg. Training loss: 2.9362 0.0134 sec/batch\n",
      "Global Step: 91300 Epoch 42/50 Iteration: 91300 Avg. Training loss: 2.9526 0.0164 sec/batch\n",
      "Global Step: 91400 Epoch 42/50 Iteration: 91400 Avg. Training loss: 2.9327 0.0180 sec/batch\n",
      "Global Step: 91500 Epoch 42/50 Iteration: 91500 Avg. Training loss: 2.9183 0.0138 sec/batch\n",
      "Global Step: 91600 Epoch 42/50 Iteration: 91600 Avg. Training loss: 2.9092 0.0155 sec/batch\n",
      "Global Step: 91700 Epoch 42/50 Iteration: 91700 Avg. Training loss: 2.9341 0.0195 sec/batch\n",
      "Global Step: 91800 Epoch 42/50 Iteration: 91800 Avg. Training loss: 2.9314 0.0186 sec/batch\n",
      "Global Step: 91900 Epoch 42/50 Iteration: 91900 Avg. Training loss: 2.9314 0.0181 sec/batch\n",
      "Global Step: 92000 Epoch 42/50 Iteration: 92000 Avg. Training loss: 2.9231 0.0147 sec/batch\n",
      "Global Step: 92100 Epoch 42/50 Iteration: 92100 Avg. Training loss: 2.9140 0.0159 sec/batch\n",
      "Global Step: 92200 Epoch 42/50 Iteration: 92200 Avg. Training loss: 2.9304 0.0147 sec/batch\n",
      "Global Step: 92300 Epoch 42/50 Iteration: 92300 Avg. Training loss: 2.9174 0.0159 sec/batch\n",
      "Global Step: 92400 Epoch 42/50 Iteration: 92400 Avg. Training loss: 2.9347 0.0163 sec/batch\n",
      "Global Step: 92500 Epoch 42/50 Iteration: 92500 Avg. Training loss: 2.9230 0.0136 sec/batch\n",
      "Global Step: 92600 Epoch 42/50 Iteration: 92600 Avg. Training loss: 2.9166 0.0129 sec/batch\n",
      "Global Step: 92700 Epoch 42/50 Iteration: 92700 Avg. Training loss: 2.9462 0.0160 sec/batch\n",
      "Global Step: 92800 Epoch 42/50 Iteration: 92800 Avg. Training loss: 2.9307 0.0169 sec/batch\n",
      "Global Step: 92900 Epoch 42/50 Iteration: 92900 Avg. Training loss: 2.9284 0.0156 sec/batch\n",
      "Global Step: 93000 Epoch 42/50 Iteration: 93000 Avg. Training loss: 2.9078 0.0167 sec/batch\n",
      "Global Step: 93100 Epoch 42/50 Iteration: 93100 Avg. Training loss: 2.9132 0.0147 sec/batch\n",
      "Global Step: 93200 Epoch 42/50 Iteration: 93200 Avg. Training loss: 2.9233 0.0166 sec/batch\n",
      "Epoch 43/50 Threshold: 0.042015604209730634 Length of Training words: 2021045\n",
      "Global Step: 93300 Epoch 43/50 Iteration: 93300 Avg. Training loss: 2.9496 0.0099 sec/batch\n",
      "Global Step: 93400 Epoch 43/50 Iteration: 93400 Avg. Training loss: 2.9815 0.0158 sec/batch\n",
      "Global Step: 93500 Epoch 43/50 Iteration: 93500 Avg. Training loss: 2.9754 0.0130 sec/batch\n",
      "Global Step: 93600 Epoch 43/50 Iteration: 93600 Avg. Training loss: 2.9472 0.0126 sec/batch\n",
      "Global Step: 93700 Epoch 43/50 Iteration: 93700 Avg. Training loss: 2.9516 0.0142 sec/batch\n",
      "Global Step: 93800 Epoch 43/50 Iteration: 93800 Avg. Training loss: 2.9620 0.0181 sec/batch\n",
      "Global Step: 93900 Epoch 43/50 Iteration: 93900 Avg. Training loss: 2.9645 0.0161 sec/batch\n",
      "Global Step: 94000 Epoch 43/50 Iteration: 94000 Avg. Training loss: 2.9684 0.0146 sec/batch\n",
      "Global Step: 94100 Epoch 43/50 Iteration: 94100 Avg. Training loss: 2.9508 0.0169 sec/batch\n",
      "Global Step: 94200 Epoch 43/50 Iteration: 94200 Avg. Training loss: 2.9456 0.0144 sec/batch\n",
      "Global Step: 94300 Epoch 43/50 Iteration: 94300 Avg. Training loss: 2.9737 0.0141 sec/batch\n",
      "Global Step: 94400 Epoch 43/50 Iteration: 94400 Avg. Training loss: 2.9552 0.0169 sec/batch\n",
      "Global Step: 94500 Epoch 43/50 Iteration: 94500 Avg. Training loss: 2.9561 0.0183 sec/batch\n",
      "Global Step: 94600 Epoch 43/50 Iteration: 94600 Avg. Training loss: 2.9604 0.0164 sec/batch\n",
      "Global Step: 94700 Epoch 43/50 Iteration: 94700 Avg. Training loss: 2.9604 0.0174 sec/batch\n",
      "Global Step: 94800 Epoch 43/50 Iteration: 94800 Avg. Training loss: 2.9643 0.0175 sec/batch\n",
      "Global Step: 94900 Epoch 43/50 Iteration: 94900 Avg. Training loss: 2.9801 0.0182 sec/batch\n",
      "Global Step: 95000 Epoch 43/50 Iteration: 95000 Avg. Training loss: 2.9427 0.0163 sec/batch\n",
      "Global Step: 95100 Epoch 43/50 Iteration: 95100 Avg. Training loss: 2.9462 0.0188 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 95200 Epoch 43/50 Iteration: 95200 Avg. Training loss: 2.9554 0.0157 sec/batch\n",
      "Epoch 44/50 Threshold: 0.056769036723964564 Length of Training words: 2200338\n",
      "Global Step: 95300 Epoch 44/50 Iteration: 95300 Avg. Training loss: 2.9410 0.0061 sec/batch\n",
      "Global Step: 95400 Epoch 44/50 Iteration: 95400 Avg. Training loss: 2.9300 0.0152 sec/batch\n",
      "Global Step: 95500 Epoch 44/50 Iteration: 95500 Avg. Training loss: 2.9243 0.0140 sec/batch\n",
      "Global Step: 95600 Epoch 44/50 Iteration: 95600 Avg. Training loss: 2.9084 0.0164 sec/batch\n",
      "Global Step: 95700 Epoch 44/50 Iteration: 95700 Avg. Training loss: 2.9042 0.0118 sec/batch\n",
      "Global Step: 95800 Epoch 44/50 Iteration: 95800 Avg. Training loss: 2.8991 0.0131 sec/batch\n",
      "Global Step: 95900 Epoch 44/50 Iteration: 95900 Avg. Training loss: 2.9054 0.0157 sec/batch\n",
      "Global Step: 96000 Epoch 44/50 Iteration: 96000 Avg. Training loss: 2.9253 0.0153 sec/batch\n",
      "Global Step: 96100 Epoch 44/50 Iteration: 96100 Avg. Training loss: 2.9137 0.0162 sec/batch\n",
      "Global Step: 96200 Epoch 44/50 Iteration: 96200 Avg. Training loss: 2.8939 0.0167 sec/batch\n",
      "Global Step: 96300 Epoch 44/50 Iteration: 96300 Avg. Training loss: 2.8922 0.0153 sec/batch\n",
      "Global Step: 96400 Epoch 44/50 Iteration: 96400 Avg. Training loss: 2.9181 0.0141 sec/batch\n",
      "Global Step: 96500 Epoch 44/50 Iteration: 96500 Avg. Training loss: 2.8948 0.0149 sec/batch\n",
      "Global Step: 96600 Epoch 44/50 Iteration: 96600 Avg. Training loss: 2.9138 0.0171 sec/batch\n",
      "Global Step: 96700 Epoch 44/50 Iteration: 96700 Avg. Training loss: 2.9025 0.0161 sec/batch\n",
      "Global Step: 96800 Epoch 44/50 Iteration: 96800 Avg. Training loss: 2.8985 0.0177 sec/batch\n",
      "Global Step: 96900 Epoch 44/50 Iteration: 96900 Avg. Training loss: 2.9281 0.0203 sec/batch\n",
      "Global Step: 97000 Epoch 44/50 Iteration: 97000 Avg. Training loss: 2.9061 0.0173 sec/batch\n",
      "Global Step: 97100 Epoch 44/50 Iteration: 97100 Avg. Training loss: 2.9227 0.0152 sec/batch\n",
      "Global Step: 97200 Epoch 44/50 Iteration: 97200 Avg. Training loss: 2.8821 0.0161 sec/batch\n",
      "Global Step: 97300 Epoch 44/50 Iteration: 97300 Avg. Training loss: 2.8944 0.0180 sec/batch\n",
      "Global Step: 97400 Epoch 44/50 Iteration: 97400 Avg. Training loss: 2.9010 0.0201 sec/batch\n",
      "Epoch 45/50 Threshold: 0.06639336610770162 Length of Training words: 2283788\n",
      "Global Step: 97500 Epoch 45/50 Iteration: 97500 Avg. Training loss: 2.9011 0.0060 sec/batch\n",
      "Global Step: 97600 Epoch 45/50 Iteration: 97600 Avg. Training loss: 2.8980 0.0180 sec/batch\n",
      "Global Step: 97700 Epoch 45/50 Iteration: 97700 Avg. Training loss: 2.9081 0.0186 sec/batch\n",
      "Global Step: 97800 Epoch 45/50 Iteration: 97800 Avg. Training loss: 2.8843 0.0138 sec/batch\n",
      "Global Step: 97900 Epoch 45/50 Iteration: 97900 Avg. Training loss: 2.8716 0.0190 sec/batch\n",
      "Global Step: 98000 Epoch 45/50 Iteration: 98000 Avg. Training loss: 2.8679 0.0131 sec/batch\n",
      "Global Step: 98100 Epoch 45/50 Iteration: 98100 Avg. Training loss: 2.8828 0.0184 sec/batch\n",
      "Global Step: 98200 Epoch 45/50 Iteration: 98200 Avg. Training loss: 2.8856 0.0163 sec/batch\n",
      "Global Step: 98300 Epoch 45/50 Iteration: 98300 Avg. Training loss: 2.8876 0.0144 sec/batch\n",
      "Global Step: 98400 Epoch 45/50 Iteration: 98400 Avg. Training loss: 2.8844 0.0150 sec/batch\n",
      "Global Step: 98500 Epoch 45/50 Iteration: 98500 Avg. Training loss: 2.8615 0.0128 sec/batch\n",
      "Global Step: 98600 Epoch 45/50 Iteration: 98600 Avg. Training loss: 2.8838 0.0142 sec/batch\n",
      "Global Step: 98700 Epoch 45/50 Iteration: 98700 Avg. Training loss: 2.8798 0.0158 sec/batch\n",
      "Global Step: 98800 Epoch 45/50 Iteration: 98800 Avg. Training loss: 2.8866 0.0143 sec/batch\n",
      "Global Step: 98900 Epoch 45/50 Iteration: 98900 Avg. Training loss: 2.8781 0.0142 sec/batch\n",
      "Global Step: 99000 Epoch 45/50 Iteration: 99000 Avg. Training loss: 2.8820 0.0149 sec/batch\n",
      "Global Step: 99100 Epoch 45/50 Iteration: 99100 Avg. Training loss: 2.8790 0.0175 sec/batch\n",
      "Global Step: 99200 Epoch 45/50 Iteration: 99200 Avg. Training loss: 2.8989 0.0208 sec/batch\n",
      "Global Step: 99300 Epoch 45/50 Iteration: 99300 Avg. Training loss: 2.8889 0.0142 sec/batch\n",
      "Global Step: 99400 Epoch 45/50 Iteration: 99400 Avg. Training loss: 2.8868 0.0150 sec/batch\n",
      "Global Step: 99500 Epoch 45/50 Iteration: 99500 Avg. Training loss: 2.8580 0.0179 sec/batch\n",
      "Global Step: 99600 Epoch 45/50 Iteration: 99600 Avg. Training loss: 2.8672 0.0151 sec/batch\n",
      "Global Step: 99700 Epoch 45/50 Iteration: 99700 Avg. Training loss: 2.8773 0.0175 sec/batch\n",
      "Epoch 46/50 Threshold: 0.04233263837108756 Length of Training words: 2025218\n",
      "Global Step: 99800 Epoch 46/50 Iteration: 99800 Avg. Training loss: 2.9222 0.0074 sec/batch\n",
      "Global Step: 99900 Epoch 46/50 Iteration: 99900 Avg. Training loss: 2.9809 0.0184 sec/batch\n",
      "Global Step: 100000 Epoch 46/50 Iteration: 100000 Avg. Training loss: 2.9752 0.0158 sec/batch\n",
      "Global Step: 100100 Epoch 46/50 Iteration: 100100 Avg. Training loss: 2.9468 0.0174 sec/batch\n",
      "Global Step: 100200 Epoch 46/50 Iteration: 100200 Avg. Training loss: 2.9526 0.0155 sec/batch\n",
      "Global Step: 100300 Epoch 46/50 Iteration: 100300 Avg. Training loss: 2.9626 0.0185 sec/batch\n",
      "Global Step: 100400 Epoch 46/50 Iteration: 100400 Avg. Training loss: 2.9579 0.0174 sec/batch\n",
      "Global Step: 100500 Epoch 46/50 Iteration: 100500 Avg. Training loss: 2.9654 0.0190 sec/batch\n",
      "Global Step: 100600 Epoch 46/50 Iteration: 100600 Avg. Training loss: 2.9537 0.0164 sec/batch\n",
      "Global Step: 100700 Epoch 46/50 Iteration: 100700 Avg. Training loss: 2.9454 0.0137 sec/batch\n",
      "Global Step: 100800 Epoch 46/50 Iteration: 100800 Avg. Training loss: 2.9750 0.0187 sec/batch\n",
      "Global Step: 100900 Epoch 46/50 Iteration: 100900 Avg. Training loss: 2.9484 0.0156 sec/batch\n",
      "Global Step: 101000 Epoch 46/50 Iteration: 101000 Avg. Training loss: 2.9589 0.0164 sec/batch\n",
      "Global Step: 101100 Epoch 46/50 Iteration: 101100 Avg. Training loss: 2.9577 0.0133 sec/batch\n",
      "Global Step: 101200 Epoch 46/50 Iteration: 101200 Avg. Training loss: 2.9565 0.0141 sec/batch\n",
      "Global Step: 101300 Epoch 46/50 Iteration: 101300 Avg. Training loss: 2.9650 0.0150 sec/batch\n",
      "Global Step: 101400 Epoch 46/50 Iteration: 101400 Avg. Training loss: 2.9772 0.0175 sec/batch\n",
      "Global Step: 101500 Epoch 46/50 Iteration: 101500 Avg. Training loss: 2.9468 0.0148 sec/batch\n",
      "Global Step: 101600 Epoch 46/50 Iteration: 101600 Avg. Training loss: 2.9369 0.0161 sec/batch\n",
      "Global Step: 101700 Epoch 46/50 Iteration: 101700 Avg. Training loss: 2.9549 0.0166 sec/batch\n",
      "Epoch 47/50 Threshold: 0.0605734780744902 Length of Training words: 2234981\n",
      "Global Step: 101800 Epoch 47/50 Iteration: 101800 Avg. Training loss: 2.9442 0.0044 sec/batch\n",
      "Global Step: 101900 Epoch 47/50 Iteration: 101900 Avg. Training loss: 2.9074 0.0147 sec/batch\n",
      "Global Step: 102000 Epoch 47/50 Iteration: 102000 Avg. Training loss: 2.9274 0.0175 sec/batch\n",
      "Global Step: 102100 Epoch 47/50 Iteration: 102100 Avg. Training loss: 2.9023 0.0167 sec/batch\n",
      "Global Step: 102200 Epoch 47/50 Iteration: 102200 Avg. Training loss: 2.8877 0.0171 sec/batch\n",
      "Global Step: 102300 Epoch 47/50 Iteration: 102300 Avg. Training loss: 2.8880 0.0170 sec/batch\n",
      "Global Step: 102400 Epoch 47/50 Iteration: 102400 Avg. Training loss: 2.8968 0.0168 sec/batch\n",
      "Global Step: 102500 Epoch 47/50 Iteration: 102500 Avg. Training loss: 2.9014 0.0161 sec/batch\n",
      "Global Step: 102600 Epoch 47/50 Iteration: 102600 Avg. Training loss: 2.8992 0.0167 sec/batch\n",
      "Global Step: 102700 Epoch 47/50 Iteration: 102700 Avg. Training loss: 2.8965 0.0149 sec/batch\n",
      "Global Step: 102800 Epoch 47/50 Iteration: 102800 Avg. Training loss: 2.8848 0.0160 sec/batch\n",
      "Global Step: 102900 Epoch 47/50 Iteration: 102900 Avg. Training loss: 2.8953 0.0177 sec/batch\n",
      "Global Step: 103000 Epoch 47/50 Iteration: 103000 Avg. Training loss: 2.8922 0.0168 sec/batch\n",
      "Global Step: 103100 Epoch 47/50 Iteration: 103100 Avg. Training loss: 2.8915 0.0174 sec/batch\n",
      "Global Step: 103200 Epoch 47/50 Iteration: 103200 Avg. Training loss: 2.9037 0.0175 sec/batch\n",
      "Global Step: 103300 Epoch 47/50 Iteration: 103300 Avg. Training loss: 2.8773 0.0176 sec/batch\n",
      "Global Step: 103400 Epoch 47/50 Iteration: 103400 Avg. Training loss: 2.9200 0.0191 sec/batch\n",
      "Global Step: 103500 Epoch 47/50 Iteration: 103500 Avg. Training loss: 2.8957 0.0172 sec/batch\n",
      "Global Step: 103600 Epoch 47/50 Iteration: 103600 Avg. Training loss: 2.9142 0.0192 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 103700 Epoch 47/50 Iteration: 103700 Avg. Training loss: 2.8851 0.0164 sec/batch\n",
      "Global Step: 103800 Epoch 47/50 Iteration: 103800 Avg. Training loss: 2.8676 0.0120 sec/batch\n",
      "Global Step: 103900 Epoch 47/50 Iteration: 103900 Avg. Training loss: 2.9020 0.0144 sec/batch\n",
      "Global Step: 104000 Epoch 47/50 Iteration: 104000 Avg. Training loss: 2.8890 0.0187 sec/batch\n",
      "Epoch 48/50 Threshold: 0.0788562008580151 Length of Training words: 2378420\n",
      "Global Step: 104100 Epoch 48/50 Iteration: 104100 Avg. Training loss: 2.8561 0.0136 sec/batch\n",
      "Global Step: 104200 Epoch 48/50 Iteration: 104200 Avg. Training loss: 2.8902 0.0129 sec/batch\n",
      "Global Step: 104300 Epoch 48/50 Iteration: 104300 Avg. Training loss: 2.8611 0.0137 sec/batch\n",
      "Global Step: 104400 Epoch 48/50 Iteration: 104400 Avg. Training loss: 2.8396 0.0151 sec/batch\n",
      "Global Step: 104500 Epoch 48/50 Iteration: 104500 Avg. Training loss: 2.8450 0.0130 sec/batch\n",
      "Global Step: 104600 Epoch 48/50 Iteration: 104600 Avg. Training loss: 2.8516 0.0127 sec/batch\n",
      "Global Step: 104700 Epoch 48/50 Iteration: 104700 Avg. Training loss: 2.8419 0.0158 sec/batch\n",
      "Global Step: 104800 Epoch 48/50 Iteration: 104800 Avg. Training loss: 2.8733 0.0166 sec/batch\n",
      "Global Step: 104900 Epoch 48/50 Iteration: 104900 Avg. Training loss: 2.8535 0.0157 sec/batch\n",
      "Global Step: 105000 Epoch 48/50 Iteration: 105000 Avg. Training loss: 2.8433 0.0192 sec/batch\n",
      "Global Step: 105100 Epoch 48/50 Iteration: 105100 Avg. Training loss: 2.8440 0.0165 sec/batch\n",
      "Global Step: 105200 Epoch 48/50 Iteration: 105200 Avg. Training loss: 2.8539 0.0151 sec/batch\n",
      "Global Step: 105300 Epoch 48/50 Iteration: 105300 Avg. Training loss: 2.8455 0.0147 sec/batch\n",
      "Global Step: 105400 Epoch 48/50 Iteration: 105400 Avg. Training loss: 2.8485 0.0167 sec/batch\n",
      "Global Step: 105500 Epoch 48/50 Iteration: 105500 Avg. Training loss: 2.8485 0.0164 sec/batch\n",
      "Global Step: 105600 Epoch 48/50 Iteration: 105600 Avg. Training loss: 2.8538 0.0151 sec/batch\n",
      "Global Step: 105700 Epoch 48/50 Iteration: 105700 Avg. Training loss: 2.8509 0.0149 sec/batch\n",
      "Global Step: 105800 Epoch 48/50 Iteration: 105800 Avg. Training loss: 2.8701 0.0177 sec/batch\n",
      "Global Step: 105900 Epoch 48/50 Iteration: 105900 Avg. Training loss: 2.8503 0.0142 sec/batch\n",
      "Global Step: 106000 Epoch 48/50 Iteration: 106000 Avg. Training loss: 2.8595 0.0169 sec/batch\n",
      "Global Step: 106100 Epoch 48/50 Iteration: 106100 Avg. Training loss: 2.8240 0.0170 sec/batch\n",
      "Global Step: 106200 Epoch 48/50 Iteration: 106200 Avg. Training loss: 2.8390 0.0136 sec/batch\n",
      "Global Step: 106300 Epoch 48/50 Iteration: 106300 Avg. Training loss: 2.8446 0.0152 sec/batch\n",
      "Epoch 49/50 Threshold: 0.05288937341093598 Length of Training words: 2159146\n",
      "Global Step: 106400 Epoch 49/50 Iteration: 106400 Avg. Training loss: 2.8571 0.0027 sec/batch\n",
      "Global Step: 106500 Epoch 49/50 Iteration: 106500 Avg. Training loss: 2.9332 0.0161 sec/batch\n",
      "Global Step: 106600 Epoch 49/50 Iteration: 106600 Avg. Training loss: 2.9524 0.0153 sec/batch\n",
      "Global Step: 106700 Epoch 49/50 Iteration: 106700 Avg. Training loss: 2.9214 0.0170 sec/batch\n",
      "Global Step: 106800 Epoch 49/50 Iteration: 106800 Avg. Training loss: 2.9101 0.0155 sec/batch\n",
      "Global Step: 106900 Epoch 49/50 Iteration: 106900 Avg. Training loss: 2.9097 0.0125 sec/batch\n",
      "Global Step: 107000 Epoch 49/50 Iteration: 107000 Avg. Training loss: 2.9205 0.0123 sec/batch\n",
      "Global Step: 107100 Epoch 49/50 Iteration: 107100 Avg. Training loss: 2.9348 0.0218 sec/batch\n",
      "Global Step: 107200 Epoch 49/50 Iteration: 107200 Avg. Training loss: 2.9213 0.0150 sec/batch\n",
      "Global Step: 107300 Epoch 49/50 Iteration: 107300 Avg. Training loss: 2.9116 0.0152 sec/batch\n",
      "Global Step: 107400 Epoch 49/50 Iteration: 107400 Avg. Training loss: 2.9038 0.0162 sec/batch\n",
      "Global Step: 107500 Epoch 49/50 Iteration: 107500 Avg. Training loss: 2.9321 0.0141 sec/batch\n",
      "Global Step: 107600 Epoch 49/50 Iteration: 107600 Avg. Training loss: 2.9064 0.0176 sec/batch\n",
      "Global Step: 107700 Epoch 49/50 Iteration: 107700 Avg. Training loss: 2.9306 0.0175 sec/batch\n",
      "Global Step: 107800 Epoch 49/50 Iteration: 107800 Avg. Training loss: 2.9138 0.0185 sec/batch\n",
      "Global Step: 107900 Epoch 49/50 Iteration: 107900 Avg. Training loss: 2.9094 0.0179 sec/batch\n",
      "Global Step: 108000 Epoch 49/50 Iteration: 108000 Avg. Training loss: 2.9424 0.0173 sec/batch\n",
      "Global Step: 108100 Epoch 49/50 Iteration: 108100 Avg. Training loss: 2.9233 0.0178 sec/batch\n",
      "Global Step: 108200 Epoch 49/50 Iteration: 108200 Avg. Training loss: 2.9240 0.0166 sec/batch\n",
      "Global Step: 108300 Epoch 49/50 Iteration: 108300 Avg. Training loss: 2.9039 0.0148 sec/batch\n",
      "Global Step: 108400 Epoch 49/50 Iteration: 108400 Avg. Training loss: 2.9062 0.0140 sec/batch\n",
      "Global Step: 108500 Epoch 49/50 Iteration: 108500 Avg. Training loss: 2.9208 0.0159 sec/batch\n",
      "Epoch 50/50 Threshold: 0.06511612652299792 Length of Training words: 2273119\n",
      "Global Step: 108600 Epoch 50/50 Iteration: 108600 Avg. Training loss: 2.9009 0.0074 sec/batch\n",
      "Global Step: 108700 Epoch 50/50 Iteration: 108700 Avg. Training loss: 2.9055 0.0167 sec/batch\n",
      "Global Step: 108800 Epoch 50/50 Iteration: 108800 Avg. Training loss: 2.9052 0.0155 sec/batch\n",
      "Global Step: 108900 Epoch 50/50 Iteration: 108900 Avg. Training loss: 2.8766 0.0158 sec/batch\n",
      "Global Step: 109000 Epoch 50/50 Iteration: 109000 Avg. Training loss: 2.8811 0.0179 sec/batch\n",
      "Global Step: 109100 Epoch 50/50 Iteration: 109100 Avg. Training loss: 2.8757 0.0141 sec/batch\n",
      "Global Step: 109200 Epoch 50/50 Iteration: 109200 Avg. Training loss: 2.8822 0.0139 sec/batch\n",
      "Global Step: 109300 Epoch 50/50 Iteration: 109300 Avg. Training loss: 2.9018 0.0166 sec/batch\n",
      "Global Step: 109400 Epoch 50/50 Iteration: 109400 Avg. Training loss: 2.8870 0.0164 sec/batch\n",
      "Global Step: 109500 Epoch 50/50 Iteration: 109500 Avg. Training loss: 2.8758 0.0171 sec/batch\n",
      "Global Step: 109600 Epoch 50/50 Iteration: 109600 Avg. Training loss: 2.8730 0.0168 sec/batch\n",
      "Global Step: 109700 Epoch 50/50 Iteration: 109700 Avg. Training loss: 2.8865 0.0162 sec/batch\n",
      "Global Step: 109800 Epoch 50/50 Iteration: 109800 Avg. Training loss: 2.8786 0.0154 sec/batch\n",
      "Global Step: 109900 Epoch 50/50 Iteration: 109900 Avg. Training loss: 2.8804 0.0162 sec/batch\n",
      "Global Step: 110000 Epoch 50/50 Iteration: 110000 Avg. Training loss: 2.8924 0.0163 sec/batch\n",
      "Global Step: 110100 Epoch 50/50 Iteration: 110100 Avg. Training loss: 2.8652 0.0158 sec/batch\n",
      "Global Step: 110200 Epoch 50/50 Iteration: 110200 Avg. Training loss: 2.9076 0.0163 sec/batch\n",
      "Global Step: 110300 Epoch 50/50 Iteration: 110300 Avg. Training loss: 2.8849 0.0141 sec/batch\n",
      "Global Step: 110400 Epoch 50/50 Iteration: 110400 Avg. Training loss: 2.9043 0.0154 sec/batch\n",
      "Global Step: 110500 Epoch 50/50 Iteration: 110500 Avg. Training loss: 2.8751 0.0152 sec/batch\n",
      "Global Step: 110600 Epoch 50/50 Iteration: 110600 Avg. Training loss: 2.8566 0.0160 sec/batch\n",
      "Global Step: 110700 Epoch 50/50 Iteration: 110700 Avg. Training loss: 2.8881 0.0187 sec/batch\n",
      "Global Step: 110800 Epoch 50/50 Iteration: 110800 Avg. Training loss: 2.8785 0.0133 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/pos/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/pos4/pos.ckpt\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos4'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB14AAAcMCAYAAAAHCCGfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAADQleAQAAAAAAABr6/0rmn1co0AxTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2af6014f98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 902,
       "width": 943
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
