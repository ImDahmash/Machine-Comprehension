{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.06, 0.09)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/pos’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Threshold: 0.08708670843710356 Length of Training words: 2429632\n",
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 6.8498 0.0283 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 4.8515 0.0158 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 3.8954 0.0156 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 3.4002 0.0161 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 3.1780 0.0152 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 3.0604 0.0136 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 2.9929 0.0124 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 2.9768 0.0178 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 2.9223 0.0158 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 2.8990 0.0155 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 2.8639 0.0148 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 2.8691 0.0139 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 2.8690 0.0154 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 2.8716 0.0153 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 2.8565 0.0152 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 2.8583 0.0159 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 2.8374 0.0144 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 2.8769 0.0162 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 2.8491 0.0173 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 2.8620 0.0167 sec/batch\n",
      "Global Step: 2100 Epoch 1/50 Iteration: 2100 Avg. Training loss: 2.8236 0.0151 sec/batch\n",
      "Global Step: 2200 Epoch 1/50 Iteration: 2200 Avg. Training loss: 2.8194 0.0168 sec/batch\n",
      "Global Step: 2300 Epoch 1/50 Iteration: 2300 Avg. Training loss: 2.8519 0.0148 sec/batch\n",
      "Global Step: 2400 Epoch 1/50 Iteration: 2400 Avg. Training loss: 2.8353 0.0141 sec/batch\n",
      "Epoch 2/50 Threshold: 0.06967566519489501 Length of Training words: 2311639\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 2.8783 0.0118 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 2.9035 0.0156 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 2.9013 0.0142 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 2.8740 0.0157 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 2.8744 0.0150 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 2.8715 0.0170 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 2.8728 0.0162 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 2.8967 0.0147 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 2.8784 0.0164 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 2.8672 0.0172 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 2.8673 0.0161 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 2.8763 0.0160 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 2.8746 0.0151 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 2.8721 0.0163 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 2.8834 0.0161 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 2.8555 0.0147 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 2.8963 0.0148 sec/batch\n",
      "Global Step: 4200 Epoch 2/50 Iteration: 4200 Avg. Training loss: 2.8781 0.0137 sec/batch\n",
      "Global Step: 4300 Epoch 2/50 Iteration: 4300 Avg. Training loss: 2.8910 0.0157 sec/batch\n",
      "Global Step: 4400 Epoch 2/50 Iteration: 4400 Avg. Training loss: 2.8711 0.0152 sec/batch\n",
      "Global Step: 4500 Epoch 2/50 Iteration: 4500 Avg. Training loss: 2.8581 0.0151 sec/batch\n",
      "Global Step: 4600 Epoch 2/50 Iteration: 4600 Avg. Training loss: 2.8675 0.0169 sec/batch\n",
      "Global Step: 4700 Epoch 2/50 Iteration: 4700 Avg. Training loss: 2.8752 0.0175 sec/batch\n",
      "Epoch 3/50 Threshold: 0.08060656649666245 Length of Training words: 2388627\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 2.8617 0.0085 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 2.8708 0.0134 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 2.8717 0.0140 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 2.8455 0.0136 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 2.8450 0.0147 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 2.8402 0.0151 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 2.8491 0.0145 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 2.8555 0.0156 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 2.8543 0.0142 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 2.8610 0.0135 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 2.8374 0.0157 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 2.8408 0.0182 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 2.8511 0.0186 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 2.8511 0.0152 sec/batch\n",
      "Global Step: 6200 Epoch 3/50 Iteration: 6200 Avg. Training loss: 2.8521 0.0143 sec/batch\n",
      "Global Step: 6300 Epoch 3/50 Iteration: 6300 Avg. Training loss: 2.8438 0.0152 sec/batch\n",
      "Global Step: 6400 Epoch 3/50 Iteration: 6400 Avg. Training loss: 2.8316 0.0167 sec/batch\n",
      "Global Step: 6500 Epoch 3/50 Iteration: 6500 Avg. Training loss: 2.8734 0.0152 sec/batch\n",
      "Global Step: 6600 Epoch 3/50 Iteration: 6600 Avg. Training loss: 2.8546 0.0148 sec/batch\n",
      "Global Step: 6700 Epoch 3/50 Iteration: 6700 Avg. Training loss: 2.8733 0.0171 sec/batch\n",
      "Global Step: 6800 Epoch 3/50 Iteration: 6800 Avg. Training loss: 2.8315 0.0166 sec/batch\n",
      "Global Step: 6900 Epoch 3/50 Iteration: 6900 Avg. Training loss: 2.8233 0.0167 sec/batch\n",
      "Global Step: 7000 Epoch 3/50 Iteration: 7000 Avg. Training loss: 2.8571 0.0148 sec/batch\n",
      "Global Step: 7100 Epoch 3/50 Iteration: 7100 Avg. Training loss: 2.8430 0.0178 sec/batch\n",
      "Epoch 4/50 Threshold: 0.0859893440396867 Length of Training words: 2421835\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 2.8485 0.0097 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 2.8605 0.0137 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 2.8588 0.0150 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 2.8267 0.0182 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 2.8364 0.0151 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 2.8293 0.0154 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 2.8401 0.0170 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 2.8421 0.0135 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 2.8450 0.0133 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 2.8486 0.0130 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 2.8212 0.0177 sec/batch\n",
      "Global Step: 8300 Epoch 4/50 Iteration: 8300 Avg. Training loss: 2.8276 0.0144 sec/batch\n",
      "Global Step: 8400 Epoch 4/50 Iteration: 8400 Avg. Training loss: 2.8445 0.0146 sec/batch\n",
      "Global Step: 8500 Epoch 4/50 Iteration: 8500 Avg. Training loss: 2.8315 0.0155 sec/batch\n",
      "Global Step: 8600 Epoch 4/50 Iteration: 8600 Avg. Training loss: 2.8451 0.0154 sec/batch\n",
      "Global Step: 8700 Epoch 4/50 Iteration: 8700 Avg. Training loss: 2.8367 0.0142 sec/batch\n",
      "Global Step: 8800 Epoch 4/50 Iteration: 8800 Avg. Training loss: 2.8189 0.0149 sec/batch\n",
      "Global Step: 8900 Epoch 4/50 Iteration: 8900 Avg. Training loss: 2.8636 0.0132 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9000 Epoch 4/50 Iteration: 9000 Avg. Training loss: 2.8367 0.0152 sec/batch\n",
      "Global Step: 9100 Epoch 4/50 Iteration: 9100 Avg. Training loss: 2.8509 0.0162 sec/batch\n",
      "Global Step: 9200 Epoch 4/50 Iteration: 9200 Avg. Training loss: 2.8298 0.0165 sec/batch\n",
      "Global Step: 9300 Epoch 4/50 Iteration: 9300 Avg. Training loss: 2.8201 0.0155 sec/batch\n",
      "Global Step: 9400 Epoch 4/50 Iteration: 9400 Avg. Training loss: 2.8320 0.0141 sec/batch\n",
      "Global Step: 9500 Epoch 4/50 Iteration: 9500 Avg. Training loss: 2.8372 0.0174 sec/batch\n",
      "Epoch 5/50 Threshold: 0.0848752245043963 Length of Training words: 2415331\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 2.8358 0.0085 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 2.8597 0.0174 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 2.8593 0.0166 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 2.8449 0.0134 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 2.8310 0.0141 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 2.8239 0.0136 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 2.8519 0.0145 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 10400 Epoch 5/50 Iteration: 10400 Avg. Training loss: 2.8355 0.0140 sec/batch\n",
      "Global Step: 10500 Epoch 5/50 Iteration: 10500 Avg. Training loss: 2.8666 0.0180 sec/batch\n",
      "Global Step: 10600 Epoch 5/50 Iteration: 10600 Avg. Training loss: 2.8232 0.0172 sec/batch\n",
      "Global Step: 10700 Epoch 5/50 Iteration: 10700 Avg. Training loss: 2.8223 0.0182 sec/batch\n",
      "Global Step: 10800 Epoch 5/50 Iteration: 10800 Avg. Training loss: 2.8563 0.0133 sec/batch\n",
      "Global Step: 10900 Epoch 5/50 Iteration: 10900 Avg. Training loss: 2.8248 0.0176 sec/batch\n",
      "Global Step: 11000 Epoch 5/50 Iteration: 11000 Avg. Training loss: 2.8444 0.0150 sec/batch\n",
      "Global Step: 11100 Epoch 5/50 Iteration: 11100 Avg. Training loss: 2.8403 0.0157 sec/batch\n",
      "Global Step: 11200 Epoch 5/50 Iteration: 11200 Avg. Training loss: 2.8208 0.0142 sec/batch\n",
      "Global Step: 11300 Epoch 5/50 Iteration: 11300 Avg. Training loss: 2.8621 0.0165 sec/batch\n",
      "Global Step: 11400 Epoch 5/50 Iteration: 11400 Avg. Training loss: 2.8416 0.0128 sec/batch\n",
      "Global Step: 11500 Epoch 5/50 Iteration: 11500 Avg. Training loss: 2.8561 0.0143 sec/batch\n",
      "Global Step: 11600 Epoch 5/50 Iteration: 11600 Avg. Training loss: 2.8463 0.0141 sec/batch\n",
      "Global Step: 11700 Epoch 5/50 Iteration: 11700 Avg. Training loss: 2.8133 0.0172 sec/batch\n",
      "Global Step: 11800 Epoch 5/50 Iteration: 11800 Avg. Training loss: 2.8268 0.0172 sec/batch\n",
      "Global Step: 11900 Epoch 5/50 Iteration: 11900 Avg. Training loss: 2.8307 0.0157 sec/batch\n",
      "Epoch 6/50 Threshold: 0.06057974082760326 Length of Training words: 2234998\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 2.8612 0.0057 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 2.9153 0.0152 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 2.9147 0.0160 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 2.8999 0.0145 sec/batch\n",
      "Global Step: 12400 Epoch 6/50 Iteration: 12400 Avg. Training loss: 2.8952 0.0145 sec/batch\n",
      "Global Step: 12500 Epoch 6/50 Iteration: 12500 Avg. Training loss: 2.8881 0.0160 sec/batch\n",
      "Global Step: 12600 Epoch 6/50 Iteration: 12600 Avg. Training loss: 2.8990 0.0178 sec/batch\n",
      "Global Step: 12700 Epoch 6/50 Iteration: 12700 Avg. Training loss: 2.9125 0.0155 sec/batch\n",
      "Global Step: 12800 Epoch 6/50 Iteration: 12800 Avg. Training loss: 2.8985 0.0142 sec/batch\n",
      "Global Step: 12900 Epoch 6/50 Iteration: 12900 Avg. Training loss: 2.8875 0.0160 sec/batch\n",
      "Global Step: 13000 Epoch 6/50 Iteration: 13000 Avg. Training loss: 2.8875 0.0151 sec/batch\n",
      "Global Step: 13100 Epoch 6/50 Iteration: 13100 Avg. Training loss: 2.9001 0.0158 sec/batch\n",
      "Global Step: 13200 Epoch 6/50 Iteration: 13200 Avg. Training loss: 2.8940 0.0142 sec/batch\n",
      "Global Step: 13300 Epoch 6/50 Iteration: 13300 Avg. Training loss: 2.8987 0.0155 sec/batch\n",
      "Global Step: 13400 Epoch 6/50 Iteration: 13400 Avg. Training loss: 2.8988 0.0147 sec/batch\n",
      "Global Step: 13500 Epoch 6/50 Iteration: 13500 Avg. Training loss: 2.8796 0.0146 sec/batch\n",
      "Global Step: 13600 Epoch 6/50 Iteration: 13600 Avg. Training loss: 2.9229 0.0173 sec/batch\n",
      "Global Step: 13700 Epoch 6/50 Iteration: 13700 Avg. Training loss: 2.8978 0.0188 sec/batch\n",
      "Global Step: 13800 Epoch 6/50 Iteration: 13800 Avg. Training loss: 2.9174 0.0175 sec/batch\n",
      "Global Step: 13900 Epoch 6/50 Iteration: 13900 Avg. Training loss: 2.8748 0.0151 sec/batch\n",
      "Global Step: 14000 Epoch 6/50 Iteration: 14000 Avg. Training loss: 2.8746 0.0157 sec/batch\n",
      "Global Step: 14100 Epoch 6/50 Iteration: 14100 Avg. Training loss: 2.8978 0.0157 sec/batch\n",
      "Epoch 7/50 Threshold: 0.0850077106535117 Length of Training words: 2416591\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 2.8935 0.0005 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 2.8553 0.0173 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 2.8703 0.0138 sec/batch\n",
      "Global Step: 14500 Epoch 7/50 Iteration: 14500 Avg. Training loss: 2.8535 0.0124 sec/batch\n",
      "Global Step: 14600 Epoch 7/50 Iteration: 14600 Avg. Training loss: 2.8206 0.0168 sec/batch\n",
      "Global Step: 14700 Epoch 7/50 Iteration: 14700 Avg. Training loss: 2.8345 0.0153 sec/batch\n",
      "Global Step: 14800 Epoch 7/50 Iteration: 14800 Avg. Training loss: 2.8407 0.0142 sec/batch\n",
      "Global Step: 14900 Epoch 7/50 Iteration: 14900 Avg. Training loss: 2.8304 0.0154 sec/batch\n",
      "Global Step: 15000 Epoch 7/50 Iteration: 15000 Avg. Training loss: 2.8644 0.0141 sec/batch\n",
      "Global Step: 15100 Epoch 7/50 Iteration: 15100 Avg. Training loss: 2.8404 0.0161 sec/batch\n",
      "Global Step: 15200 Epoch 7/50 Iteration: 15200 Avg. Training loss: 2.8351 0.0156 sec/batch\n",
      "Global Step: 15300 Epoch 7/50 Iteration: 15300 Avg. Training loss: 2.8273 0.0154 sec/batch\n",
      "Global Step: 15400 Epoch 7/50 Iteration: 15400 Avg. Training loss: 2.8339 0.0151 sec/batch\n",
      "Global Step: 15500 Epoch 7/50 Iteration: 15500 Avg. Training loss: 2.8274 0.0165 sec/batch\n",
      "Global Step: 15600 Epoch 7/50 Iteration: 15600 Avg. Training loss: 2.8442 0.0156 sec/batch\n",
      "Global Step: 15700 Epoch 7/50 Iteration: 15700 Avg. Training loss: 2.8390 0.0145 sec/batch\n",
      "Global Step: 15800 Epoch 7/50 Iteration: 15800 Avg. Training loss: 2.8466 0.0144 sec/batch\n",
      "Global Step: 15900 Epoch 7/50 Iteration: 15900 Avg. Training loss: 2.8240 0.0137 sec/batch\n",
      "Global Step: 16000 Epoch 7/50 Iteration: 16000 Avg. Training loss: 2.8630 0.0160 sec/batch\n",
      "Global Step: 16100 Epoch 7/50 Iteration: 16100 Avg. Training loss: 2.8418 0.0159 sec/batch\n",
      "Global Step: 16200 Epoch 7/50 Iteration: 16200 Avg. Training loss: 2.8544 0.0152 sec/batch\n",
      "Global Step: 16300 Epoch 7/50 Iteration: 16300 Avg. Training loss: 2.8151 0.0183 sec/batch\n",
      "Global Step: 16400 Epoch 7/50 Iteration: 16400 Avg. Training loss: 2.8179 0.0167 sec/batch\n",
      "Global Step: 16500 Epoch 7/50 Iteration: 16500 Avg. Training loss: 2.8445 0.0160 sec/batch\n",
      "Global Step: 16600 Epoch 7/50 Iteration: 16600 Avg. Training loss: 2.8349 0.0171 sec/batch\n",
      "Epoch 8/50 Threshold: 0.08467242047489035 Length of Training words: 2414420\n",
      "Global Step: 16700 Epoch 8/50 Iteration: 16700 Avg. Training loss: 2.8454 0.0163 sec/batch\n",
      "Global Step: 16800 Epoch 8/50 Iteration: 16800 Avg. Training loss: 2.8691 0.0147 sec/batch\n",
      "Global Step: 16900 Epoch 8/50 Iteration: 16900 Avg. Training loss: 2.8577 0.0149 sec/batch\n",
      "Global Step: 17000 Epoch 8/50 Iteration: 17000 Avg. Training loss: 2.8350 0.0139 sec/batch\n",
      "Global Step: 17100 Epoch 8/50 Iteration: 17100 Avg. Training loss: 2.8341 0.0148 sec/batch\n",
      "Global Step: 17200 Epoch 8/50 Iteration: 17200 Avg. Training loss: 2.8312 0.0130 sec/batch\n",
      "Global Step: 17300 Epoch 8/50 Iteration: 17300 Avg. Training loss: 2.8392 0.0140 sec/batch\n",
      "Global Step: 17400 Epoch 8/50 Iteration: 17400 Avg. Training loss: 2.8545 0.0137 sec/batch\n",
      "Global Step: 17500 Epoch 8/50 Iteration: 17500 Avg. Training loss: 2.8421 0.0141 sec/batch\n",
      "Global Step: 17600 Epoch 8/50 Iteration: 17600 Avg. Training loss: 2.8445 0.0170 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17700 Epoch 8/50 Iteration: 17700 Avg. Training loss: 2.8237 0.0144 sec/batch\n",
      "Global Step: 17800 Epoch 8/50 Iteration: 17800 Avg. Training loss: 2.8300 0.0127 sec/batch\n",
      "Global Step: 17900 Epoch 8/50 Iteration: 17900 Avg. Training loss: 2.8458 0.0158 sec/batch\n",
      "Global Step: 18000 Epoch 8/50 Iteration: 18000 Avg. Training loss: 2.8464 0.0136 sec/batch\n",
      "Global Step: 18100 Epoch 8/50 Iteration: 18100 Avg. Training loss: 2.8346 0.0166 sec/batch\n",
      "Global Step: 18200 Epoch 8/50 Iteration: 18200 Avg. Training loss: 2.8472 0.0168 sec/batch\n",
      "Global Step: 18300 Epoch 8/50 Iteration: 18300 Avg. Training loss: 2.8252 0.0158 sec/batch\n",
      "Global Step: 18400 Epoch 8/50 Iteration: 18400 Avg. Training loss: 2.8632 0.0176 sec/batch\n",
      "Global Step: 18500 Epoch 8/50 Iteration: 18500 Avg. Training loss: 2.8403 0.0161 sec/batch\n",
      "Global Step: 18600 Epoch 8/50 Iteration: 18600 Avg. Training loss: 2.8588 0.0135 sec/batch\n",
      "Global Step: 18700 Epoch 8/50 Iteration: 18700 Avg. Training loss: 2.8214 0.0137 sec/batch\n",
      "Global Step: 18800 Epoch 8/50 Iteration: 18800 Avg. Training loss: 2.8139 0.0155 sec/batch\n",
      "Global Step: 18900 Epoch 8/50 Iteration: 18900 Avg. Training loss: 2.8470 0.0151 sec/batch\n",
      "Global Step: 19000 Epoch 8/50 Iteration: 19000 Avg. Training loss: 2.8332 0.0172 sec/batch\n",
      "Epoch 9/50 Threshold: 0.08782582800636445 Length of Training words: 2434382\n",
      "Global Step: 19100 Epoch 9/50 Iteration: 19100 Avg. Training loss: 2.8397 0.0130 sec/batch\n",
      "Global Step: 19200 Epoch 9/50 Iteration: 19200 Avg. Training loss: 2.8512 0.0173 sec/batch\n",
      "Global Step: 19300 Epoch 9/50 Iteration: 19300 Avg. Training loss: 2.8553 0.0164 sec/batch\n",
      "Global Step: 19400 Epoch 9/50 Iteration: 19400 Avg. Training loss: 2.8236 0.0150 sec/batch\n",
      "Global Step: 19500 Epoch 9/50 Iteration: 19500 Avg. Training loss: 2.8288 0.0146 sec/batch\n",
      "Global Step: 19600 Epoch 9/50 Iteration: 19600 Avg. Training loss: 2.8261 0.0143 sec/batch\n",
      "Global Step: 19700 Epoch 9/50 Iteration: 19700 Avg. Training loss: 2.8345 0.0138 sec/batch\n",
      "Global Step: 19800 Epoch 9/50 Iteration: 19800 Avg. Training loss: 2.8411 0.0150 sec/batch\n",
      "Global Step: 19900 Epoch 9/50 Iteration: 19900 Avg. Training loss: 2.8350 0.0155 sec/batch\n",
      "Global Step: 20000 Epoch 9/50 Iteration: 20000 Avg. Training loss: 2.8528 0.0161 sec/batch\n",
      "Global Step: 20100 Epoch 9/50 Iteration: 20100 Avg. Training loss: 2.8137 0.0150 sec/batch\n",
      "Global Step: 20200 Epoch 9/50 Iteration: 20200 Avg. Training loss: 2.8243 0.0162 sec/batch\n",
      "Global Step: 20300 Epoch 9/50 Iteration: 20300 Avg. Training loss: 2.8447 0.0161 sec/batch\n",
      "Global Step: 20400 Epoch 9/50 Iteration: 20400 Avg. Training loss: 2.8207 0.0160 sec/batch\n",
      "Global Step: 20500 Epoch 9/50 Iteration: 20500 Avg. Training loss: 2.8408 0.0153 sec/batch\n",
      "Global Step: 20600 Epoch 9/50 Iteration: 20600 Avg. Training loss: 2.8344 0.0136 sec/batch\n",
      "Global Step: 20700 Epoch 9/50 Iteration: 20700 Avg. Training loss: 2.8122 0.0140 sec/batch\n",
      "Global Step: 20800 Epoch 9/50 Iteration: 20800 Avg. Training loss: 2.8549 0.0149 sec/batch\n",
      "Global Step: 20900 Epoch 9/50 Iteration: 20900 Avg. Training loss: 2.8309 0.0150 sec/batch\n",
      "Global Step: 21000 Epoch 9/50 Iteration: 21000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 21100 Epoch 9/50 Iteration: 21100 Avg. Training loss: 2.8344 0.0172 sec/batch\n",
      "Global Step: 21200 Epoch 9/50 Iteration: 21200 Avg. Training loss: 2.8136 0.0154 sec/batch\n",
      "Global Step: 21300 Epoch 9/50 Iteration: 21300 Avg. Training loss: 2.8192 0.0151 sec/batch\n",
      "Global Step: 21400 Epoch 9/50 Iteration: 21400 Avg. Training loss: 2.8302 0.0147 sec/batch\n",
      "Epoch 10/50 Threshold: 0.08681111929218609 Length of Training words: 2427436\n",
      "Global Step: 21500 Epoch 10/50 Iteration: 21500 Avg. Training loss: 2.8344 0.0059 sec/batch\n",
      "Global Step: 21600 Epoch 10/50 Iteration: 21600 Avg. Training loss: 2.8455 0.0158 sec/batch\n",
      "Global Step: 21700 Epoch 10/50 Iteration: 21700 Avg. Training loss: 2.8662 0.0176 sec/batch\n",
      "Global Step: 21800 Epoch 10/50 Iteration: 21800 Avg. Training loss: 2.8459 0.0143 sec/batch\n",
      "Global Step: 21900 Epoch 10/50 Iteration: 21900 Avg. Training loss: 2.8286 0.0145 sec/batch\n",
      "Global Step: 22000 Epoch 10/50 Iteration: 22000 Avg. Training loss: 2.8245 0.0144 sec/batch\n",
      "Global Step: 22100 Epoch 10/50 Iteration: 22100 Avg. Training loss: 2.8291 0.0187 sec/batch\n",
      "Global Step: 22200 Epoch 10/50 Iteration: 22200 Avg. Training loss: 2.8406 0.0187 sec/batch\n",
      "Global Step: 22300 Epoch 10/50 Iteration: 22300 Avg. Training loss: 2.8366 0.0160 sec/batch\n",
      "Global Step: 22400 Epoch 10/50 Iteration: 22400 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 22500 Epoch 10/50 Iteration: 22500 Avg. Training loss: 2.8220 0.0154 sec/batch\n",
      "Global Step: 22600 Epoch 10/50 Iteration: 22600 Avg. Training loss: 2.8159 0.0148 sec/batch\n",
      "Global Step: 22700 Epoch 10/50 Iteration: 22700 Avg. Training loss: 2.8466 0.0155 sec/batch\n",
      "Global Step: 22800 Epoch 10/50 Iteration: 22800 Avg. Training loss: 2.8275 0.0137 sec/batch\n",
      "Global Step: 22900 Epoch 10/50 Iteration: 22900 Avg. Training loss: 2.8319 0.0152 sec/batch\n",
      "Global Step: 23000 Epoch 10/50 Iteration: 23000 Avg. Training loss: 2.8433 0.0153 sec/batch\n",
      "Global Step: 23100 Epoch 10/50 Iteration: 23100 Avg. Training loss: 2.8259 0.0154 sec/batch\n",
      "Global Step: 23200 Epoch 10/50 Iteration: 23200 Avg. Training loss: 2.8376 0.0145 sec/batch\n",
      "Global Step: 23300 Epoch 10/50 Iteration: 23300 Avg. Training loss: 2.8581 0.0167 sec/batch\n",
      "Global Step: 23400 Epoch 10/50 Iteration: 23400 Avg. Training loss: 2.8369 0.0155 sec/batch\n",
      "Global Step: 23500 Epoch 10/50 Iteration: 23500 Avg. Training loss: 2.8404 0.0117 sec/batch\n",
      "Global Step: 23600 Epoch 10/50 Iteration: 23600 Avg. Training loss: 2.8159 0.0146 sec/batch\n",
      "Global Step: 23700 Epoch 10/50 Iteration: 23700 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 23800 Epoch 10/50 Iteration: 23800 Avg. Training loss: 2.8264 0.0151 sec/batch\n",
      "Epoch 11/50 Threshold: 0.08986173649493497 Length of Training words: 2445824\n",
      "Global Step: 23900 Epoch 11/50 Iteration: 23900 Avg. Training loss: 2.8282 0.0015 sec/batch\n",
      "Global Step: 24000 Epoch 11/50 Iteration: 24000 Avg. Training loss: 2.8505 0.0152 sec/batch\n",
      "Global Step: 24100 Epoch 11/50 Iteration: 24100 Avg. Training loss: 2.8602 0.0140 sec/batch\n",
      "Global Step: 24200 Epoch 11/50 Iteration: 24200 Avg. Training loss: 2.8411 0.0137 sec/batch\n",
      "Global Step: 24300 Epoch 11/50 Iteration: 24300 Avg. Training loss: 2.8116 0.0134 sec/batch\n",
      "Global Step: 24400 Epoch 11/50 Iteration: 24400 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 24500 Epoch 11/50 Iteration: 24500 Avg. Training loss: 2.8330 0.0149 sec/batch\n",
      "Global Step: 24600 Epoch 11/50 Iteration: 24600 Avg. Training loss: 2.8196 0.0159 sec/batch\n",
      "Global Step: 24700 Epoch 11/50 Iteration: 24700 Avg. Training loss: 2.8562 0.0134 sec/batch\n",
      "Global Step: 24800 Epoch 11/50 Iteration: 24800 Avg. Training loss: 2.8342 0.0162 sec/batch\n",
      "Global Step: 24900 Epoch 11/50 Iteration: 24900 Avg. Training loss: 2.8242 0.0138 sec/batch\n",
      "Global Step: 25000 Epoch 11/50 Iteration: 25000 Avg. Training loss: 2.8181 0.0175 sec/batch\n",
      "Global Step: 25100 Epoch 11/50 Iteration: 25100 Avg. Training loss: 2.8224 0.0222 sec/batch\n",
      "Global Step: 25200 Epoch 11/50 Iteration: 25200 Avg. Training loss: 2.8228 0.0155 sec/batch\n",
      "Global Step: 25300 Epoch 11/50 Iteration: 25300 Avg. Training loss: 2.8374 0.0185 sec/batch\n",
      "Global Step: 25400 Epoch 11/50 Iteration: 25400 Avg. Training loss: 2.8314 0.0150 sec/batch\n",
      "Global Step: 25500 Epoch 11/50 Iteration: 25500 Avg. Training loss: 2.8323 0.0132 sec/batch\n",
      "Global Step: 25600 Epoch 11/50 Iteration: 25600 Avg. Training loss: 2.8125 0.0152 sec/batch\n",
      "Global Step: 25700 Epoch 11/50 Iteration: 25700 Avg. Training loss: 2.8541 0.0151 sec/batch\n",
      "Global Step: 25800 Epoch 11/50 Iteration: 25800 Avg. Training loss: 2.8278 0.0160 sec/batch\n",
      "Global Step: 25900 Epoch 11/50 Iteration: 25900 Avg. Training loss: 2.8493 0.0146 sec/batch\n",
      "Global Step: 26000 Epoch 11/50 Iteration: 26000 Avg. Training loss: 2.8121 0.0156 sec/batch\n",
      "Global Step: 26100 Epoch 11/50 Iteration: 26100 Avg. Training loss: 2.8029 0.0142 sec/batch\n",
      "Global Step: 26200 Epoch 11/50 Iteration: 26200 Avg. Training loss: 2.8370 0.0149 sec/batch\n",
      "Global Step: 26300 Epoch 11/50 Iteration: 26300 Avg. Training loss: 2.8203 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Threshold: 0.08110778818964093 Length of Training words: 2392894\n",
      "Global Step: 26400 Epoch 12/50 Iteration: 26400 Avg. Training loss: 2.8435 0.0091 sec/batch\n",
      "Global Step: 26500 Epoch 12/50 Iteration: 26500 Avg. Training loss: 2.8679 0.0172 sec/batch\n",
      "Global Step: 26600 Epoch 12/50 Iteration: 26600 Avg. Training loss: 2.8693 0.0159 sec/batch\n",
      "Global Step: 26700 Epoch 12/50 Iteration: 26700 Avg. Training loss: 2.8375 0.0153 sec/batch\n",
      "Global Step: 26800 Epoch 12/50 Iteration: 26800 Avg. Training loss: 2.8435 0.0164 sec/batch\n",
      "Global Step: 26900 Epoch 12/50 Iteration: 26900 Avg. Training loss: 2.8369 0.0145 sec/batch\n",
      "Global Step: 27000 Epoch 12/50 Iteration: 27000 Avg. Training loss: 2.8479 0.0164 sec/batch\n",
      "Global Step: 27100 Epoch 12/50 Iteration: 27100 Avg. Training loss: 2.8547 0.0155 sec/batch\n",
      "Global Step: 27200 Epoch 12/50 Iteration: 27200 Avg. Training loss: 2.8543 0.0150 sec/batch\n",
      "Global Step: 27300 Epoch 12/50 Iteration: 27300 Avg. Training loss: 2.8543 0.0140 sec/batch\n",
      "Global Step: 27400 Epoch 12/50 Iteration: 27400 Avg. Training loss: 2.8292 0.0176 sec/batch\n",
      "Global Step: 27500 Epoch 12/50 Iteration: 27500 Avg. Training loss: 2.8413 0.0163 sec/batch\n",
      "Global Step: 27600 Epoch 12/50 Iteration: 27600 Avg. Training loss: 2.8461 0.0153 sec/batch\n",
      "Global Step: 27700 Epoch 12/50 Iteration: 27700 Avg. Training loss: 2.8538 0.0149 sec/batch\n",
      "Global Step: 27800 Epoch 12/50 Iteration: 27800 Avg. Training loss: 2.8460 0.0162 sec/batch\n",
      "Global Step: 27900 Epoch 12/50 Iteration: 27900 Avg. Training loss: 2.8436 0.0162 sec/batch\n",
      "Global Step: 28000 Epoch 12/50 Iteration: 28000 Avg. Training loss: 2.8351 0.0163 sec/batch\n",
      "Global Step: 28100 Epoch 12/50 Iteration: 28100 Avg. Training loss: 2.8640 0.0164 sec/batch\n",
      "Global Step: 28200 Epoch 12/50 Iteration: 28200 Avg. Training loss: 2.8487 0.0128 sec/batch\n",
      "Global Step: 28300 Epoch 12/50 Iteration: 28300 Avg. Training loss: 2.8673 0.0152 sec/batch\n",
      "Global Step: 28400 Epoch 12/50 Iteration: 28400 Avg. Training loss: 2.8257 0.0147 sec/batch\n",
      "Global Step: 28500 Epoch 12/50 Iteration: 28500 Avg. Training loss: 2.8200 0.0152 sec/batch\n",
      "Global Step: 28600 Epoch 12/50 Iteration: 28600 Avg. Training loss: 2.8543 0.0156 sec/batch\n",
      "Global Step: 28700 Epoch 12/50 Iteration: 28700 Avg. Training loss: 2.8426 0.0189 sec/batch\n",
      "Epoch 13/50 Threshold: 0.08717083632765375 Length of Training words: 2430009\n",
      "Global Step: 28800 Epoch 13/50 Iteration: 28800 Avg. Training loss: 2.8433 0.0119 sec/batch\n",
      "Global Step: 28900 Epoch 13/50 Iteration: 28900 Avg. Training loss: 2.8544 0.0155 sec/batch\n",
      "Global Step: 29000 Epoch 13/50 Iteration: 29000 Avg. Training loss: 2.8561 0.0183 sec/batch\n",
      "Global Step: 29100 Epoch 13/50 Iteration: 29100 Avg. Training loss: 2.8250 0.0165 sec/batch\n",
      "Global Step: 29200 Epoch 13/50 Iteration: 29200 Avg. Training loss: 2.8316 0.0147 sec/batch\n",
      "Global Step: 29300 Epoch 13/50 Iteration: 29300 Avg. Training loss: 2.8254 0.0164 sec/batch\n",
      "Global Step: 29400 Epoch 13/50 Iteration: 29400 Avg. Training loss: 2.8380 0.0132 sec/batch\n",
      "Global Step: 29500 Epoch 13/50 Iteration: 29500 Avg. Training loss: 2.8419 0.0152 sec/batch\n",
      "Global Step: 29600 Epoch 13/50 Iteration: 29600 Avg. Training loss: 2.8412 0.0165 sec/batch\n",
      "Global Step: 29700 Epoch 13/50 Iteration: 29700 Avg. Training loss: 2.8453 0.0150 sec/batch\n",
      "Global Step: 29800 Epoch 13/50 Iteration: 29800 Avg. Training loss: 2.8164 0.0160 sec/batch\n",
      "Global Step: 29900 Epoch 13/50 Iteration: 29900 Avg. Training loss: 2.8203 0.0165 sec/batch\n",
      "Global Step: 30000 Epoch 13/50 Iteration: 30000 Avg. Training loss: 2.8475 0.0163 sec/batch\n",
      "Global Step: 30100 Epoch 13/50 Iteration: 30100 Avg. Training loss: 2.8233 0.0131 sec/batch\n",
      "Global Step: 30200 Epoch 13/50 Iteration: 30200 Avg. Training loss: 2.8436 0.0148 sec/batch\n",
      "Global Step: 30300 Epoch 13/50 Iteration: 30300 Avg. Training loss: 2.8325 0.0129 sec/batch\n",
      "Global Step: 30400 Epoch 13/50 Iteration: 30400 Avg. Training loss: 2.8146 0.0169 sec/batch\n",
      "Global Step: 30500 Epoch 13/50 Iteration: 30500 Avg. Training loss: 2.8589 0.0126 sec/batch\n",
      "Global Step: 30600 Epoch 13/50 Iteration: 30600 Avg. Training loss: 2.8321 0.0148 sec/batch\n",
      "Global Step: 30700 Epoch 13/50 Iteration: 30700 Avg. Training loss: 2.8510 0.0167 sec/batch\n",
      "Global Step: 30800 Epoch 13/50 Iteration: 30800 Avg. Training loss: 2.8292 0.0177 sec/batch\n",
      "Global Step: 30900 Epoch 13/50 Iteration: 30900 Avg. Training loss: 2.8177 0.0145 sec/batch\n",
      "Global Step: 31000 Epoch 13/50 Iteration: 31000 Avg. Training loss: 2.8219 0.0189 sec/batch\n",
      "Global Step: 31100 Epoch 13/50 Iteration: 31100 Avg. Training loss: 2.8284 0.0155 sec/batch\n",
      "Epoch 14/50 Threshold: 0.07736795802688062 Length of Training words: 2369330\n",
      "Global Step: 31200 Epoch 14/50 Iteration: 31200 Avg. Training loss: 2.8465 0.0079 sec/batch\n",
      "Global Step: 31300 Epoch 14/50 Iteration: 31300 Avg. Training loss: 2.8703 0.0140 sec/batch\n",
      "Global Step: 31400 Epoch 14/50 Iteration: 31400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 31500 Epoch 14/50 Iteration: 31500 Avg. Training loss: 2.8589 0.0144 sec/batch\n",
      "Global Step: 31600 Epoch 14/50 Iteration: 31600 Avg. Training loss: 2.8476 0.0167 sec/batch\n",
      "Global Step: 31700 Epoch 14/50 Iteration: 31700 Avg. Training loss: 2.8344 0.0144 sec/batch\n",
      "Global Step: 31800 Epoch 14/50 Iteration: 31800 Avg. Training loss: 2.8710 0.0176 sec/batch\n",
      "Global Step: 31900 Epoch 14/50 Iteration: 31900 Avg. Training loss: 2.8550 0.0190 sec/batch\n",
      "Global Step: 32000 Epoch 14/50 Iteration: 32000 Avg. Training loss: 2.8547 0.0152 sec/batch\n",
      "Global Step: 32100 Epoch 14/50 Iteration: 32100 Avg. Training loss: 2.8728 0.0167 sec/batch\n",
      "Global Step: 32200 Epoch 14/50 Iteration: 32200 Avg. Training loss: 2.8362 0.0161 sec/batch\n",
      "Global Step: 32300 Epoch 14/50 Iteration: 32300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 32400 Epoch 14/50 Iteration: 32400 Avg. Training loss: 2.8629 0.0148 sec/batch\n",
      "Global Step: 32500 Epoch 14/50 Iteration: 32500 Avg. Training loss: 2.8484 0.0145 sec/batch\n",
      "Global Step: 32600 Epoch 14/50 Iteration: 32600 Avg. Training loss: 2.8589 0.0150 sec/batch\n",
      "Global Step: 32700 Epoch 14/50 Iteration: 32700 Avg. Training loss: 2.8483 0.0118 sec/batch\n",
      "Global Step: 32800 Epoch 14/50 Iteration: 32800 Avg. Training loss: 2.8380 0.0158 sec/batch\n",
      "Global Step: 32900 Epoch 14/50 Iteration: 32900 Avg. Training loss: 2.8775 0.0144 sec/batch\n",
      "Global Step: 33000 Epoch 14/50 Iteration: 33000 Avg. Training loss: 2.8572 0.0178 sec/batch\n",
      "Global Step: 33100 Epoch 14/50 Iteration: 33100 Avg. Training loss: 2.8785 0.0160 sec/batch\n",
      "Global Step: 33200 Epoch 14/50 Iteration: 33200 Avg. Training loss: 2.8371 0.0187 sec/batch\n",
      "Global Step: 33300 Epoch 14/50 Iteration: 33300 Avg. Training loss: 2.8276 0.0170 sec/batch\n",
      "Global Step: 33400 Epoch 14/50 Iteration: 33400 Avg. Training loss: 2.8591 0.0198 sec/batch\n",
      "Global Step: 33500 Epoch 14/50 Iteration: 33500 Avg. Training loss: 2.8482 0.0157 sec/batch\n",
      "Epoch 15/50 Threshold: 0.07867502562390928 Length of Training words: 2377258\n",
      "Global Step: 33600 Epoch 15/50 Iteration: 33600 Avg. Training loss: 2.8611 0.0127 sec/batch\n",
      "Global Step: 33700 Epoch 15/50 Iteration: 33700 Avg. Training loss: 2.8720 0.0163 sec/batch\n",
      "Global Step: 33800 Epoch 15/50 Iteration: 33800 Avg. Training loss: 2.8717 0.0160 sec/batch\n",
      "Global Step: 33900 Epoch 15/50 Iteration: 33900 Avg. Training loss: 2.8478 0.0143 sec/batch\n",
      "Global Step: 34000 Epoch 15/50 Iteration: 34000 Avg. Training loss: 2.8473 0.0149 sec/batch\n",
      "Global Step: 34100 Epoch 15/50 Iteration: 34100 Avg. Training loss: 2.8435 0.0159 sec/batch\n",
      "Global Step: 34200 Epoch 15/50 Iteration: 34200 Avg. Training loss: 2.8486 0.0162 sec/batch\n",
      "Global Step: 34300 Epoch 15/50 Iteration: 34300 Avg. Training loss: 2.8660 0.0167 sec/batch\n",
      "Global Step: 34400 Epoch 15/50 Iteration: 34400 Avg. Training loss: 2.8560 0.0142 sec/batch\n",
      "Global Step: 34500 Epoch 15/50 Iteration: 34500 Avg. Training loss: 2.8551 0.0142 sec/batch\n",
      "Global Step: 34600 Epoch 15/50 Iteration: 34600 Avg. Training loss: 2.8281 0.0140 sec/batch\n",
      "Global Step: 34700 Epoch 15/50 Iteration: 34700 Avg. Training loss: 2.8506 0.0139 sec/batch\n",
      "Global Step: 34800 Epoch 15/50 Iteration: 34800 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 34900 Epoch 15/50 Iteration: 34900 Avg. Training loss: 2.8578 0.0156 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 35000 Epoch 15/50 Iteration: 35000 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 35100 Epoch 15/50 Iteration: 35100 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 35200 Epoch 15/50 Iteration: 35200 Avg. Training loss: 2.8403 0.0138 sec/batch\n",
      "Global Step: 35300 Epoch 15/50 Iteration: 35300 Avg. Training loss: 2.8762 0.0164 sec/batch\n",
      "Global Step: 35400 Epoch 15/50 Iteration: 35400 Avg. Training loss: 2.8581 0.0161 sec/batch\n",
      "Global Step: 35500 Epoch 15/50 Iteration: 35500 Avg. Training loss: 2.8638 0.0139 sec/batch\n",
      "Global Step: 35600 Epoch 15/50 Iteration: 35600 Avg. Training loss: 2.8266 0.0152 sec/batch\n",
      "Global Step: 35700 Epoch 15/50 Iteration: 35700 Avg. Training loss: 2.8329 0.0153 sec/batch\n",
      "Global Step: 35800 Epoch 15/50 Iteration: 35800 Avg. Training loss: 2.8430 0.0177 sec/batch\n",
      "Global Step: 35900 Epoch 15/50 Iteration: 35900 Avg. Training loss: 2.8503 0.0130 sec/batch\n",
      "Epoch 16/50 Threshold: 0.0720019299680255 Length of Training words: 2329165\n",
      "Global Step: 36000 Epoch 16/50 Iteration: 36000 Avg. Training loss: 2.8825 0.0148 sec/batch\n",
      "Global Step: 36100 Epoch 16/50 Iteration: 36100 Avg. Training loss: 2.8978 0.0151 sec/batch\n",
      "Global Step: 36200 Epoch 16/50 Iteration: 36200 Avg. Training loss: 2.8758 0.0147 sec/batch\n",
      "Global Step: 36300 Epoch 16/50 Iteration: 36300 Avg. Training loss: 2.8573 0.0144 sec/batch\n",
      "Global Step: 36400 Epoch 16/50 Iteration: 36400 Avg. Training loss: 2.8587 0.0150 sec/batch\n",
      "Global Step: 36500 Epoch 16/50 Iteration: 36500 Avg. Training loss: 2.8645 0.0132 sec/batch\n",
      "Global Step: 36600 Epoch 16/50 Iteration: 36600 Avg. Training loss: 2.8690 0.0175 sec/batch\n",
      "Global Step: 36700 Epoch 16/50 Iteration: 36700 Avg. Training loss: 2.8724 0.0186 sec/batch\n",
      "Global Step: 36800 Epoch 16/50 Iteration: 36800 Avg. Training loss: 2.8878 0.0156 sec/batch\n",
      "Global Step: 36900 Epoch 16/50 Iteration: 36900 Avg. Training loss: 2.8542 0.0145 sec/batch\n",
      "Global Step: 37000 Epoch 16/50 Iteration: 37000 Avg. Training loss: 2.8491 0.0160 sec/batch\n",
      "Global Step: 37100 Epoch 16/50 Iteration: 37100 Avg. Training loss: 2.8816 0.0162 sec/batch\n",
      "Global Step: 37200 Epoch 16/50 Iteration: 37200 Avg. Training loss: 2.8555 0.0143 sec/batch\n",
      "Global Step: 37300 Epoch 16/50 Iteration: 37300 Avg. Training loss: 2.8749 0.0158 sec/batch\n",
      "Global Step: 37400 Epoch 16/50 Iteration: 37400 Avg. Training loss: 2.8679 0.0150 sec/batch\n",
      "Global Step: 37500 Epoch 16/50 Iteration: 37500 Avg. Training loss: 2.8492 0.0180 sec/batch\n",
      "Global Step: 37600 Epoch 16/50 Iteration: 37600 Avg. Training loss: 2.8883 0.0165 sec/batch\n",
      "Global Step: 37700 Epoch 16/50 Iteration: 37700 Avg. Training loss: 2.8683 0.0145 sec/batch\n",
      "Global Step: 37800 Epoch 16/50 Iteration: 37800 Avg. Training loss: 2.8865 0.0156 sec/batch\n",
      "Global Step: 37900 Epoch 16/50 Iteration: 37900 Avg. Training loss: 2.8610 0.0161 sec/batch\n",
      "Global Step: 38000 Epoch 16/50 Iteration: 38000 Avg. Training loss: 2.8436 0.0163 sec/batch\n",
      "Global Step: 38100 Epoch 16/50 Iteration: 38100 Avg. Training loss: 2.8677 0.0152 sec/batch\n",
      "Global Step: 38200 Epoch 16/50 Iteration: 38200 Avg. Training loss: 2.8611 0.0151 sec/batch\n",
      "Epoch 17/50 Threshold: 0.08720321176489212 Length of Training words: 2429828\n",
      "Global Step: 38300 Epoch 17/50 Iteration: 38300 Avg. Training loss: 2.8484 0.0092 sec/batch\n",
      "Global Step: 38400 Epoch 17/50 Iteration: 38400 Avg. Training loss: 2.8508 0.0141 sec/batch\n",
      "Global Step: 38500 Epoch 17/50 Iteration: 38500 Avg. Training loss: 2.8600 0.0147 sec/batch\n",
      "Global Step: 38600 Epoch 17/50 Iteration: 38600 Avg. Training loss: 2.8243 0.0113 sec/batch\n",
      "Global Step: 38700 Epoch 17/50 Iteration: 38700 Avg. Training loss: 2.8304 0.0119 sec/batch\n",
      "Global Step: 38800 Epoch 17/50 Iteration: 38800 Avg. Training loss: 2.8248 0.0125 sec/batch\n",
      "Global Step: 38900 Epoch 17/50 Iteration: 38900 Avg. Training loss: 2.8357 0.0140 sec/batch\n",
      "Global Step: 39000 Epoch 17/50 Iteration: 39000 Avg. Training loss: 2.8408 0.0191 sec/batch\n",
      "Global Step: 39100 Epoch 17/50 Iteration: 39100 Avg. Training loss: 2.8364 0.0169 sec/batch\n",
      "Global Step: 39200 Epoch 17/50 Iteration: 39200 Avg. Training loss: 2.8535 0.0160 sec/batch\n",
      "Global Step: 39300 Epoch 17/50 Iteration: 39300 Avg. Training loss: 2.8166 0.0167 sec/batch\n",
      "Global Step: 39400 Epoch 17/50 Iteration: 39400 Avg. Training loss: 2.8239 0.0158 sec/batch\n",
      "Global Step: 39500 Epoch 17/50 Iteration: 39500 Avg. Training loss: 2.8466 0.0130 sec/batch\n",
      "Global Step: 39600 Epoch 17/50 Iteration: 39600 Avg. Training loss: 2.8177 0.0142 sec/batch\n",
      "Global Step: 39700 Epoch 17/50 Iteration: 39700 Avg. Training loss: 2.8436 0.0166 sec/batch\n",
      "Global Step: 39800 Epoch 17/50 Iteration: 39800 Avg. Training loss: 2.8375 0.0138 sec/batch\n",
      "Global Step: 39900 Epoch 17/50 Iteration: 39900 Avg. Training loss: 2.8115 0.0134 sec/batch\n",
      "Global Step: 40000 Epoch 17/50 Iteration: 40000 Avg. Training loss: 2.8589 0.0158 sec/batch\n",
      "Global Step: 40100 Epoch 17/50 Iteration: 40100 Avg. Training loss: 2.8330 0.0148 sec/batch\n",
      "Global Step: 40200 Epoch 17/50 Iteration: 40200 Avg. Training loss: 2.8518 0.0157 sec/batch\n",
      "Global Step: 40300 Epoch 17/50 Iteration: 40300 Avg. Training loss: 2.8338 0.0133 sec/batch\n",
      "Global Step: 40400 Epoch 17/50 Iteration: 40400 Avg. Training loss: 2.8156 0.0137 sec/batch\n",
      "Global Step: 40500 Epoch 17/50 Iteration: 40500 Avg. Training loss: 2.8206 0.0139 sec/batch\n",
      "Global Step: 40600 Epoch 17/50 Iteration: 40600 Avg. Training loss: 2.8285 0.0118 sec/batch\n",
      "Epoch 18/50 Threshold: 0.07863276721785589 Length of Training words: 2376633\n",
      "Global Step: 40700 Epoch 18/50 Iteration: 40700 Avg. Training loss: 2.8469 0.0056 sec/batch\n",
      "Global Step: 40800 Epoch 18/50 Iteration: 40800 Avg. Training loss: 2.8651 0.0117 sec/batch\n",
      "Global Step: 40900 Epoch 18/50 Iteration: 40900 Avg. Training loss: 2.8781 0.0132 sec/batch\n",
      "Global Step: 41000 Epoch 18/50 Iteration: 41000 Avg. Training loss: 2.8563 0.0117 sec/batch\n",
      "Global Step: 41100 Epoch 18/50 Iteration: 41100 Avg. Training loss: 2.8441 0.0126 sec/batch\n",
      "Global Step: 41200 Epoch 18/50 Iteration: 41200 Avg. Training loss: 2.8358 0.0129 sec/batch\n",
      "Global Step: 41300 Epoch 18/50 Iteration: 41300 Avg. Training loss: 2.8593 0.0126 sec/batch\n",
      "Global Step: 41400 Epoch 18/50 Iteration: 41400 Avg. Training loss: 2.8597 0.0126 sec/batch\n",
      "Global Step: 41500 Epoch 18/50 Iteration: 41500 Avg. Training loss: 2.8469 0.0143 sec/batch\n",
      "Global Step: 41600 Epoch 18/50 Iteration: 41600 Avg. Training loss: 2.8755 0.0138 sec/batch\n",
      "Global Step: 41700 Epoch 18/50 Iteration: 41700 Avg. Training loss: 2.8338 0.0155 sec/batch\n",
      "Global Step: 41800 Epoch 18/50 Iteration: 41800 Avg. Training loss: 2.8415 0.0143 sec/batch\n",
      "Global Step: 41900 Epoch 18/50 Iteration: 41900 Avg. Training loss: 2.8631 0.0147 sec/batch\n",
      "Global Step: 42000 Epoch 18/50 Iteration: 42000 Avg. Training loss: 2.8376 0.0160 sec/batch\n",
      "Global Step: 42100 Epoch 18/50 Iteration: 42100 Avg. Training loss: 2.8589 0.0148 sec/batch\n",
      "Global Step: 42200 Epoch 18/50 Iteration: 42200 Avg. Training loss: 2.8495 0.0156 sec/batch\n",
      "Global Step: 42300 Epoch 18/50 Iteration: 42300 Avg. Training loss: 2.8306 0.0156 sec/batch\n",
      "Global Step: 42400 Epoch 18/50 Iteration: 42400 Avg. Training loss: 2.8768 0.0153 sec/batch\n",
      "Global Step: 42500 Epoch 18/50 Iteration: 42500 Avg. Training loss: 2.8515 0.0144 sec/batch\n",
      "Global Step: 42600 Epoch 18/50 Iteration: 42600 Avg. Training loss: 2.8700 0.0133 sec/batch\n",
      "Global Step: 42700 Epoch 18/50 Iteration: 42700 Avg. Training loss: 2.8418 0.0129 sec/batch\n",
      "Global Step: 42800 Epoch 18/50 Iteration: 42800 Avg. Training loss: 2.8286 0.0127 sec/batch\n",
      "Global Step: 42900 Epoch 18/50 Iteration: 42900 Avg. Training loss: 2.8542 0.0137 sec/batch\n",
      "Global Step: 43000 Epoch 18/50 Iteration: 43000 Avg. Training loss: 2.8467 0.0126 sec/batch\n",
      "Epoch 19/50 Threshold: 0.084364621536794 Length of Training words: 2411957\n",
      "Global Step: 43100 Epoch 19/50 Iteration: 43100 Avg. Training loss: 2.8475 0.0077 sec/batch\n",
      "Global Step: 43200 Epoch 19/50 Iteration: 43200 Avg. Training loss: 2.8590 0.0144 sec/batch\n",
      "Global Step: 43300 Epoch 19/50 Iteration: 43300 Avg. Training loss: 2.8653 0.0121 sec/batch\n",
      "Global Step: 43400 Epoch 19/50 Iteration: 43400 Avg. Training loss: 2.8360 0.0134 sec/batch\n",
      "Global Step: 43500 Epoch 19/50 Iteration: 43500 Avg. Training loss: 2.8348 0.0142 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 43600 Epoch 19/50 Iteration: 43600 Avg. Training loss: 2.8298 0.0130 sec/batch\n",
      "Global Step: 43700 Epoch 19/50 Iteration: 43700 Avg. Training loss: 2.8439 0.0126 sec/batch\n",
      "Global Step: 43800 Epoch 19/50 Iteration: 43800 Avg. Training loss: 2.8480 0.0170 sec/batch\n",
      "Global Step: 43900 Epoch 19/50 Iteration: 43900 Avg. Training loss: 2.8422 0.0172 sec/batch\n",
      "Global Step: 44000 Epoch 19/50 Iteration: 44000 Avg. Training loss: 2.8582 0.0153 sec/batch\n",
      "Global Step: 44100 Epoch 19/50 Iteration: 44100 Avg. Training loss: 2.8216 0.0152 sec/batch\n",
      "Global Step: 44200 Epoch 19/50 Iteration: 44200 Avg. Training loss: 2.8287 0.0126 sec/batch\n",
      "Global Step: 44300 Epoch 19/50 Iteration: 44300 Avg. Training loss: 2.8507 0.0135 sec/batch\n",
      "Global Step: 44400 Epoch 19/50 Iteration: 44400 Avg. Training loss: 2.8275 0.0149 sec/batch\n",
      "Global Step: 44500 Epoch 19/50 Iteration: 44500 Avg. Training loss: 2.8478 0.0157 sec/batch\n",
      "Global Step: 44600 Epoch 19/50 Iteration: 44600 Avg. Training loss: 2.8371 0.0156 sec/batch\n",
      "Global Step: 44700 Epoch 19/50 Iteration: 44700 Avg. Training loss: 2.8216 0.0141 sec/batch\n",
      "Global Step: 44800 Epoch 19/50 Iteration: 44800 Avg. Training loss: 2.8652 0.0153 sec/batch\n",
      "Global Step: 44900 Epoch 19/50 Iteration: 44900 Avg. Training loss: 2.8342 0.0136 sec/batch\n",
      "Global Step: 45000 Epoch 19/50 Iteration: 45000 Avg. Training loss: 2.8565 0.0120 sec/batch\n",
      "Global Step: 45100 Epoch 19/50 Iteration: 45100 Avg. Training loss: 2.8320 0.0129 sec/batch\n",
      "Global Step: 45200 Epoch 19/50 Iteration: 45200 Avg. Training loss: 2.8217 0.0132 sec/batch\n",
      "Global Step: 45300 Epoch 19/50 Iteration: 45300 Avg. Training loss: 2.8350 0.0144 sec/batch\n",
      "Global Step: 45400 Epoch 19/50 Iteration: 45400 Avg. Training loss: 2.8393 0.0121 sec/batch\n",
      "Epoch 20/50 Threshold: 0.06092402220480751 Length of Training words: 2238418\n",
      "Global Step: 45500 Epoch 20/50 Iteration: 45500 Avg. Training loss: 2.8707 0.0072 sec/batch\n",
      "Global Step: 45600 Epoch 20/50 Iteration: 45600 Avg. Training loss: 2.9149 0.0124 sec/batch\n",
      "Global Step: 45700 Epoch 20/50 Iteration: 45700 Avg. Training loss: 2.9167 0.0126 sec/batch\n",
      "Global Step: 45800 Epoch 20/50 Iteration: 45800 Avg. Training loss: 2.8921 0.0117 sec/batch\n",
      "Global Step: 45900 Epoch 20/50 Iteration: 45900 Avg. Training loss: 2.8866 0.0116 sec/batch\n",
      "Global Step: 46000 Epoch 20/50 Iteration: 46000 Avg. Training loss: 2.8914 0.0139 sec/batch\n",
      "Global Step: 46100 Epoch 20/50 Iteration: 46100 Avg. Training loss: 2.8929 0.0158 sec/batch\n",
      "Global Step: 46200 Epoch 20/50 Iteration: 46200 Avg. Training loss: 2.9094 0.0138 sec/batch\n",
      "Global Step: 46300 Epoch 20/50 Iteration: 46300 Avg. Training loss: 2.9070 0.0130 sec/batch\n",
      "Global Step: 46400 Epoch 20/50 Iteration: 46400 Avg. Training loss: 2.8830 0.0156 sec/batch\n",
      "Global Step: 46500 Epoch 20/50 Iteration: 46500 Avg. Training loss: 2.8782 0.0147 sec/batch\n",
      "Global Step: 46600 Epoch 20/50 Iteration: 46600 Avg. Training loss: 2.9095 0.0153 sec/batch\n",
      "Global Step: 46700 Epoch 20/50 Iteration: 46700 Avg. Training loss: 2.8839 0.0135 sec/batch\n",
      "Global Step: 46800 Epoch 20/50 Iteration: 46800 Avg. Training loss: 2.9048 0.0163 sec/batch\n",
      "Global Step: 46900 Epoch 20/50 Iteration: 46900 Avg. Training loss: 2.8951 0.0152 sec/batch\n",
      "Global Step: 47000 Epoch 20/50 Iteration: 47000 Avg. Training loss: 2.8764 0.0155 sec/batch\n",
      "Global Step: 47100 Epoch 20/50 Iteration: 47100 Avg. Training loss: 2.9205 0.0149 sec/batch\n",
      "Global Step: 47200 Epoch 20/50 Iteration: 47200 Avg. Training loss: 2.8997 0.0120 sec/batch\n",
      "Global Step: 47300 Epoch 20/50 Iteration: 47300 Avg. Training loss: 2.9113 0.0122 sec/batch\n",
      "Global Step: 47400 Epoch 20/50 Iteration: 47400 Avg. Training loss: 2.8768 0.0139 sec/batch\n",
      "Global Step: 47500 Epoch 20/50 Iteration: 47500 Avg. Training loss: 2.8812 0.0124 sec/batch\n",
      "Global Step: 47600 Epoch 20/50 Iteration: 47600 Avg. Training loss: 2.8851 0.0131 sec/batch\n",
      "Epoch 21/50 Threshold: 0.06706143014707991 Length of Training words: 2289788\n",
      "Global Step: 47700 Epoch 21/50 Iteration: 47700 Avg. Training loss: 2.8957 0.0020 sec/batch\n",
      "Global Step: 47800 Epoch 21/50 Iteration: 47800 Avg. Training loss: 2.8904 0.0126 sec/batch\n",
      "Global Step: 47900 Epoch 21/50 Iteration: 47900 Avg. Training loss: 2.9109 0.0124 sec/batch\n",
      "Global Step: 48000 Epoch 21/50 Iteration: 48000 Avg. Training loss: 2.8883 0.0127 sec/batch\n",
      "Global Step: 48100 Epoch 21/50 Iteration: 48100 Avg. Training loss: 2.8717 0.0141 sec/batch\n",
      "Global Step: 48200 Epoch 21/50 Iteration: 48200 Avg. Training loss: 2.8689 0.0142 sec/batch\n",
      "Global Step: 48300 Epoch 21/50 Iteration: 48300 Avg. Training loss: 2.8858 0.0149 sec/batch\n",
      "Global Step: 48400 Epoch 21/50 Iteration: 48400 Avg. Training loss: 2.8819 0.0152 sec/batch\n",
      "Global Step: 48500 Epoch 21/50 Iteration: 48500 Avg. Training loss: 2.8791 0.0160 sec/batch\n",
      "Global Step: 48600 Epoch 21/50 Iteration: 48600 Avg. Training loss: 2.8976 0.0150 sec/batch\n",
      "Global Step: 48700 Epoch 21/50 Iteration: 48700 Avg. Training loss: 2.8656 0.0150 sec/batch\n",
      "Global Step: 48800 Epoch 21/50 Iteration: 48800 Avg. Training loss: 2.8699 0.0140 sec/batch\n",
      "Global Step: 48900 Epoch 21/50 Iteration: 48900 Avg. Training loss: 2.8773 0.0165 sec/batch\n",
      "Global Step: 49000 Epoch 21/50 Iteration: 49000 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 49100 Epoch 21/50 Iteration: 49100 Avg. Training loss: 2.8780 0.0171 sec/batch\n",
      "Global Step: 49200 Epoch 21/50 Iteration: 49200 Avg. Training loss: 2.8829 0.0150 sec/batch\n",
      "Global Step: 49300 Epoch 21/50 Iteration: 49300 Avg. Training loss: 2.8680 0.0130 sec/batch\n",
      "Global Step: 49400 Epoch 21/50 Iteration: 49400 Avg. Training loss: 2.9020 0.0130 sec/batch\n",
      "Global Step: 49500 Epoch 21/50 Iteration: 49500 Avg. Training loss: 2.8804 0.0127 sec/batch\n",
      "Global Step: 49600 Epoch 21/50 Iteration: 49600 Avg. Training loss: 2.8893 0.0136 sec/batch\n",
      "Global Step: 49700 Epoch 21/50 Iteration: 49700 Avg. Training loss: 2.8556 0.0127 sec/batch\n",
      "Global Step: 49800 Epoch 21/50 Iteration: 49800 Avg. Training loss: 2.8666 0.0138 sec/batch\n",
      "Global Step: 49900 Epoch 21/50 Iteration: 49900 Avg. Training loss: 2.8745 0.0135 sec/batch\n",
      "Epoch 22/50 Threshold: 0.07384798892845924 Length of Training words: 2344481\n",
      "Global Step: 50000 Epoch 22/50 Iteration: 50000 Avg. Training loss: 2.8710 0.0043 sec/batch\n",
      "Global Step: 50100 Epoch 22/50 Iteration: 50100 Avg. Training loss: 2.8709 0.0139 sec/batch\n",
      "Global Step: 50200 Epoch 22/50 Iteration: 50200 Avg. Training loss: 2.8957 0.0150 sec/batch\n",
      "Global Step: 50300 Epoch 22/50 Iteration: 50300 Avg. Training loss: 2.8718 0.0127 sec/batch\n",
      "Global Step: 50400 Epoch 22/50 Iteration: 50400 Avg. Training loss: 2.8550 0.0156 sec/batch\n",
      "Global Step: 50500 Epoch 22/50 Iteration: 50500 Avg. Training loss: 2.8489 0.0155 sec/batch\n",
      "Global Step: 50600 Epoch 22/50 Iteration: 50600 Avg. Training loss: 2.8670 0.0153 sec/batch\n",
      "Global Step: 50700 Epoch 22/50 Iteration: 50700 Avg. Training loss: 2.8698 0.0156 sec/batch\n",
      "Global Step: 50800 Epoch 22/50 Iteration: 50800 Avg. Training loss: 2.8572 0.0148 sec/batch\n",
      "Global Step: 50900 Epoch 22/50 Iteration: 50900 Avg. Training loss: 2.8870 0.0133 sec/batch\n",
      "Global Step: 51000 Epoch 22/50 Iteration: 51000 Avg. Training loss: 2.8440 0.0124 sec/batch\n",
      "Global Step: 51100 Epoch 22/50 Iteration: 51100 Avg. Training loss: 2.8533 0.0152 sec/batch\n",
      "Global Step: 51200 Epoch 22/50 Iteration: 51200 Avg. Training loss: 2.8699 0.0147 sec/batch\n",
      "Global Step: 51300 Epoch 22/50 Iteration: 51300 Avg. Training loss: 2.8494 0.0143 sec/batch\n",
      "Global Step: 51400 Epoch 22/50 Iteration: 51400 Avg. Training loss: 2.8701 0.0146 sec/batch\n",
      "Global Step: 51500 Epoch 22/50 Iteration: 51500 Avg. Training loss: 2.8587 0.0146 sec/batch\n",
      "Global Step: 51600 Epoch 22/50 Iteration: 51600 Avg. Training loss: 2.8490 0.0124 sec/batch\n",
      "Global Step: 51700 Epoch 22/50 Iteration: 51700 Avg. Training loss: 2.8839 0.0129 sec/batch\n",
      "Global Step: 51800 Epoch 22/50 Iteration: 51800 Avg. Training loss: 2.8656 0.0130 sec/batch\n",
      "Global Step: 51900 Epoch 22/50 Iteration: 51900 Avg. Training loss: 2.8848 0.0125 sec/batch\n",
      "Global Step: 52000 Epoch 22/50 Iteration: 52000 Avg. Training loss: 2.8423 0.0138 sec/batch\n",
      "Global Step: 52100 Epoch 22/50 Iteration: 52100 Avg. Training loss: 2.8388 0.0139 sec/batch\n",
      "Global Step: 52200 Epoch 22/50 Iteration: 52200 Avg. Training loss: 2.8699 0.0129 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 52300 Epoch 22/50 Iteration: 52300 Avg. Training loss: 2.8550 0.0132 sec/batch\n",
      "Epoch 23/50 Threshold: 0.07978773587287813 Length of Training words: 2383586\n",
      "Global Step: 52400 Epoch 23/50 Iteration: 52400 Avg. Training loss: 2.8550 0.0097 sec/batch\n",
      "Global Step: 52500 Epoch 23/50 Iteration: 52500 Avg. Training loss: 2.8775 0.0147 sec/batch\n",
      "Global Step: 52600 Epoch 23/50 Iteration: 52600 Avg. Training loss: 2.8682 0.0155 sec/batch\n",
      "Global Step: 52700 Epoch 23/50 Iteration: 52700 Avg. Training loss: 2.8443 0.0162 sec/batch\n",
      "Global Step: 52800 Epoch 23/50 Iteration: 52800 Avg. Training loss: 2.8399 0.0147 sec/batch\n",
      "Global Step: 52900 Epoch 23/50 Iteration: 52900 Avg. Training loss: 2.8438 0.0173 sec/batch\n",
      "Global Step: 53000 Epoch 23/50 Iteration: 53000 Avg. Training loss: 2.8473 0.0147 sec/batch\n",
      "Global Step: 53100 Epoch 23/50 Iteration: 53100 Avg. Training loss: 2.8656 0.0167 sec/batch\n",
      "Global Step: 53200 Epoch 23/50 Iteration: 53200 Avg. Training loss: 2.8504 0.0156 sec/batch\n",
      "Global Step: 53300 Epoch 23/50 Iteration: 53300 Avg. Training loss: 2.8504 0.0166 sec/batch\n",
      "Global Step: 53400 Epoch 23/50 Iteration: 53400 Avg. Training loss: 2.8326 0.0156 sec/batch\n",
      "Global Step: 53500 Epoch 23/50 Iteration: 53500 Avg. Training loss: 2.8474 0.0140 sec/batch\n",
      "Global Step: 53600 Epoch 23/50 Iteration: 53600 Avg. Training loss: 2.8433 0.0134 sec/batch\n",
      "Global Step: 53700 Epoch 23/50 Iteration: 53700 Avg. Training loss: 2.8552 0.0142 sec/batch\n",
      "Global Step: 53800 Epoch 23/50 Iteration: 53800 Avg. Training loss: 2.8489 0.0139 sec/batch\n",
      "Global Step: 53900 Epoch 23/50 Iteration: 53900 Avg. Training loss: 2.8549 0.0100 sec/batch\n",
      "Global Step: 54000 Epoch 23/50 Iteration: 54000 Avg. Training loss: 2.8393 0.0120 sec/batch\n",
      "Global Step: 54100 Epoch 23/50 Iteration: 54100 Avg. Training loss: 2.8720 0.0134 sec/batch\n",
      "Global Step: 54200 Epoch 23/50 Iteration: 54200 Avg. Training loss: 2.8509 0.0123 sec/batch\n",
      "Global Step: 54300 Epoch 23/50 Iteration: 54300 Avg. Training loss: 2.8595 0.0141 sec/batch\n",
      "Global Step: 54400 Epoch 23/50 Iteration: 54400 Avg. Training loss: 2.8235 0.0159 sec/batch\n",
      "Global Step: 54500 Epoch 23/50 Iteration: 54500 Avg. Training loss: 2.8347 0.0173 sec/batch\n",
      "Global Step: 54600 Epoch 23/50 Iteration: 54600 Avg. Training loss: 2.8431 0.0149 sec/batch\n",
      "Global Step: 54700 Epoch 23/50 Iteration: 54700 Avg. Training loss: 2.8479 0.0154 sec/batch\n",
      "Epoch 24/50 Threshold: 0.0780015896553169 Length of Training words: 2372332\n",
      "Global Step: 54800 Epoch 24/50 Iteration: 54800 Avg. Training loss: 2.8681 0.0129 sec/batch\n",
      "Global Step: 54900 Epoch 24/50 Iteration: 54900 Avg. Training loss: 2.8848 0.0151 sec/batch\n",
      "Global Step: 55000 Epoch 24/50 Iteration: 55000 Avg. Training loss: 2.8635 0.0135 sec/batch\n",
      "Global Step: 55100 Epoch 24/50 Iteration: 55100 Avg. Training loss: 2.8363 0.0169 sec/batch\n",
      "Global Step: 55200 Epoch 24/50 Iteration: 55200 Avg. Training loss: 2.8517 0.0175 sec/batch\n",
      "Global Step: 55300 Epoch 24/50 Iteration: 55300 Avg. Training loss: 2.8504 0.0152 sec/batch\n",
      "Global Step: 55400 Epoch 24/50 Iteration: 55400 Avg. Training loss: 2.8488 0.0162 sec/batch\n",
      "Global Step: 55500 Epoch 24/50 Iteration: 55500 Avg. Training loss: 2.8712 0.0149 sec/batch\n",
      "Global Step: 55600 Epoch 24/50 Iteration: 55600 Avg. Training loss: 2.8663 0.0151 sec/batch\n",
      "Global Step: 55700 Epoch 24/50 Iteration: 55700 Avg. Training loss: 2.8392 0.0144 sec/batch\n",
      "Global Step: 55800 Epoch 24/50 Iteration: 55800 Avg. Training loss: 2.8393 0.0137 sec/batch\n",
      "Global Step: 55900 Epoch 24/50 Iteration: 55900 Avg. Training loss: 2.8572 0.0121 sec/batch\n",
      "Global Step: 56000 Epoch 24/50 Iteration: 56000 Avg. Training loss: 2.8480 0.0133 sec/batch\n",
      "Global Step: 56100 Epoch 24/50 Iteration: 56100 Avg. Training loss: 2.8465 0.0165 sec/batch\n",
      "Global Step: 56200 Epoch 24/50 Iteration: 56200 Avg. Training loss: 2.8622 0.0161 sec/batch\n",
      "Global Step: 56300 Epoch 24/50 Iteration: 56300 Avg. Training loss: 2.8479 0.0135 sec/batch\n",
      "Global Step: 56400 Epoch 24/50 Iteration: 56400 Avg. Training loss: 2.8543 0.0144 sec/batch\n",
      "Global Step: 56500 Epoch 24/50 Iteration: 56500 Avg. Training loss: 2.8700 0.0149 sec/batch\n",
      "Global Step: 56600 Epoch 24/50 Iteration: 56600 Avg. Training loss: 2.8564 0.0142 sec/batch\n",
      "Global Step: 56700 Epoch 24/50 Iteration: 56700 Avg. Training loss: 2.8645 0.0153 sec/batch\n",
      "Global Step: 56800 Epoch 24/50 Iteration: 56800 Avg. Training loss: 2.8281 0.0164 sec/batch\n",
      "Global Step: 56900 Epoch 24/50 Iteration: 56900 Avg. Training loss: 2.8405 0.0150 sec/batch\n",
      "Global Step: 57000 Epoch 24/50 Iteration: 57000 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Epoch 25/50 Threshold: 0.08888214480232033 Length of Training words: 2439982\n",
      "Global Step: 57100 Epoch 25/50 Iteration: 57100 Avg. Training loss: 2.8459 0.0050 sec/batch\n",
      "Global Step: 57200 Epoch 25/50 Iteration: 57200 Avg. Training loss: 2.8390 0.0158 sec/batch\n",
      "Global Step: 57300 Epoch 25/50 Iteration: 57300 Avg. Training loss: 2.8665 0.0159 sec/batch\n",
      "Global Step: 57400 Epoch 25/50 Iteration: 57400 Avg. Training loss: 2.8340 0.0161 sec/batch\n",
      "Global Step: 57500 Epoch 25/50 Iteration: 57500 Avg. Training loss: 2.8204 0.0156 sec/batch\n",
      "Global Step: 57600 Epoch 25/50 Iteration: 57600 Avg. Training loss: 2.8238 0.0147 sec/batch\n",
      "Global Step: 57700 Epoch 25/50 Iteration: 57700 Avg. Training loss: 2.8264 0.0164 sec/batch\n",
      "Global Step: 57800 Epoch 25/50 Iteration: 57800 Avg. Training loss: 2.8334 0.0129 sec/batch\n",
      "Global Step: 57900 Epoch 25/50 Iteration: 57900 Avg. Training loss: 2.8425 0.0142 sec/batch\n",
      "Global Step: 58000 Epoch 25/50 Iteration: 58000 Avg. Training loss: 2.8515 0.0137 sec/batch\n",
      "Global Step: 58100 Epoch 25/50 Iteration: 58100 Avg. Training loss: 2.8138 0.0134 sec/batch\n",
      "Global Step: 58200 Epoch 25/50 Iteration: 58200 Avg. Training loss: 2.8194 0.0123 sec/batch\n",
      "Global Step: 58300 Epoch 25/50 Iteration: 58300 Avg. Training loss: 2.8313 0.0131 sec/batch\n",
      "Global Step: 58400 Epoch 25/50 Iteration: 58400 Avg. Training loss: 2.8262 0.0141 sec/batch\n",
      "Global Step: 58500 Epoch 25/50 Iteration: 58500 Avg. Training loss: 2.8276 0.0120 sec/batch\n",
      "Global Step: 58600 Epoch 25/50 Iteration: 58600 Avg. Training loss: 2.8294 0.0125 sec/batch\n",
      "Global Step: 58700 Epoch 25/50 Iteration: 58700 Avg. Training loss: 2.8341 0.0139 sec/batch\n",
      "Global Step: 58800 Epoch 25/50 Iteration: 58800 Avg. Training loss: 2.8255 0.0140 sec/batch\n",
      "Global Step: 58900 Epoch 25/50 Iteration: 58900 Avg. Training loss: 2.8509 0.0144 sec/batch\n",
      "Global Step: 59000 Epoch 25/50 Iteration: 59000 Avg. Training loss: 2.8332 0.0173 sec/batch\n",
      "Global Step: 59100 Epoch 25/50 Iteration: 59100 Avg. Training loss: 2.8422 0.0174 sec/batch\n",
      "Global Step: 59200 Epoch 25/50 Iteration: 59200 Avg. Training loss: 2.8070 0.0156 sec/batch\n",
      "Global Step: 59300 Epoch 25/50 Iteration: 59300 Avg. Training loss: 2.8117 0.0152 sec/batch\n",
      "Global Step: 59400 Epoch 25/50 Iteration: 59400 Avg. Training loss: 2.8299 0.0140 sec/batch\n",
      "Global Step: 59500 Epoch 25/50 Iteration: 59500 Avg. Training loss: 2.8283 0.0153 sec/batch\n",
      "Epoch 26/50 Threshold: 0.07345678619703419 Length of Training words: 2341446\n",
      "Global Step: 59600 Epoch 26/50 Iteration: 59600 Avg. Training loss: 2.8632 0.0132 sec/batch\n",
      "Global Step: 59700 Epoch 26/50 Iteration: 59700 Avg. Training loss: 2.9014 0.0175 sec/batch\n",
      "Global Step: 59800 Epoch 26/50 Iteration: 59800 Avg. Training loss: 2.8739 0.0173 sec/batch\n",
      "Global Step: 59900 Epoch 26/50 Iteration: 59900 Avg. Training loss: 2.8502 0.0133 sec/batch\n",
      "Global Step: 60000 Epoch 26/50 Iteration: 60000 Avg. Training loss: 2.8588 0.0154 sec/batch\n",
      "Global Step: 60100 Epoch 26/50 Iteration: 60100 Avg. Training loss: 2.8628 0.0153 sec/batch\n",
      "Global Step: 60200 Epoch 26/50 Iteration: 60200 Avg. Training loss: 2.8578 0.0140 sec/batch\n",
      "Global Step: 60300 Epoch 26/50 Iteration: 60300 Avg. Training loss: 2.8807 0.0139 sec/batch\n",
      "Global Step: 60400 Epoch 26/50 Iteration: 60400 Avg. Training loss: 2.8789 0.0155 sec/batch\n",
      "Global Step: 60500 Epoch 26/50 Iteration: 60500 Avg. Training loss: 2.8458 0.0132 sec/batch\n",
      "Global Step: 60600 Epoch 26/50 Iteration: 60600 Avg. Training loss: 2.8506 0.0154 sec/batch\n",
      "Global Step: 60700 Epoch 26/50 Iteration: 60700 Avg. Training loss: 2.8664 0.0173 sec/batch\n",
      "Global Step: 60800 Epoch 26/50 Iteration: 60800 Avg. Training loss: 2.8588 0.0144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 60900 Epoch 26/50 Iteration: 60900 Avg. Training loss: 2.8592 0.0140 sec/batch\n",
      "Global Step: 61000 Epoch 26/50 Iteration: 61000 Avg. Training loss: 2.8718 0.0146 sec/batch\n",
      "Global Step: 61100 Epoch 26/50 Iteration: 61100 Avg. Training loss: 2.8490 0.0119 sec/batch\n",
      "Global Step: 61200 Epoch 26/50 Iteration: 61200 Avg. Training loss: 2.8788 0.0142 sec/batch\n",
      "Global Step: 61300 Epoch 26/50 Iteration: 61300 Avg. Training loss: 2.8717 0.0144 sec/batch\n",
      "Global Step: 61400 Epoch 26/50 Iteration: 61400 Avg. Training loss: 2.8806 0.0137 sec/batch\n",
      "Global Step: 61500 Epoch 26/50 Iteration: 61500 Avg. Training loss: 2.8656 0.0122 sec/batch\n",
      "Global Step: 61600 Epoch 26/50 Iteration: 61600 Avg. Training loss: 2.8393 0.0152 sec/batch\n",
      "Global Step: 61700 Epoch 26/50 Iteration: 61700 Avg. Training loss: 2.8497 0.0163 sec/batch\n",
      "Global Step: 61800 Epoch 26/50 Iteration: 61800 Avg. Training loss: 2.8591 0.0134 sec/batch\n",
      "Epoch 27/50 Threshold: 0.06887011513088591 Length of Training words: 2304434\n",
      "Global Step: 61900 Epoch 27/50 Iteration: 61900 Avg. Training loss: 2.8719 0.0065 sec/batch\n",
      "Global Step: 62000 Epoch 27/50 Iteration: 62000 Avg. Training loss: 2.8947 0.0164 sec/batch\n",
      "Global Step: 62100 Epoch 27/50 Iteration: 62100 Avg. Training loss: 2.8940 0.0152 sec/batch\n",
      "Global Step: 62200 Epoch 27/50 Iteration: 62200 Avg. Training loss: 2.8749 0.0151 sec/batch\n",
      "Global Step: 62300 Epoch 27/50 Iteration: 62300 Avg. Training loss: 2.8666 0.0146 sec/batch\n",
      "Global Step: 62400 Epoch 27/50 Iteration: 62400 Avg. Training loss: 2.8653 0.0144 sec/batch\n",
      "Global Step: 62500 Epoch 27/50 Iteration: 62500 Avg. Training loss: 2.8751 0.0145 sec/batch\n",
      "Global Step: 62600 Epoch 27/50 Iteration: 62600 Avg. Training loss: 2.8865 0.0162 sec/batch\n",
      "Global Step: 62700 Epoch 27/50 Iteration: 62700 Avg. Training loss: 2.8800 0.0163 sec/batch\n",
      "Global Step: 62800 Epoch 27/50 Iteration: 62800 Avg. Training loss: 2.8802 0.0151 sec/batch\n",
      "Global Step: 62900 Epoch 27/50 Iteration: 62900 Avg. Training loss: 2.8586 0.0163 sec/batch\n",
      "Global Step: 63000 Epoch 27/50 Iteration: 63000 Avg. Training loss: 2.8810 0.0153 sec/batch\n",
      "Global Step: 63100 Epoch 27/50 Iteration: 63100 Avg. Training loss: 2.8715 0.0163 sec/batch\n",
      "Global Step: 63200 Epoch 27/50 Iteration: 63200 Avg. Training loss: 2.8706 0.0136 sec/batch\n",
      "Global Step: 63300 Epoch 27/50 Iteration: 63300 Avg. Training loss: 2.8690 0.0142 sec/batch\n",
      "Global Step: 63400 Epoch 27/50 Iteration: 63400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 63500 Epoch 27/50 Iteration: 63500 Avg. Training loss: 2.8745 0.0145 sec/batch\n",
      "Global Step: 63600 Epoch 27/50 Iteration: 63600 Avg. Training loss: 2.8927 0.0141 sec/batch\n",
      "Global Step: 63700 Epoch 27/50 Iteration: 63700 Avg. Training loss: 2.8815 0.0149 sec/batch\n",
      "Global Step: 63800 Epoch 27/50 Iteration: 63800 Avg. Training loss: 2.8809 0.0149 sec/batch\n",
      "Global Step: 63900 Epoch 27/50 Iteration: 63900 Avg. Training loss: 2.8529 0.0154 sec/batch\n",
      "Global Step: 64000 Epoch 27/50 Iteration: 64000 Avg. Training loss: 2.8606 0.0159 sec/batch\n",
      "Global Step: 64100 Epoch 27/50 Iteration: 64100 Avg. Training loss: 2.8647 0.0147 sec/batch\n",
      "Epoch 28/50 Threshold: 0.08345389394934948 Length of Training words: 2406327\n",
      "Global Step: 64200 Epoch 28/50 Iteration: 64200 Avg. Training loss: 2.8669 0.0068 sec/batch\n",
      "Global Step: 64300 Epoch 28/50 Iteration: 64300 Avg. Training loss: 2.8533 0.0153 sec/batch\n",
      "Global Step: 64400 Epoch 28/50 Iteration: 64400 Avg. Training loss: 2.8678 0.0171 sec/batch\n",
      "Global Step: 64500 Epoch 28/50 Iteration: 64500 Avg. Training loss: 2.8451 0.0160 sec/batch\n",
      "Global Step: 64600 Epoch 28/50 Iteration: 64600 Avg. Training loss: 2.8374 0.0173 sec/batch\n",
      "Global Step: 64700 Epoch 28/50 Iteration: 64700 Avg. Training loss: 2.8261 0.0156 sec/batch\n",
      "Global Step: 64800 Epoch 28/50 Iteration: 64800 Avg. Training loss: 2.8465 0.0147 sec/batch\n",
      "Global Step: 64900 Epoch 28/50 Iteration: 64900 Avg. Training loss: 2.8482 0.0152 sec/batch\n",
      "Global Step: 65000 Epoch 28/50 Iteration: 65000 Avg. Training loss: 2.8399 0.0148 sec/batch\n",
      "Global Step: 65100 Epoch 28/50 Iteration: 65100 Avg. Training loss: 2.8677 0.0125 sec/batch\n",
      "Global Step: 65200 Epoch 28/50 Iteration: 65200 Avg. Training loss: 2.8238 0.0138 sec/batch\n",
      "Global Step: 65300 Epoch 28/50 Iteration: 65300 Avg. Training loss: 2.8220 0.0142 sec/batch\n",
      "Global Step: 65400 Epoch 28/50 Iteration: 65400 Avg. Training loss: 2.8588 0.0149 sec/batch\n",
      "Global Step: 65500 Epoch 28/50 Iteration: 65500 Avg. Training loss: 2.8271 0.0172 sec/batch\n",
      "Global Step: 65600 Epoch 28/50 Iteration: 65600 Avg. Training loss: 2.8475 0.0139 sec/batch\n",
      "Global Step: 65700 Epoch 28/50 Iteration: 65700 Avg. Training loss: 2.8460 0.0169 sec/batch\n",
      "Global Step: 65800 Epoch 28/50 Iteration: 65800 Avg. Training loss: 2.8226 0.0142 sec/batch\n",
      "Global Step: 65900 Epoch 28/50 Iteration: 65900 Avg. Training loss: 2.8628 0.0144 sec/batch\n",
      "Global Step: 66000 Epoch 28/50 Iteration: 66000 Avg. Training loss: 2.8420 0.0164 sec/batch\n",
      "Global Step: 66100 Epoch 28/50 Iteration: 66100 Avg. Training loss: 2.8640 0.0122 sec/batch\n",
      "Global Step: 66200 Epoch 28/50 Iteration: 66200 Avg. Training loss: 2.8489 0.0126 sec/batch\n",
      "Global Step: 66300 Epoch 28/50 Iteration: 66300 Avg. Training loss: 2.8178 0.0141 sec/batch\n",
      "Global Step: 66400 Epoch 28/50 Iteration: 66400 Avg. Training loss: 2.8261 0.0148 sec/batch\n",
      "Global Step: 66500 Epoch 28/50 Iteration: 66500 Avg. Training loss: 2.8332 0.0136 sec/batch\n",
      "Epoch 29/50 Threshold: 0.07780942376694977 Length of Training words: 2371334\n",
      "Global Step: 66600 Epoch 29/50 Iteration: 66600 Avg. Training loss: 2.8515 0.0051 sec/batch\n",
      "Global Step: 66700 Epoch 29/50 Iteration: 66700 Avg. Training loss: 2.8699 0.0144 sec/batch\n",
      "Global Step: 66800 Epoch 29/50 Iteration: 66800 Avg. Training loss: 2.8806 0.0142 sec/batch\n",
      "Global Step: 66900 Epoch 29/50 Iteration: 66900 Avg. Training loss: 2.8606 0.0174 sec/batch\n",
      "Global Step: 67000 Epoch 29/50 Iteration: 67000 Avg. Training loss: 2.8471 0.0158 sec/batch\n",
      "Global Step: 67100 Epoch 29/50 Iteration: 67100 Avg. Training loss: 2.8366 0.0155 sec/batch\n",
      "Global Step: 67200 Epoch 29/50 Iteration: 67200 Avg. Training loss: 2.8587 0.0172 sec/batch\n",
      "Global Step: 67300 Epoch 29/50 Iteration: 67300 Avg. Training loss: 2.8587 0.0173 sec/batch\n",
      "Global Step: 67400 Epoch 29/50 Iteration: 67400 Avg. Training loss: 2.8475 0.0143 sec/batch\n",
      "Global Step: 67500 Epoch 29/50 Iteration: 67500 Avg. Training loss: 2.8783 0.0162 sec/batch\n",
      "Global Step: 67600 Epoch 29/50 Iteration: 67600 Avg. Training loss: 2.8358 0.0146 sec/batch\n",
      "Global Step: 67700 Epoch 29/50 Iteration: 67700 Avg. Training loss: 2.8432 0.0171 sec/batch\n",
      "Global Step: 67800 Epoch 29/50 Iteration: 67800 Avg. Training loss: 2.8625 0.0133 sec/batch\n",
      "Global Step: 67900 Epoch 29/50 Iteration: 67900 Avg. Training loss: 2.8385 0.0154 sec/batch\n",
      "Global Step: 68000 Epoch 29/50 Iteration: 68000 Avg. Training loss: 2.8606 0.0161 sec/batch\n",
      "Global Step: 68100 Epoch 29/50 Iteration: 68100 Avg. Training loss: 2.8533 0.0178 sec/batch\n",
      "Global Step: 68200 Epoch 29/50 Iteration: 68200 Avg. Training loss: 2.8337 0.0147 sec/batch\n",
      "Global Step: 68300 Epoch 29/50 Iteration: 68300 Avg. Training loss: 2.8782 0.0142 sec/batch\n",
      "Global Step: 68400 Epoch 29/50 Iteration: 68400 Avg. Training loss: 2.8537 0.0148 sec/batch\n",
      "Global Step: 68500 Epoch 29/50 Iteration: 68500 Avg. Training loss: 2.8706 0.0155 sec/batch\n",
      "Global Step: 68600 Epoch 29/50 Iteration: 68600 Avg. Training loss: 2.8400 0.0144 sec/batch\n",
      "Global Step: 68700 Epoch 29/50 Iteration: 68700 Avg. Training loss: 2.8288 0.0144 sec/batch\n",
      "Global Step: 68800 Epoch 29/50 Iteration: 68800 Avg. Training loss: 2.8570 0.0160 sec/batch\n",
      "Global Step: 68900 Epoch 29/50 Iteration: 68900 Avg. Training loss: 2.8468 0.0142 sec/batch\n",
      "Epoch 30/50 Threshold: 0.0654034721364846 Length of Training words: 2275887\n",
      "Global Step: 69000 Epoch 30/50 Iteration: 69000 Avg. Training loss: 2.8809 0.0074 sec/batch\n",
      "Global Step: 69100 Epoch 30/50 Iteration: 69100 Avg. Training loss: 2.9058 0.0126 sec/batch\n",
      "Global Step: 69200 Epoch 30/50 Iteration: 69200 Avg. Training loss: 2.9044 0.0157 sec/batch\n",
      "Global Step: 69300 Epoch 30/50 Iteration: 69300 Avg. Training loss: 2.8774 0.0135 sec/batch\n",
      "Global Step: 69400 Epoch 30/50 Iteration: 69400 Avg. Training loss: 2.8757 0.0148 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 69500 Epoch 30/50 Iteration: 69500 Avg. Training loss: 2.8781 0.0181 sec/batch\n",
      "Global Step: 69600 Epoch 30/50 Iteration: 69600 Avg. Training loss: 2.8759 0.0168 sec/batch\n",
      "Global Step: 69700 Epoch 30/50 Iteration: 69700 Avg. Training loss: 2.8965 0.0163 sec/batch\n",
      "Global Step: 69800 Epoch 30/50 Iteration: 69800 Avg. Training loss: 2.8986 0.0151 sec/batch\n",
      "Global Step: 69900 Epoch 30/50 Iteration: 69900 Avg. Training loss: 2.8659 0.0150 sec/batch\n",
      "Global Step: 70000 Epoch 30/50 Iteration: 70000 Avg. Training loss: 2.8674 0.0157 sec/batch\n",
      "Global Step: 70100 Epoch 30/50 Iteration: 70100 Avg. Training loss: 2.8952 0.0150 sec/batch\n",
      "Global Step: 70200 Epoch 30/50 Iteration: 70200 Avg. Training loss: 2.8774 0.0138 sec/batch\n",
      "Global Step: 70300 Epoch 30/50 Iteration: 70300 Avg. Training loss: 2.8865 0.0144 sec/batch\n",
      "Global Step: 70400 Epoch 30/50 Iteration: 70400 Avg. Training loss: 2.8862 0.0161 sec/batch\n",
      "Global Step: 70500 Epoch 30/50 Iteration: 70500 Avg. Training loss: 2.8653 0.0143 sec/batch\n",
      "Global Step: 70600 Epoch 30/50 Iteration: 70600 Avg. Training loss: 2.9084 0.0151 sec/batch\n",
      "Global Step: 70700 Epoch 30/50 Iteration: 70700 Avg. Training loss: 2.8830 0.0147 sec/batch\n",
      "Global Step: 70800 Epoch 30/50 Iteration: 70800 Avg. Training loss: 2.9020 0.0159 sec/batch\n",
      "Global Step: 70900 Epoch 30/50 Iteration: 70900 Avg. Training loss: 2.8643 0.0157 sec/batch\n",
      "Global Step: 71000 Epoch 30/50 Iteration: 71000 Avg. Training loss: 2.8571 0.0135 sec/batch\n",
      "Global Step: 71100 Epoch 30/50 Iteration: 71100 Avg. Training loss: 2.8914 0.0143 sec/batch\n",
      "Global Step: 71200 Epoch 30/50 Iteration: 71200 Avg. Training loss: 2.8760 0.0159 sec/batch\n",
      "Epoch 31/50 Threshold: 0.060578662616402365 Length of Training words: 2234747\n",
      "Global Step: 71300 Epoch 31/50 Iteration: 71300 Avg. Training loss: 2.9042 0.0133 sec/batch\n",
      "Global Step: 71400 Epoch 31/50 Iteration: 71400 Avg. Training loss: 2.9262 0.0137 sec/batch\n",
      "Global Step: 71500 Epoch 31/50 Iteration: 71500 Avg. Training loss: 2.9054 0.0148 sec/batch\n",
      "Global Step: 71600 Epoch 31/50 Iteration: 71600 Avg. Training loss: 2.8830 0.0137 sec/batch\n",
      "Global Step: 71700 Epoch 31/50 Iteration: 71700 Avg. Training loss: 2.8938 0.0169 sec/batch\n",
      "Global Step: 71800 Epoch 31/50 Iteration: 71800 Avg. Training loss: 2.8912 0.0160 sec/batch\n",
      "Global Step: 71900 Epoch 31/50 Iteration: 71900 Avg. Training loss: 2.9045 0.0142 sec/batch\n",
      "Global Step: 72000 Epoch 31/50 Iteration: 72000 Avg. Training loss: 2.8956 0.0136 sec/batch\n",
      "Global Step: 72100 Epoch 31/50 Iteration: 72100 Avg. Training loss: 2.9144 0.0149 sec/batch\n",
      "Global Step: 72200 Epoch 31/50 Iteration: 72200 Avg. Training loss: 2.8830 0.0165 sec/batch\n",
      "Global Step: 72300 Epoch 31/50 Iteration: 72300 Avg. Training loss: 2.8844 0.0176 sec/batch\n",
      "Global Step: 72400 Epoch 31/50 Iteration: 72400 Avg. Training loss: 2.8942 0.0157 sec/batch\n",
      "Global Step: 72500 Epoch 31/50 Iteration: 72500 Avg. Training loss: 2.8993 0.0172 sec/batch\n",
      "Global Step: 72600 Epoch 31/50 Iteration: 72600 Avg. Training loss: 2.8914 0.0152 sec/batch\n",
      "Global Step: 72700 Epoch 31/50 Iteration: 72700 Avg. Training loss: 2.9023 0.0141 sec/batch\n",
      "Global Step: 72800 Epoch 31/50 Iteration: 72800 Avg. Training loss: 2.8960 0.0148 sec/batch\n",
      "Global Step: 72900 Epoch 31/50 Iteration: 72900 Avg. Training loss: 2.9102 0.0161 sec/batch\n",
      "Global Step: 73000 Epoch 31/50 Iteration: 73000 Avg. Training loss: 2.9006 0.0173 sec/batch\n",
      "Global Step: 73100 Epoch 31/50 Iteration: 73100 Avg. Training loss: 2.9065 0.0148 sec/batch\n",
      "Global Step: 73200 Epoch 31/50 Iteration: 73200 Avg. Training loss: 2.8709 0.0145 sec/batch\n",
      "Global Step: 73300 Epoch 31/50 Iteration: 73300 Avg. Training loss: 2.8839 0.0138 sec/batch\n",
      "Global Step: 73400 Epoch 31/50 Iteration: 73400 Avg. Training loss: 2.8975 0.0167 sec/batch\n",
      "Epoch 32/50 Threshold: 0.0802312071663469 Length of Training words: 2387534\n",
      "Global Step: 73500 Epoch 32/50 Iteration: 73500 Avg. Training loss: 2.8713 0.0091 sec/batch\n",
      "Global Step: 73600 Epoch 32/50 Iteration: 73600 Avg. Training loss: 2.8711 0.0160 sec/batch\n",
      "Global Step: 73700 Epoch 32/50 Iteration: 73700 Avg. Training loss: 2.8705 0.0147 sec/batch\n",
      "Global Step: 73800 Epoch 32/50 Iteration: 73800 Avg. Training loss: 2.8468 0.0126 sec/batch\n",
      "Global Step: 73900 Epoch 32/50 Iteration: 73900 Avg. Training loss: 2.8437 0.0126 sec/batch\n",
      "Global Step: 74000 Epoch 32/50 Iteration: 74000 Avg. Training loss: 2.8388 0.0149 sec/batch\n",
      "Global Step: 74100 Epoch 32/50 Iteration: 74100 Avg. Training loss: 2.8522 0.0127 sec/batch\n",
      "Global Step: 74200 Epoch 32/50 Iteration: 74200 Avg. Training loss: 2.8525 0.0155 sec/batch\n",
      "Global Step: 74300 Epoch 32/50 Iteration: 74300 Avg. Training loss: 2.8508 0.0142 sec/batch\n",
      "Global Step: 74400 Epoch 32/50 Iteration: 74400 Avg. Training loss: 2.8595 0.0144 sec/batch\n",
      "Global Step: 74500 Epoch 32/50 Iteration: 74500 Avg. Training loss: 2.8337 0.0141 sec/batch\n",
      "Global Step: 74600 Epoch 32/50 Iteration: 74600 Avg. Training loss: 2.8405 0.0152 sec/batch\n",
      "Global Step: 74700 Epoch 32/50 Iteration: 74700 Avg. Training loss: 2.8522 0.0141 sec/batch\n",
      "Global Step: 74800 Epoch 32/50 Iteration: 74800 Avg. Training loss: 2.8450 0.0175 sec/batch\n",
      "Global Step: 74900 Epoch 32/50 Iteration: 74900 Avg. Training loss: 2.8539 0.0170 sec/batch\n",
      "Global Step: 75000 Epoch 32/50 Iteration: 75000 Avg. Training loss: 2.8420 0.0170 sec/batch\n",
      "Global Step: 75100 Epoch 32/50 Iteration: 75100 Avg. Training loss: 2.8333 0.0126 sec/batch\n",
      "Global Step: 75200 Epoch 32/50 Iteration: 75200 Avg. Training loss: 2.8720 0.0178 sec/batch\n",
      "Global Step: 75300 Epoch 32/50 Iteration: 75300 Avg. Training loss: 2.8506 0.0153 sec/batch\n",
      "Global Step: 75400 Epoch 32/50 Iteration: 75400 Avg. Training loss: 2.8680 0.0153 sec/batch\n",
      "Global Step: 75500 Epoch 32/50 Iteration: 75500 Avg. Training loss: 2.8328 0.0161 sec/batch\n",
      "Global Step: 75600 Epoch 32/50 Iteration: 75600 Avg. Training loss: 2.8223 0.0147 sec/batch\n",
      "Global Step: 75700 Epoch 32/50 Iteration: 75700 Avg. Training loss: 2.8545 0.0159 sec/batch\n",
      "Global Step: 75800 Epoch 32/50 Iteration: 75800 Avg. Training loss: 2.8407 0.0147 sec/batch\n",
      "Epoch 33/50 Threshold: 0.06324289916713363 Length of Training words: 2258450\n",
      "Global Step: 75900 Epoch 33/50 Iteration: 75900 Avg. Training loss: 2.8857 0.0120 sec/batch\n",
      "Global Step: 76000 Epoch 33/50 Iteration: 76000 Avg. Training loss: 2.9106 0.0153 sec/batch\n",
      "Global Step: 76100 Epoch 33/50 Iteration: 76100 Avg. Training loss: 2.9095 0.0159 sec/batch\n",
      "Global Step: 76200 Epoch 33/50 Iteration: 76200 Avg. Training loss: 2.8801 0.0162 sec/batch\n",
      "Global Step: 76300 Epoch 33/50 Iteration: 76300 Avg. Training loss: 2.8828 0.0148 sec/batch\n",
      "Global Step: 76400 Epoch 33/50 Iteration: 76400 Avg. Training loss: 2.8858 0.0148 sec/batch\n",
      "Global Step: 76500 Epoch 33/50 Iteration: 76500 Avg. Training loss: 2.8860 0.0133 sec/batch\n",
      "Global Step: 76600 Epoch 33/50 Iteration: 76600 Avg. Training loss: 2.8963 0.0137 sec/batch\n",
      "Global Step: 76700 Epoch 33/50 Iteration: 76700 Avg. Training loss: 2.9017 0.0129 sec/batch\n",
      "Global Step: 76800 Epoch 33/50 Iteration: 76800 Avg. Training loss: 2.8796 0.0135 sec/batch\n",
      "Global Step: 76900 Epoch 33/50 Iteration: 76900 Avg. Training loss: 2.8701 0.0151 sec/batch\n",
      "Global Step: 77000 Epoch 33/50 Iteration: 77000 Avg. Training loss: 2.9061 0.0164 sec/batch\n",
      "Global Step: 77100 Epoch 33/50 Iteration: 77100 Avg. Training loss: 2.8756 0.0163 sec/batch\n",
      "Global Step: 77200 Epoch 33/50 Iteration: 77200 Avg. Training loss: 2.8963 0.0188 sec/batch\n",
      "Global Step: 77300 Epoch 33/50 Iteration: 77300 Avg. Training loss: 2.8865 0.0152 sec/batch\n",
      "Global Step: 77400 Epoch 33/50 Iteration: 77400 Avg. Training loss: 2.8772 0.0142 sec/batch\n",
      "Global Step: 77500 Epoch 33/50 Iteration: 77500 Avg. Training loss: 2.9122 0.0154 sec/batch\n",
      "Global Step: 77600 Epoch 33/50 Iteration: 77600 Avg. Training loss: 2.8915 0.0157 sec/batch\n",
      "Global Step: 77700 Epoch 33/50 Iteration: 77700 Avg. Training loss: 2.9027 0.0142 sec/batch\n",
      "Global Step: 77800 Epoch 33/50 Iteration: 77800 Avg. Training loss: 2.8703 0.0156 sec/batch\n",
      "Global Step: 77900 Epoch 33/50 Iteration: 77900 Avg. Training loss: 2.8723 0.0163 sec/batch\n",
      "Global Step: 78000 Epoch 33/50 Iteration: 78000 Avg. Training loss: 2.8794 0.0150 sec/batch\n",
      "Epoch 34/50 Threshold: 0.07746417489937713 Length of Training words: 2369277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 78100 Epoch 34/50 Iteration: 78100 Avg. Training loss: 2.8833 0.0020 sec/batch\n",
      "Global Step: 78200 Epoch 34/50 Iteration: 78200 Avg. Training loss: 2.8695 0.0169 sec/batch\n",
      "Global Step: 78300 Epoch 34/50 Iteration: 78300 Avg. Training loss: 2.8849 0.0144 sec/batch\n",
      "Global Step: 78400 Epoch 34/50 Iteration: 78400 Avg. Training loss: 2.8621 0.0124 sec/batch\n",
      "Global Step: 78500 Epoch 34/50 Iteration: 78500 Avg. Training loss: 2.8413 0.0143 sec/batch\n",
      "Global Step: 78600 Epoch 34/50 Iteration: 78600 Avg. Training loss: 2.8482 0.0140 sec/batch\n",
      "Global Step: 78700 Epoch 34/50 Iteration: 78700 Avg. Training loss: 2.8491 0.0151 sec/batch\n",
      "Global Step: 78800 Epoch 34/50 Iteration: 78800 Avg. Training loss: 2.8581 0.0144 sec/batch\n",
      "Global Step: 78900 Epoch 34/50 Iteration: 78900 Avg. Training loss: 2.8598 0.0135 sec/batch\n",
      "Global Step: 79000 Epoch 34/50 Iteration: 79000 Avg. Training loss: 2.8737 0.0139 sec/batch\n",
      "Global Step: 79100 Epoch 34/50 Iteration: 79100 Avg. Training loss: 2.8411 0.0155 sec/batch\n",
      "Global Step: 79200 Epoch 34/50 Iteration: 79200 Avg. Training loss: 2.8360 0.0127 sec/batch\n",
      "Global Step: 79300 Epoch 34/50 Iteration: 79300 Avg. Training loss: 2.8640 0.0158 sec/batch\n",
      "Global Step: 79400 Epoch 34/50 Iteration: 79400 Avg. Training loss: 2.8485 0.0161 sec/batch\n",
      "Global Step: 79500 Epoch 34/50 Iteration: 79500 Avg. Training loss: 2.8551 0.0152 sec/batch\n",
      "Global Step: 79600 Epoch 34/50 Iteration: 79600 Avg. Training loss: 2.8634 0.0156 sec/batch\n",
      "Global Step: 79700 Epoch 34/50 Iteration: 79700 Avg. Training loss: 2.8329 0.0141 sec/batch\n",
      "Global Step: 79800 Epoch 34/50 Iteration: 79800 Avg. Training loss: 2.8742 0.0139 sec/batch\n",
      "Global Step: 79900 Epoch 34/50 Iteration: 79900 Avg. Training loss: 2.8585 0.0163 sec/batch\n",
      "Global Step: 80000 Epoch 34/50 Iteration: 80000 Avg. Training loss: 2.8706 0.0178 sec/batch\n",
      "Global Step: 80100 Epoch 34/50 Iteration: 80100 Avg. Training loss: 2.8558 0.0120 sec/batch\n",
      "Global Step: 80200 Epoch 34/50 Iteration: 80200 Avg. Training loss: 2.8265 0.0132 sec/batch\n",
      "Global Step: 80300 Epoch 34/50 Iteration: 80300 Avg. Training loss: 2.8422 0.0160 sec/batch\n",
      "Global Step: 80400 Epoch 34/50 Iteration: 80400 Avg. Training loss: 2.8493 0.0157 sec/batch\n",
      "Epoch 35/50 Threshold: 0.08898776615666429 Length of Training words: 2440799\n",
      "Global Step: 80500 Epoch 35/50 Iteration: 80500 Avg. Training loss: 2.8503 0.0068 sec/batch\n",
      "Global Step: 80600 Epoch 35/50 Iteration: 80600 Avg. Training loss: 2.8414 0.0151 sec/batch\n",
      "Global Step: 80700 Epoch 35/50 Iteration: 80700 Avg. Training loss: 2.8569 0.0150 sec/batch\n",
      "Global Step: 80800 Epoch 35/50 Iteration: 80800 Avg. Training loss: 2.8380 0.0153 sec/batch\n",
      "Global Step: 80900 Epoch 35/50 Iteration: 80900 Avg. Training loss: 2.8241 0.0176 sec/batch\n",
      "Global Step: 81000 Epoch 35/50 Iteration: 81000 Avg. Training loss: 2.8170 0.0160 sec/batch\n",
      "Global Step: 81100 Epoch 35/50 Iteration: 81100 Avg. Training loss: 2.8271 0.0163 sec/batch\n",
      "Global Step: 81200 Epoch 35/50 Iteration: 81200 Avg. Training loss: 2.8338 0.0138 sec/batch\n",
      "Global Step: 81300 Epoch 35/50 Iteration: 81300 Avg. Training loss: 2.8322 0.0136 sec/batch\n",
      "Global Step: 81400 Epoch 35/50 Iteration: 81400 Avg. Training loss: 2.8520 0.0137 sec/batch\n",
      "Global Step: 81500 Epoch 35/50 Iteration: 81500 Avg. Training loss: 2.8154 0.0151 sec/batch\n",
      "Global Step: 81600 Epoch 35/50 Iteration: 81600 Avg. Training loss: 2.8102 0.0130 sec/batch\n",
      "Global Step: 81700 Epoch 35/50 Iteration: 81700 Avg. Training loss: 2.8400 0.0128 sec/batch\n",
      "Global Step: 81800 Epoch 35/50 Iteration: 81800 Avg. Training loss: 2.8253 0.0143 sec/batch\n",
      "Global Step: 81900 Epoch 35/50 Iteration: 81900 Avg. Training loss: 2.8221 0.0161 sec/batch\n",
      "Global Step: 82000 Epoch 35/50 Iteration: 82000 Avg. Training loss: 2.8401 0.0177 sec/batch\n",
      "Global Step: 82100 Epoch 35/50 Iteration: 82100 Avg. Training loss: 2.8228 0.0163 sec/batch\n",
      "Global Step: 82200 Epoch 35/50 Iteration: 82200 Avg. Training loss: 2.8296 0.0148 sec/batch\n",
      "Global Step: 82300 Epoch 35/50 Iteration: 82300 Avg. Training loss: 2.8532 0.0159 sec/batch\n",
      "Global Step: 82400 Epoch 35/50 Iteration: 82400 Avg. Training loss: 2.8319 0.0166 sec/batch\n",
      "Global Step: 82500 Epoch 35/50 Iteration: 82500 Avg. Training loss: 2.8408 0.0160 sec/batch\n",
      "Global Step: 82600 Epoch 35/50 Iteration: 82600 Avg. Training loss: 2.8031 0.0148 sec/batch\n",
      "Global Step: 82700 Epoch 35/50 Iteration: 82700 Avg. Training loss: 2.8184 0.0164 sec/batch\n",
      "Global Step: 82800 Epoch 35/50 Iteration: 82800 Avg. Training loss: 2.8197 0.0151 sec/batch\n",
      "Epoch 36/50 Threshold: 0.07534745402699021 Length of Training words: 2355673\n",
      "Global Step: 82900 Epoch 36/50 Iteration: 82900 Avg. Training loss: 2.8268 0.0005 sec/batch\n",
      "Global Step: 83000 Epoch 36/50 Iteration: 83000 Avg. Training loss: 2.8803 0.0144 sec/batch\n",
      "Global Step: 83100 Epoch 36/50 Iteration: 83100 Avg. Training loss: 2.8871 0.0165 sec/batch\n",
      "Global Step: 83200 Epoch 36/50 Iteration: 83200 Avg. Training loss: 2.8660 0.0155 sec/batch\n",
      "Global Step: 83300 Epoch 36/50 Iteration: 83300 Avg. Training loss: 2.8487 0.0161 sec/batch\n",
      "Global Step: 83400 Epoch 36/50 Iteration: 83400 Avg. Training loss: 2.8498 0.0166 sec/batch\n",
      "Global Step: 83500 Epoch 36/50 Iteration: 83500 Avg. Training loss: 2.8536 0.0141 sec/batch\n",
      "Global Step: 83600 Epoch 36/50 Iteration: 83600 Avg. Training loss: 2.8602 0.0123 sec/batch\n",
      "Global Step: 83700 Epoch 36/50 Iteration: 83700 Avg. Training loss: 2.8686 0.0147 sec/batch\n",
      "Global Step: 83800 Epoch 36/50 Iteration: 83800 Avg. Training loss: 2.8764 0.0137 sec/batch\n",
      "Global Step: 83900 Epoch 36/50 Iteration: 83900 Avg. Training loss: 2.8483 0.0134 sec/batch\n",
      "Global Step: 84000 Epoch 36/50 Iteration: 84000 Avg. Training loss: 2.8404 0.0154 sec/batch\n",
      "Global Step: 84100 Epoch 36/50 Iteration: 84100 Avg. Training loss: 2.8691 0.0132 sec/batch\n",
      "Global Step: 84200 Epoch 36/50 Iteration: 84200 Avg. Training loss: 2.8506 0.0136 sec/batch\n",
      "Global Step: 84300 Epoch 36/50 Iteration: 84300 Avg. Training loss: 2.8566 0.0149 sec/batch\n",
      "Global Step: 84400 Epoch 36/50 Iteration: 84400 Avg. Training loss: 2.8662 0.0173 sec/batch\n",
      "Global Step: 84500 Epoch 36/50 Iteration: 84500 Avg. Training loss: 2.8357 0.0162 sec/batch\n",
      "Global Step: 84600 Epoch 36/50 Iteration: 84600 Avg. Training loss: 2.8759 0.0141 sec/batch\n",
      "Global Step: 84700 Epoch 36/50 Iteration: 84700 Avg. Training loss: 2.8624 0.0146 sec/batch\n",
      "Global Step: 84800 Epoch 36/50 Iteration: 84800 Avg. Training loss: 2.8741 0.0152 sec/batch\n",
      "Global Step: 84900 Epoch 36/50 Iteration: 84900 Avg. Training loss: 2.8591 0.0141 sec/batch\n",
      "Global Step: 85000 Epoch 36/50 Iteration: 85000 Avg. Training loss: 2.8377 0.0164 sec/batch\n",
      "Global Step: 85100 Epoch 36/50 Iteration: 85100 Avg. Training loss: 2.8441 0.0161 sec/batch\n",
      "Global Step: 85200 Epoch 36/50 Iteration: 85200 Avg. Training loss: 2.8583 0.0158 sec/batch\n",
      "Epoch 37/50 Threshold: 0.08809174339175921 Length of Training words: 2435024\n",
      "Global Step: 85300 Epoch 37/50 Iteration: 85300 Avg. Training loss: 2.8461 0.0072 sec/batch\n",
      "Global Step: 85400 Epoch 37/50 Iteration: 85400 Avg. Training loss: 2.8475 0.0144 sec/batch\n",
      "Global Step: 85500 Epoch 37/50 Iteration: 85500 Avg. Training loss: 2.8557 0.0151 sec/batch\n",
      "Global Step: 85600 Epoch 37/50 Iteration: 85600 Avg. Training loss: 2.8360 0.0150 sec/batch\n",
      "Global Step: 85700 Epoch 37/50 Iteration: 85700 Avg. Training loss: 2.8275 0.0140 sec/batch\n",
      "Global Step: 85800 Epoch 37/50 Iteration: 85800 Avg. Training loss: 2.8168 0.0154 sec/batch\n",
      "Global Step: 85900 Epoch 37/50 Iteration: 85900 Avg. Training loss: 2.8360 0.0179 sec/batch\n",
      "Global Step: 86000 Epoch 37/50 Iteration: 86000 Avg. Training loss: 2.8351 0.0162 sec/batch\n",
      "Global Step: 86100 Epoch 37/50 Iteration: 86100 Avg. Training loss: 2.8330 0.0163 sec/batch\n",
      "Global Step: 86200 Epoch 37/50 Iteration: 86200 Avg. Training loss: 2.8584 0.0152 sec/batch\n",
      "Global Step: 86300 Epoch 37/50 Iteration: 86300 Avg. Training loss: 2.8175 0.0149 sec/batch\n",
      "Global Step: 86400 Epoch 37/50 Iteration: 86400 Avg. Training loss: 2.8073 0.0147 sec/batch\n",
      "Global Step: 86500 Epoch 37/50 Iteration: 86500 Avg. Training loss: 2.8441 0.0155 sec/batch\n",
      "Global Step: 86600 Epoch 37/50 Iteration: 86600 Avg. Training loss: 2.8234 0.0148 sec/batch\n",
      "Global Step: 86700 Epoch 37/50 Iteration: 86700 Avg. Training loss: 2.8302 0.0146 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 86800 Epoch 37/50 Iteration: 86800 Avg. Training loss: 2.8378 0.0138 sec/batch\n",
      "Global Step: 86900 Epoch 37/50 Iteration: 86900 Avg. Training loss: 2.8186 0.0152 sec/batch\n",
      "Global Step: 87000 Epoch 37/50 Iteration: 87000 Avg. Training loss: 2.8400 0.0166 sec/batch\n",
      "Global Step: 87100 Epoch 37/50 Iteration: 87100 Avg. Training loss: 2.8483 0.0149 sec/batch\n",
      "Global Step: 87200 Epoch 37/50 Iteration: 87200 Avg. Training loss: 2.8330 0.0185 sec/batch\n",
      "Global Step: 87300 Epoch 37/50 Iteration: 87300 Avg. Training loss: 2.8375 0.0140 sec/batch\n",
      "Global Step: 87400 Epoch 37/50 Iteration: 87400 Avg. Training loss: 2.8104 0.0163 sec/batch\n",
      "Global Step: 87500 Epoch 37/50 Iteration: 87500 Avg. Training loss: 2.8165 0.0159 sec/batch\n",
      "Global Step: 87600 Epoch 37/50 Iteration: 87600 Avg. Training loss: 2.8289 0.0159 sec/batch\n",
      "Epoch 38/50 Threshold: 0.07277438964797873 Length of Training words: 2336721\n",
      "Global Step: 87700 Epoch 38/50 Iteration: 87700 Avg. Training loss: 2.8265 0.0021 sec/batch\n",
      "Global Step: 87800 Epoch 38/50 Iteration: 87800 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 87900 Epoch 38/50 Iteration: 87900 Avg. Training loss: 2.8937 0.0167 sec/batch\n",
      "Global Step: 88000 Epoch 38/50 Iteration: 88000 Avg. Training loss: 2.8696 0.0152 sec/batch\n",
      "Global Step: 88100 Epoch 38/50 Iteration: 88100 Avg. Training loss: 2.8591 0.0154 sec/batch\n",
      "Global Step: 88200 Epoch 38/50 Iteration: 88200 Avg. Training loss: 2.8545 0.0150 sec/batch\n",
      "Global Step: 88300 Epoch 38/50 Iteration: 88300 Avg. Training loss: 2.8603 0.0136 sec/batch\n",
      "Global Step: 88400 Epoch 38/50 Iteration: 88400 Avg. Training loss: 2.8678 0.0148 sec/batch\n",
      "Global Step: 88500 Epoch 38/50 Iteration: 88500 Avg. Training loss: 2.8669 0.0148 sec/batch\n",
      "Global Step: 88600 Epoch 38/50 Iteration: 88600 Avg. Training loss: 2.8888 0.0155 sec/batch\n",
      "Global Step: 88700 Epoch 38/50 Iteration: 88700 Avg. Training loss: 2.8488 0.0144 sec/batch\n",
      "Global Step: 88800 Epoch 38/50 Iteration: 88800 Avg. Training loss: 2.8481 0.0158 sec/batch\n",
      "Global Step: 88900 Epoch 38/50 Iteration: 88900 Avg. Training loss: 2.8802 0.0144 sec/batch\n",
      "Global Step: 89000 Epoch 38/50 Iteration: 89000 Avg. Training loss: 2.8504 0.0139 sec/batch\n",
      "Global Step: 89100 Epoch 38/50 Iteration: 89100 Avg. Training loss: 2.8718 0.0139 sec/batch\n",
      "Global Step: 89200 Epoch 38/50 Iteration: 89200 Avg. Training loss: 2.8640 0.0132 sec/batch\n",
      "Global Step: 89300 Epoch 38/50 Iteration: 89300 Avg. Training loss: 2.8444 0.0161 sec/batch\n",
      "Global Step: 89400 Epoch 38/50 Iteration: 89400 Avg. Training loss: 2.8877 0.0164 sec/batch\n",
      "Global Step: 89500 Epoch 38/50 Iteration: 89500 Avg. Training loss: 2.8663 0.0166 sec/batch\n",
      "Global Step: 89600 Epoch 38/50 Iteration: 89600 Avg. Training loss: 2.8782 0.0177 sec/batch\n",
      "Global Step: 89700 Epoch 38/50 Iteration: 89700 Avg. Training loss: 2.8529 0.0159 sec/batch\n",
      "Global Step: 89800 Epoch 38/50 Iteration: 89800 Avg. Training loss: 2.8370 0.0167 sec/batch\n",
      "Global Step: 89900 Epoch 38/50 Iteration: 89900 Avg. Training loss: 2.8713 0.0162 sec/batch\n",
      "Global Step: 90000 Epoch 38/50 Iteration: 90000 Avg. Training loss: 2.8581 0.0159 sec/batch\n",
      "Epoch 39/50 Threshold: 0.086121294696839 Length of Training words: 2423215\n",
      "Global Step: 90100 Epoch 39/50 Iteration: 90100 Avg. Training loss: 2.8511 0.0120 sec/batch\n",
      "Global Step: 90200 Epoch 39/50 Iteration: 90200 Avg. Training loss: 2.8597 0.0154 sec/batch\n",
      "Global Step: 90300 Epoch 39/50 Iteration: 90300 Avg. Training loss: 2.8542 0.0154 sec/batch\n",
      "Global Step: 90400 Epoch 39/50 Iteration: 90400 Avg. Training loss: 2.8285 0.0158 sec/batch\n",
      "Global Step: 90500 Epoch 39/50 Iteration: 90500 Avg. Training loss: 2.8313 0.0158 sec/batch\n",
      "Global Step: 90600 Epoch 39/50 Iteration: 90600 Avg. Training loss: 2.8206 0.0167 sec/batch\n",
      "Global Step: 90700 Epoch 39/50 Iteration: 90700 Avg. Training loss: 2.8388 0.0134 sec/batch\n",
      "Global Step: 90800 Epoch 39/50 Iteration: 90800 Avg. Training loss: 2.8449 0.0150 sec/batch\n",
      "Global Step: 90900 Epoch 39/50 Iteration: 90900 Avg. Training loss: 2.8452 0.0154 sec/batch\n",
      "Global Step: 91000 Epoch 39/50 Iteration: 91000 Avg. Training loss: 2.8437 0.0156 sec/batch\n",
      "Global Step: 91100 Epoch 39/50 Iteration: 91100 Avg. Training loss: 2.8218 0.0153 sec/batch\n",
      "Global Step: 91200 Epoch 39/50 Iteration: 91200 Avg. Training loss: 2.8275 0.0127 sec/batch\n",
      "Global Step: 91300 Epoch 39/50 Iteration: 91300 Avg. Training loss: 2.8395 0.0145 sec/batch\n",
      "Global Step: 91400 Epoch 39/50 Iteration: 91400 Avg. Training loss: 2.8329 0.0142 sec/batch\n",
      "Global Step: 91500 Epoch 39/50 Iteration: 91500 Avg. Training loss: 2.8406 0.0128 sec/batch\n",
      "Global Step: 91600 Epoch 39/50 Iteration: 91600 Avg. Training loss: 2.8300 0.0147 sec/batch\n",
      "Global Step: 91700 Epoch 39/50 Iteration: 91700 Avg. Training loss: 2.8212 0.0161 sec/batch\n",
      "Global Step: 91800 Epoch 39/50 Iteration: 91800 Avg. Training loss: 2.8601 0.0151 sec/batch\n",
      "Global Step: 91900 Epoch 39/50 Iteration: 91900 Avg. Training loss: 2.8365 0.0163 sec/batch\n",
      "Global Step: 92000 Epoch 39/50 Iteration: 92000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 92100 Epoch 39/50 Iteration: 92100 Avg. Training loss: 2.8259 0.0174 sec/batch\n",
      "Global Step: 92200 Epoch 39/50 Iteration: 92200 Avg. Training loss: 2.8148 0.0173 sec/batch\n",
      "Global Step: 92300 Epoch 39/50 Iteration: 92300 Avg. Training loss: 2.8339 0.0164 sec/batch\n",
      "Global Step: 92400 Epoch 39/50 Iteration: 92400 Avg. Training loss: 2.8303 0.0153 sec/batch\n",
      "Epoch 40/50 Threshold: 0.07775738919778097 Length of Training words: 2370779\n",
      "Global Step: 92500 Epoch 40/50 Iteration: 92500 Avg. Training loss: 2.8442 0.0083 sec/batch\n",
      "Global Step: 92600 Epoch 40/50 Iteration: 92600 Avg. Training loss: 2.8734 0.0159 sec/batch\n",
      "Global Step: 92700 Epoch 40/50 Iteration: 92700 Avg. Training loss: 2.8727 0.0155 sec/batch\n",
      "Global Step: 92800 Epoch 40/50 Iteration: 92800 Avg. Training loss: 2.8527 0.0161 sec/batch\n",
      "Global Step: 92900 Epoch 40/50 Iteration: 92900 Avg. Training loss: 2.8473 0.0153 sec/batch\n",
      "Global Step: 93000 Epoch 40/50 Iteration: 93000 Avg. Training loss: 2.8441 0.0145 sec/batch\n",
      "Global Step: 93100 Epoch 40/50 Iteration: 93100 Avg. Training loss: 2.8565 0.0141 sec/batch\n",
      "Global Step: 93200 Epoch 40/50 Iteration: 93200 Avg. Training loss: 2.8573 0.0162 sec/batch\n",
      "Global Step: 93300 Epoch 40/50 Iteration: 93300 Avg. Training loss: 2.8574 0.0136 sec/batch\n",
      "Global Step: 93400 Epoch 40/50 Iteration: 93400 Avg. Training loss: 2.8654 0.0169 sec/batch\n",
      "Global Step: 93500 Epoch 40/50 Iteration: 93500 Avg. Training loss: 2.8369 0.0162 sec/batch\n",
      "Global Step: 93600 Epoch 40/50 Iteration: 93600 Avg. Training loss: 2.8472 0.0164 sec/batch\n",
      "Global Step: 93700 Epoch 40/50 Iteration: 93700 Avg. Training loss: 2.8550 0.0155 sec/batch\n",
      "Global Step: 93800 Epoch 40/50 Iteration: 93800 Avg. Training loss: 2.8572 0.0164 sec/batch\n",
      "Global Step: 93900 Epoch 40/50 Iteration: 93900 Avg. Training loss: 2.8508 0.0150 sec/batch\n",
      "Global Step: 94000 Epoch 40/50 Iteration: 94000 Avg. Training loss: 2.8506 0.0128 sec/batch\n",
      "Global Step: 94100 Epoch 40/50 Iteration: 94100 Avg. Training loss: 2.8424 0.0131 sec/batch\n",
      "Global Step: 94200 Epoch 40/50 Iteration: 94200 Avg. Training loss: 2.8757 0.0172 sec/batch\n",
      "Global Step: 94300 Epoch 40/50 Iteration: 94300 Avg. Training loss: 2.8537 0.0172 sec/batch\n",
      "Global Step: 94400 Epoch 40/50 Iteration: 94400 Avg. Training loss: 2.8676 0.0167 sec/batch\n",
      "Global Step: 94500 Epoch 40/50 Iteration: 94500 Avg. Training loss: 2.8301 0.0161 sec/batch\n",
      "Global Step: 94600 Epoch 40/50 Iteration: 94600 Avg. Training loss: 2.8309 0.0158 sec/batch\n",
      "Global Step: 94700 Epoch 40/50 Iteration: 94700 Avg. Training loss: 2.8634 0.0155 sec/batch\n",
      "Global Step: 94800 Epoch 40/50 Iteration: 94800 Avg. Training loss: 2.8429 0.0156 sec/batch\n",
      "Epoch 41/50 Threshold: 0.06232496225957763 Length of Training words: 2250332\n",
      "Global Step: 94900 Epoch 41/50 Iteration: 94900 Avg. Training loss: 2.8899 0.0141 sec/batch\n",
      "Global Step: 95000 Epoch 41/50 Iteration: 95000 Avg. Training loss: 2.9321 0.0138 sec/batch\n",
      "Global Step: 95100 Epoch 41/50 Iteration: 95100 Avg. Training loss: 2.9010 0.0171 sec/batch\n",
      "Global Step: 95200 Epoch 41/50 Iteration: 95200 Avg. Training loss: 2.8778 0.0167 sec/batch\n",
      "Global Step: 95300 Epoch 41/50 Iteration: 95300 Avg. Training loss: 2.8830 0.0155 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 95400 Epoch 41/50 Iteration: 95400 Avg. Training loss: 2.8868 0.0145 sec/batch\n",
      "Global Step: 95500 Epoch 41/50 Iteration: 95500 Avg. Training loss: 2.8928 0.0157 sec/batch\n",
      "Global Step: 95600 Epoch 41/50 Iteration: 95600 Avg. Training loss: 2.8940 0.0146 sec/batch\n",
      "Global Step: 95700 Epoch 41/50 Iteration: 95700 Avg. Training loss: 2.9165 0.0161 sec/batch\n",
      "Global Step: 95800 Epoch 41/50 Iteration: 95800 Avg. Training loss: 2.8731 0.0170 sec/batch\n",
      "Global Step: 95900 Epoch 41/50 Iteration: 95900 Avg. Training loss: 2.8816 0.0140 sec/batch\n",
      "Global Step: 96000 Epoch 41/50 Iteration: 96000 Avg. Training loss: 2.8972 0.0143 sec/batch\n",
      "Global Step: 96100 Epoch 41/50 Iteration: 96100 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 96200 Epoch 41/50 Iteration: 96200 Avg. Training loss: 2.8846 0.0137 sec/batch\n",
      "Global Step: 96300 Epoch 41/50 Iteration: 96300 Avg. Training loss: 2.8934 0.0146 sec/batch\n",
      "Global Step: 96400 Epoch 41/50 Iteration: 96400 Avg. Training loss: 2.8802 0.0138 sec/batch\n",
      "Global Step: 96500 Epoch 41/50 Iteration: 96500 Avg. Training loss: 2.9145 0.0160 sec/batch\n",
      "Global Step: 96600 Epoch 41/50 Iteration: 96600 Avg. Training loss: 2.8921 0.0145 sec/batch\n",
      "Global Step: 96700 Epoch 41/50 Iteration: 96700 Avg. Training loss: 2.8998 0.0132 sec/batch\n",
      "Global Step: 96800 Epoch 41/50 Iteration: 96800 Avg. Training loss: 2.8715 0.0166 sec/batch\n",
      "Global Step: 96900 Epoch 41/50 Iteration: 96900 Avg. Training loss: 2.8779 0.0163 sec/batch\n",
      "Global Step: 97000 Epoch 41/50 Iteration: 97000 Avg. Training loss: 2.8849 0.0165 sec/batch\n",
      "Epoch 42/50 Threshold: 0.06905878343510341 Length of Training words: 2305711\n",
      "Global Step: 97100 Epoch 42/50 Iteration: 97100 Avg. Training loss: 2.8886 0.0056 sec/batch\n",
      "Global Step: 97200 Epoch 42/50 Iteration: 97200 Avg. Training loss: 2.8876 0.0161 sec/batch\n",
      "Global Step: 97300 Epoch 42/50 Iteration: 97300 Avg. Training loss: 2.9018 0.0153 sec/batch\n",
      "Global Step: 97400 Epoch 42/50 Iteration: 97400 Avg. Training loss: 2.8790 0.0148 sec/batch\n",
      "Global Step: 97500 Epoch 42/50 Iteration: 97500 Avg. Training loss: 2.8686 0.0143 sec/batch\n",
      "Global Step: 97600 Epoch 42/50 Iteration: 97600 Avg. Training loss: 2.8547 0.0159 sec/batch\n",
      "Global Step: 97700 Epoch 42/50 Iteration: 97700 Avg. Training loss: 2.8821 0.0165 sec/batch\n",
      "Global Step: 97800 Epoch 42/50 Iteration: 97800 Avg. Training loss: 2.8767 0.0151 sec/batch\n",
      "Global Step: 97900 Epoch 42/50 Iteration: 97900 Avg. Training loss: 2.8810 0.0187 sec/batch\n",
      "Global Step: 98000 Epoch 42/50 Iteration: 98000 Avg. Training loss: 2.8814 0.0158 sec/batch\n",
      "Global Step: 98100 Epoch 42/50 Iteration: 98100 Avg. Training loss: 2.8569 0.0162 sec/batch\n",
      "Global Step: 98200 Epoch 42/50 Iteration: 98200 Avg. Training loss: 2.8645 0.0189 sec/batch\n",
      "Global Step: 98300 Epoch 42/50 Iteration: 98300 Avg. Training loss: 2.8741 0.0150 sec/batch\n",
      "Global Step: 98400 Epoch 42/50 Iteration: 98400 Avg. Training loss: 2.8784 0.0151 sec/batch\n",
      "Global Step: 98500 Epoch 42/50 Iteration: 98500 Avg. Training loss: 2.8714 0.0166 sec/batch\n",
      "Global Step: 98600 Epoch 42/50 Iteration: 98600 Avg. Training loss: 2.8792 0.0173 sec/batch\n",
      "Global Step: 98700 Epoch 42/50 Iteration: 98700 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 98800 Epoch 42/50 Iteration: 98800 Avg. Training loss: 2.8923 0.0146 sec/batch\n",
      "Global Step: 98900 Epoch 42/50 Iteration: 98900 Avg. Training loss: 2.8740 0.0138 sec/batch\n",
      "Global Step: 99000 Epoch 42/50 Iteration: 99000 Avg. Training loss: 2.8820 0.0144 sec/batch\n",
      "Global Step: 99100 Epoch 42/50 Iteration: 99100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 99200 Epoch 42/50 Iteration: 99200 Avg. Training loss: 2.8596 0.0159 sec/batch\n",
      "Global Step: 99300 Epoch 42/50 Iteration: 99300 Avg. Training loss: 2.8690 0.0170 sec/batch\n",
      "Epoch 43/50 Threshold: 0.06065749365533461 Length of Training words: 2235476\n",
      "Global Step: 99400 Epoch 43/50 Iteration: 99400 Avg. Training loss: 2.8792 0.0034 sec/batch\n",
      "Global Step: 99500 Epoch 43/50 Iteration: 99500 Avg. Training loss: 2.9161 0.0162 sec/batch\n",
      "Global Step: 99600 Epoch 43/50 Iteration: 99600 Avg. Training loss: 2.9233 0.0153 sec/batch\n",
      "Global Step: 99700 Epoch 43/50 Iteration: 99700 Avg. Training loss: 2.8968 0.0151 sec/batch\n",
      "Global Step: 99800 Epoch 43/50 Iteration: 99800 Avg. Training loss: 2.8904 0.0156 sec/batch\n",
      "Global Step: 99900 Epoch 43/50 Iteration: 99900 Avg. Training loss: 2.8856 0.0143 sec/batch\n",
      "Global Step: 100000 Epoch 43/50 Iteration: 100000 Avg. Training loss: 2.8945 0.0159 sec/batch\n",
      "Global Step: 100100 Epoch 43/50 Iteration: 100100 Avg. Training loss: 2.9092 0.0124 sec/batch\n",
      "Global Step: 100200 Epoch 43/50 Iteration: 100200 Avg. Training loss: 2.9013 0.0134 sec/batch\n",
      "Global Step: 100300 Epoch 43/50 Iteration: 100300 Avg. Training loss: 2.8949 0.0157 sec/batch\n",
      "Global Step: 100400 Epoch 43/50 Iteration: 100400 Avg. Training loss: 2.8842 0.0185 sec/batch\n",
      "Global Step: 100500 Epoch 43/50 Iteration: 100500 Avg. Training loss: 2.8981 0.0157 sec/batch\n",
      "Global Step: 100600 Epoch 43/50 Iteration: 100600 Avg. Training loss: 2.8920 0.0155 sec/batch\n",
      "Global Step: 100700 Epoch 43/50 Iteration: 100700 Avg. Training loss: 2.8917 0.0169 sec/batch\n",
      "Global Step: 100800 Epoch 43/50 Iteration: 100800 Avg. Training loss: 2.9001 0.0136 sec/batch\n",
      "Global Step: 100900 Epoch 43/50 Iteration: 100900 Avg. Training loss: 2.8770 0.0158 sec/batch\n",
      "Global Step: 101000 Epoch 43/50 Iteration: 101000 Avg. Training loss: 2.9147 0.0154 sec/batch\n",
      "Global Step: 101100 Epoch 43/50 Iteration: 101100 Avg. Training loss: 2.8994 0.0128 sec/batch\n",
      "Global Step: 101200 Epoch 43/50 Iteration: 101200 Avg. Training loss: 2.9157 0.0151 sec/batch\n",
      "Global Step: 101300 Epoch 43/50 Iteration: 101300 Avg. Training loss: 2.8816 0.0136 sec/batch\n",
      "Global Step: 101400 Epoch 43/50 Iteration: 101400 Avg. Training loss: 2.8681 0.0128 sec/batch\n",
      "Global Step: 101500 Epoch 43/50 Iteration: 101500 Avg. Training loss: 2.8983 0.0150 sec/batch\n",
      "Global Step: 101600 Epoch 43/50 Iteration: 101600 Avg. Training loss: 2.8918 0.0141 sec/batch\n",
      "Epoch 44/50 Threshold: 0.07064468846220132 Length of Training words: 2319882\n",
      "Global Step: 101700 Epoch 44/50 Iteration: 101700 Avg. Training loss: 2.8802 0.0135 sec/batch\n",
      "Global Step: 101800 Epoch 44/50 Iteration: 101800 Avg. Training loss: 2.9037 0.0139 sec/batch\n",
      "Global Step: 101900 Epoch 44/50 Iteration: 101900 Avg. Training loss: 2.8799 0.0148 sec/batch\n",
      "Global Step: 102000 Epoch 44/50 Iteration: 102000 Avg. Training loss: 2.8559 0.0163 sec/batch\n",
      "Global Step: 102100 Epoch 44/50 Iteration: 102100 Avg. Training loss: 2.8637 0.0165 sec/batch\n",
      "Global Step: 102200 Epoch 44/50 Iteration: 102200 Avg. Training loss: 2.8651 0.0165 sec/batch\n",
      "Global Step: 102300 Epoch 44/50 Iteration: 102300 Avg. Training loss: 2.8706 0.0147 sec/batch\n",
      "Global Step: 102400 Epoch 44/50 Iteration: 102400 Avg. Training loss: 2.8747 0.0145 sec/batch\n",
      "Global Step: 102500 Epoch 44/50 Iteration: 102500 Avg. Training loss: 2.8868 0.0137 sec/batch\n",
      "Global Step: 102600 Epoch 44/50 Iteration: 102600 Avg. Training loss: 2.8564 0.0145 sec/batch\n",
      "Global Step: 102700 Epoch 44/50 Iteration: 102700 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 102800 Epoch 44/50 Iteration: 102800 Avg. Training loss: 2.8858 0.0156 sec/batch\n",
      "Global Step: 102900 Epoch 44/50 Iteration: 102900 Avg. Training loss: 2.8561 0.0139 sec/batch\n",
      "Global Step: 103000 Epoch 44/50 Iteration: 103000 Avg. Training loss: 2.8760 0.0147 sec/batch\n",
      "Global Step: 103100 Epoch 44/50 Iteration: 103100 Avg. Training loss: 2.8720 0.0138 sec/batch\n",
      "Global Step: 103200 Epoch 44/50 Iteration: 103200 Avg. Training loss: 2.8514 0.0139 sec/batch\n",
      "Global Step: 103300 Epoch 44/50 Iteration: 103300 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 103400 Epoch 44/50 Iteration: 103400 Avg. Training loss: 2.8709 0.0161 sec/batch\n",
      "Global Step: 103500 Epoch 44/50 Iteration: 103500 Avg. Training loss: 2.8870 0.0152 sec/batch\n",
      "Global Step: 103600 Epoch 44/50 Iteration: 103600 Avg. Training loss: 2.8601 0.0167 sec/batch\n",
      "Global Step: 103700 Epoch 44/50 Iteration: 103700 Avg. Training loss: 2.8425 0.0152 sec/batch\n",
      "Global Step: 103800 Epoch 44/50 Iteration: 103800 Avg. Training loss: 2.8710 0.0137 sec/batch\n",
      "Global Step: 103900 Epoch 44/50 Iteration: 103900 Avg. Training loss: 2.8641 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Threshold: 0.07593991723833231 Length of Training words: 2358860\n",
      "Global Step: 104000 Epoch 45/50 Iteration: 104000 Avg. Training loss: 2.8658 0.0114 sec/batch\n",
      "Global Step: 104100 Epoch 45/50 Iteration: 104100 Avg. Training loss: 2.8793 0.0146 sec/batch\n",
      "Global Step: 104200 Epoch 45/50 Iteration: 104200 Avg. Training loss: 2.8787 0.0137 sec/batch\n",
      "Global Step: 104300 Epoch 45/50 Iteration: 104300 Avg. Training loss: 2.8509 0.0148 sec/batch\n",
      "Global Step: 104400 Epoch 45/50 Iteration: 104400 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 104500 Epoch 45/50 Iteration: 104500 Avg. Training loss: 2.8496 0.0140 sec/batch\n",
      "Global Step: 104600 Epoch 45/50 Iteration: 104600 Avg. Training loss: 2.8555 0.0157 sec/batch\n",
      "Global Step: 104700 Epoch 45/50 Iteration: 104700 Avg. Training loss: 2.8734 0.0144 sec/batch\n",
      "Global Step: 104800 Epoch 45/50 Iteration: 104800 Avg. Training loss: 2.8605 0.0169 sec/batch\n",
      "Global Step: 104900 Epoch 45/50 Iteration: 104900 Avg. Training loss: 2.8560 0.0176 sec/batch\n",
      "Global Step: 105000 Epoch 45/50 Iteration: 105000 Avg. Training loss: 2.8451 0.0167 sec/batch\n",
      "Global Step: 105100 Epoch 45/50 Iteration: 105100 Avg. Training loss: 2.8596 0.0154 sec/batch\n",
      "Global Step: 105200 Epoch 45/50 Iteration: 105200 Avg. Training loss: 2.8482 0.0153 sec/batch\n",
      "Global Step: 105300 Epoch 45/50 Iteration: 105300 Avg. Training loss: 2.8573 0.0152 sec/batch\n",
      "Global Step: 105400 Epoch 45/50 Iteration: 105400 Avg. Training loss: 2.8552 0.0155 sec/batch\n",
      "Global Step: 105500 Epoch 45/50 Iteration: 105500 Avg. Training loss: 2.8594 0.0152 sec/batch\n",
      "Global Step: 105600 Epoch 45/50 Iteration: 105600 Avg. Training loss: 2.8578 0.0136 sec/batch\n",
      "Global Step: 105700 Epoch 45/50 Iteration: 105700 Avg. Training loss: 2.8719 0.0152 sec/batch\n",
      "Global Step: 105800 Epoch 45/50 Iteration: 105800 Avg. Training loss: 2.8567 0.0153 sec/batch\n",
      "Global Step: 105900 Epoch 45/50 Iteration: 105900 Avg. Training loss: 2.8654 0.0128 sec/batch\n",
      "Global Step: 106000 Epoch 45/50 Iteration: 106000 Avg. Training loss: 2.8337 0.0138 sec/batch\n",
      "Global Step: 106100 Epoch 45/50 Iteration: 106100 Avg. Training loss: 2.8447 0.0169 sec/batch\n",
      "Global Step: 106200 Epoch 45/50 Iteration: 106200 Avg. Training loss: 2.8509 0.0156 sec/batch\n",
      "Epoch 46/50 Threshold: 0.07458249463143053 Length of Training words: 2350270\n",
      "Global Step: 106300 Epoch 46/50 Iteration: 106300 Avg. Training loss: 2.8559 0.0024 sec/batch\n",
      "Global Step: 106400 Epoch 46/50 Iteration: 106400 Avg. Training loss: 2.8692 0.0153 sec/batch\n",
      "Global Step: 106500 Epoch 46/50 Iteration: 106500 Avg. Training loss: 2.8916 0.0146 sec/batch\n",
      "Global Step: 106600 Epoch 46/50 Iteration: 106600 Avg. Training loss: 2.8670 0.0152 sec/batch\n",
      "Global Step: 106700 Epoch 46/50 Iteration: 106700 Avg. Training loss: 2.8505 0.0148 sec/batch\n",
      "Global Step: 106800 Epoch 46/50 Iteration: 106800 Avg. Training loss: 2.8512 0.0154 sec/batch\n",
      "Global Step: 106900 Epoch 46/50 Iteration: 106900 Avg. Training loss: 2.8534 0.0144 sec/batch\n",
      "Global Step: 107000 Epoch 46/50 Iteration: 107000 Avg. Training loss: 2.8648 0.0134 sec/batch\n",
      "Global Step: 107100 Epoch 46/50 Iteration: 107100 Avg. Training loss: 2.8622 0.0183 sec/batch\n",
      "Global Step: 107200 Epoch 46/50 Iteration: 107200 Avg. Training loss: 2.8821 0.0146 sec/batch\n",
      "Global Step: 107300 Epoch 46/50 Iteration: 107300 Avg. Training loss: 2.8442 0.0166 sec/batch\n",
      "Global Step: 107400 Epoch 46/50 Iteration: 107400 Avg. Training loss: 2.8414 0.0156 sec/batch\n",
      "Global Step: 107500 Epoch 46/50 Iteration: 107500 Avg. Training loss: 2.8783 0.0160 sec/batch\n",
      "Global Step: 107600 Epoch 46/50 Iteration: 107600 Avg. Training loss: 2.8440 0.0163 sec/batch\n",
      "Global Step: 107700 Epoch 46/50 Iteration: 107700 Avg. Training loss: 2.8680 0.0140 sec/batch\n",
      "Global Step: 107800 Epoch 46/50 Iteration: 107800 Avg. Training loss: 2.8608 0.0161 sec/batch\n",
      "Global Step: 107900 Epoch 46/50 Iteration: 107900 Avg. Training loss: 2.8416 0.0155 sec/batch\n",
      "Global Step: 108000 Epoch 46/50 Iteration: 108000 Avg. Training loss: 2.8843 0.0150 sec/batch\n",
      "Global Step: 108100 Epoch 46/50 Iteration: 108100 Avg. Training loss: 2.8579 0.0149 sec/batch\n",
      "Global Step: 108200 Epoch 46/50 Iteration: 108200 Avg. Training loss: 2.8778 0.0148 sec/batch\n",
      "Global Step: 108300 Epoch 46/50 Iteration: 108300 Avg. Training loss: 2.8508 0.0152 sec/batch\n",
      "Global Step: 108400 Epoch 46/50 Iteration: 108400 Avg. Training loss: 2.8374 0.0154 sec/batch\n",
      "Global Step: 108500 Epoch 46/50 Iteration: 108500 Avg. Training loss: 2.8620 0.0161 sec/batch\n",
      "Global Step: 108600 Epoch 46/50 Iteration: 108600 Avg. Training loss: 2.8523 0.0139 sec/batch\n",
      "Epoch 47/50 Threshold: 0.07293145990513991 Length of Training words: 2337022\n",
      "Global Step: 108700 Epoch 47/50 Iteration: 108700 Avg. Training loss: 2.8708 0.0092 sec/batch\n",
      "Global Step: 108800 Epoch 47/50 Iteration: 108800 Avg. Training loss: 2.8870 0.0129 sec/batch\n",
      "Global Step: 108900 Epoch 47/50 Iteration: 108900 Avg. Training loss: 2.8835 0.0147 sec/batch\n",
      "Global Step: 109000 Epoch 47/50 Iteration: 109000 Avg. Training loss: 2.8593 0.0136 sec/batch\n",
      "Global Step: 109100 Epoch 47/50 Iteration: 109100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 109200 Epoch 47/50 Iteration: 109200 Avg. Training loss: 2.8550 0.0148 sec/batch\n",
      "Global Step: 109300 Epoch 47/50 Iteration: 109300 Avg. Training loss: 2.8643 0.0150 sec/batch\n",
      "Global Step: 109400 Epoch 47/50 Iteration: 109400 Avg. Training loss: 2.8806 0.0133 sec/batch\n",
      "Global Step: 109500 Epoch 47/50 Iteration: 109500 Avg. Training loss: 2.8665 0.0158 sec/batch\n",
      "Global Step: 109600 Epoch 47/50 Iteration: 109600 Avg. Training loss: 2.8622 0.0177 sec/batch\n",
      "Global Step: 109700 Epoch 47/50 Iteration: 109700 Avg. Training loss: 2.8536 0.0176 sec/batch\n",
      "Global Step: 109800 Epoch 47/50 Iteration: 109800 Avg. Training loss: 2.8700 0.0164 sec/batch\n",
      "Global Step: 109900 Epoch 47/50 Iteration: 109900 Avg. Training loss: 2.8559 0.0151 sec/batch\n",
      "Global Step: 110000 Epoch 47/50 Iteration: 110000 Avg. Training loss: 2.8598 0.0162 sec/batch\n",
      "Global Step: 110100 Epoch 47/50 Iteration: 110100 Avg. Training loss: 2.8601 0.0161 sec/batch\n",
      "Global Step: 110200 Epoch 47/50 Iteration: 110200 Avg. Training loss: 2.8660 0.0161 sec/batch\n",
      "Global Step: 110300 Epoch 47/50 Iteration: 110300 Avg. Training loss: 2.8643 0.0162 sec/batch\n",
      "Global Step: 110400 Epoch 47/50 Iteration: 110400 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 110500 Epoch 47/50 Iteration: 110500 Avg. Training loss: 2.8634 0.0154 sec/batch\n",
      "Global Step: 110600 Epoch 47/50 Iteration: 110600 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 110700 Epoch 47/50 Iteration: 110700 Avg. Training loss: 2.8442 0.0165 sec/batch\n",
      "Global Step: 110800 Epoch 47/50 Iteration: 110800 Avg. Training loss: 2.8501 0.0166 sec/batch\n",
      "Global Step: 110900 Epoch 47/50 Iteration: 110900 Avg. Training loss: 2.8555 0.0148 sec/batch\n",
      "Epoch 48/50 Threshold: 0.06985650474361509 Length of Training words: 2312860\n",
      "Global Step: 111000 Epoch 48/50 Iteration: 111000 Avg. Training loss: 2.8629 0.0047 sec/batch\n",
      "Global Step: 111100 Epoch 48/50 Iteration: 111100 Avg. Training loss: 2.8838 0.0139 sec/batch\n",
      "Global Step: 111200 Epoch 48/50 Iteration: 111200 Avg. Training loss: 2.9036 0.0141 sec/batch\n",
      "Global Step: 111300 Epoch 48/50 Iteration: 111300 Avg. Training loss: 2.8771 0.0161 sec/batch\n",
      "Global Step: 111400 Epoch 48/50 Iteration: 111400 Avg. Training loss: 2.8649 0.0138 sec/batch\n",
      "Global Step: 111500 Epoch 48/50 Iteration: 111500 Avg. Training loss: 2.8544 0.0125 sec/batch\n",
      "Global Step: 111600 Epoch 48/50 Iteration: 111600 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 111700 Epoch 48/50 Iteration: 111700 Avg. Training loss: 2.8774 0.0144 sec/batch\n",
      "Global Step: 111800 Epoch 48/50 Iteration: 111800 Avg. Training loss: 2.8724 0.0117 sec/batch\n",
      "Global Step: 111900 Epoch 48/50 Iteration: 111900 Avg. Training loss: 2.8840 0.0141 sec/batch\n",
      "Global Step: 112000 Epoch 48/50 Iteration: 112000 Avg. Training loss: 2.8562 0.0156 sec/batch\n",
      "Global Step: 112100 Epoch 48/50 Iteration: 112100 Avg. Training loss: 2.8637 0.0178 sec/batch\n",
      "Global Step: 112200 Epoch 48/50 Iteration: 112200 Avg. Training loss: 2.8711 0.0175 sec/batch\n",
      "Global Step: 112300 Epoch 48/50 Iteration: 112300 Avg. Training loss: 2.8781 0.0157 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 112400 Epoch 48/50 Iteration: 112400 Avg. Training loss: 2.8706 0.0165 sec/batch\n",
      "Global Step: 112500 Epoch 48/50 Iteration: 112500 Avg. Training loss: 2.8753 0.0185 sec/batch\n",
      "Global Step: 112600 Epoch 48/50 Iteration: 112600 Avg. Training loss: 2.8575 0.0160 sec/batch\n",
      "Global Step: 112700 Epoch 48/50 Iteration: 112700 Avg. Training loss: 2.8922 0.0139 sec/batch\n",
      "Global Step: 112800 Epoch 48/50 Iteration: 112800 Avg. Training loss: 2.8744 0.0159 sec/batch\n",
      "Global Step: 112900 Epoch 48/50 Iteration: 112900 Avg. Training loss: 2.8827 0.0152 sec/batch\n",
      "Global Step: 113000 Epoch 48/50 Iteration: 113000 Avg. Training loss: 2.8446 0.0156 sec/batch\n",
      "Global Step: 113100 Epoch 48/50 Iteration: 113100 Avg. Training loss: 2.8607 0.0175 sec/batch\n",
      "Global Step: 113200 Epoch 48/50 Iteration: 113200 Avg. Training loss: 2.8686 0.0163 sec/batch\n",
      "Epoch 49/50 Threshold: 0.07556541599744111 Length of Training words: 2356231\n",
      "Global Step: 113300 Epoch 49/50 Iteration: 113300 Avg. Training loss: 2.8701 0.0023 sec/batch\n",
      "Global Step: 113400 Epoch 49/50 Iteration: 113400 Avg. Training loss: 2.8662 0.0152 sec/batch\n",
      "Global Step: 113500 Epoch 49/50 Iteration: 113500 Avg. Training loss: 2.8920 0.0169 sec/batch\n",
      "Global Step: 113600 Epoch 49/50 Iteration: 113600 Avg. Training loss: 2.8648 0.0153 sec/batch\n",
      "Global Step: 113700 Epoch 49/50 Iteration: 113700 Avg. Training loss: 2.8487 0.0147 sec/batch\n",
      "Global Step: 113800 Epoch 49/50 Iteration: 113800 Avg. Training loss: 2.8506 0.0156 sec/batch\n",
      "Global Step: 113900 Epoch 49/50 Iteration: 113900 Avg. Training loss: 2.8528 0.0130 sec/batch\n",
      "Global Step: 114000 Epoch 49/50 Iteration: 114000 Avg. Training loss: 2.8622 0.0158 sec/batch\n",
      "Global Step: 114100 Epoch 49/50 Iteration: 114100 Avg. Training loss: 2.8607 0.0137 sec/batch\n",
      "Global Step: 114200 Epoch 49/50 Iteration: 114200 Avg. Training loss: 2.8839 0.0116 sec/batch\n",
      "Global Step: 114300 Epoch 49/50 Iteration: 114300 Avg. Training loss: 2.8428 0.0129 sec/batch\n",
      "Global Step: 114400 Epoch 49/50 Iteration: 114400 Avg. Training loss: 2.8368 0.0153 sec/batch\n",
      "Global Step: 114500 Epoch 49/50 Iteration: 114500 Avg. Training loss: 2.8732 0.0148 sec/batch\n",
      "Global Step: 114600 Epoch 49/50 Iteration: 114600 Avg. Training loss: 2.8440 0.0137 sec/batch\n",
      "Global Step: 114700 Epoch 49/50 Iteration: 114700 Avg. Training loss: 2.8673 0.0145 sec/batch\n",
      "Global Step: 114800 Epoch 49/50 Iteration: 114800 Avg. Training loss: 2.8568 0.0178 sec/batch\n",
      "Global Step: 114900 Epoch 49/50 Iteration: 114900 Avg. Training loss: 2.8395 0.0161 sec/batch\n",
      "Global Step: 115000 Epoch 49/50 Iteration: 115000 Avg. Training loss: 2.8809 0.0165 sec/batch\n",
      "Global Step: 115100 Epoch 49/50 Iteration: 115100 Avg. Training loss: 2.8554 0.0174 sec/batch\n",
      "Global Step: 115200 Epoch 49/50 Iteration: 115200 Avg. Training loss: 2.8734 0.0160 sec/batch\n",
      "Global Step: 115300 Epoch 49/50 Iteration: 115300 Avg. Training loss: 2.8501 0.0163 sec/batch\n",
      "Global Step: 115400 Epoch 49/50 Iteration: 115400 Avg. Training loss: 2.8378 0.0137 sec/batch\n",
      "Global Step: 115500 Epoch 49/50 Iteration: 115500 Avg. Training loss: 2.8551 0.0174 sec/batch\n",
      "Global Step: 115600 Epoch 49/50 Iteration: 115600 Avg. Training loss: 2.8550 0.0152 sec/batch\n",
      "Epoch 50/50 Threshold: 0.0823238264796941 Length of Training words: 2399910\n",
      "Global Step: 115700 Epoch 50/50 Iteration: 115700 Avg. Training loss: 2.8504 0.0103 sec/batch\n",
      "Global Step: 115800 Epoch 50/50 Iteration: 115800 Avg. Training loss: 2.8617 0.0149 sec/batch\n",
      "Global Step: 115900 Epoch 50/50 Iteration: 115900 Avg. Training loss: 2.8669 0.0154 sec/batch\n",
      "Global Step: 116000 Epoch 50/50 Iteration: 116000 Avg. Training loss: 2.8363 0.0146 sec/batch\n",
      "Global Step: 116100 Epoch 50/50 Iteration: 116100 Avg. Training loss: 2.8398 0.0156 sec/batch\n",
      "Global Step: 116200 Epoch 50/50 Iteration: 116200 Avg. Training loss: 2.8349 0.0153 sec/batch\n",
      "Global Step: 116300 Epoch 50/50 Iteration: 116300 Avg. Training loss: 2.8455 0.0153 sec/batch\n",
      "Global Step: 116400 Epoch 50/50 Iteration: 116400 Avg. Training loss: 2.8501 0.0135 sec/batch\n",
      "Global Step: 116500 Epoch 50/50 Iteration: 116500 Avg. Training loss: 2.8486 0.0170 sec/batch\n",
      "Global Step: 116600 Epoch 50/50 Iteration: 116600 Avg. Training loss: 2.8564 0.0153 sec/batch\n",
      "Global Step: 116700 Epoch 50/50 Iteration: 116700 Avg. Training loss: 2.8292 0.0132 sec/batch\n",
      "Global Step: 116800 Epoch 50/50 Iteration: 116800 Avg. Training loss: 2.8331 0.0137 sec/batch\n",
      "Global Step: 116900 Epoch 50/50 Iteration: 116900 Avg. Training loss: 2.8493 0.0138 sec/batch\n",
      "Global Step: 117000 Epoch 50/50 Iteration: 117000 Avg. Training loss: 2.8371 0.0149 sec/batch\n",
      "Global Step: 117100 Epoch 50/50 Iteration: 117100 Avg. Training loss: 2.8497 0.0154 sec/batch\n",
      "Global Step: 117200 Epoch 50/50 Iteration: 117200 Avg. Training loss: 2.8351 0.0131 sec/batch\n",
      "Global Step: 117300 Epoch 50/50 Iteration: 117300 Avg. Training loss: 2.8299 0.0124 sec/batch\n",
      "Global Step: 117400 Epoch 50/50 Iteration: 117400 Avg. Training loss: 2.8669 0.0193 sec/batch\n",
      "Global Step: 117500 Epoch 50/50 Iteration: 117500 Avg. Training loss: 2.8450 0.0135 sec/batch\n",
      "Global Step: 117600 Epoch 50/50 Iteration: 117600 Avg. Training loss: 2.8595 0.0148 sec/batch\n",
      "Global Step: 117700 Epoch 50/50 Iteration: 117700 Avg. Training loss: 2.8310 0.0169 sec/batch\n",
      "Global Step: 117800 Epoch 50/50 Iteration: 117800 Avg. Training loss: 2.8190 0.0164 sec/batch\n",
      "Global Step: 117900 Epoch 50/50 Iteration: 117900 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Global Step: 118000 Epoch 50/50 Iteration: 118000 Avg. Training loss: 2.8372 0.0135 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/pos/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
