{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.06, 0.09)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/pos’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Threshold: 0.08708670843710356 Length of Training words: 2429632\n",
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 6.8498 0.0283 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 4.8515 0.0158 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 3.8954 0.0156 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 3.4002 0.0161 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 3.1780 0.0152 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 3.0604 0.0136 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 2.9929 0.0124 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 2.9768 0.0178 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 2.9223 0.0158 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 2.8990 0.0155 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 2.8639 0.0148 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 2.8691 0.0139 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 2.8690 0.0154 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 2.8716 0.0153 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 2.8565 0.0152 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 2.8583 0.0159 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 2.8374 0.0144 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 2.8769 0.0162 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 2.8491 0.0173 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 2.8620 0.0167 sec/batch\n",
      "Global Step: 2100 Epoch 1/50 Iteration: 2100 Avg. Training loss: 2.8236 0.0151 sec/batch\n",
      "Global Step: 2200 Epoch 1/50 Iteration: 2200 Avg. Training loss: 2.8194 0.0168 sec/batch\n",
      "Global Step: 2300 Epoch 1/50 Iteration: 2300 Avg. Training loss: 2.8519 0.0148 sec/batch\n",
      "Global Step: 2400 Epoch 1/50 Iteration: 2400 Avg. Training loss: 2.8353 0.0141 sec/batch\n",
      "Epoch 2/50 Threshold: 0.06967566519489501 Length of Training words: 2311639\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 2.8783 0.0118 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 2.9035 0.0156 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 2.9013 0.0142 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 2.8740 0.0157 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 2.8744 0.0150 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 2.8715 0.0170 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 2.8728 0.0162 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 2.8967 0.0147 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 2.8784 0.0164 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 2.8672 0.0172 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 2.8673 0.0161 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 2.8763 0.0160 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 2.8746 0.0151 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 2.8721 0.0163 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 2.8834 0.0161 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 2.8555 0.0147 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 2.8963 0.0148 sec/batch\n",
      "Global Step: 4200 Epoch 2/50 Iteration: 4200 Avg. Training loss: 2.8781 0.0137 sec/batch\n",
      "Global Step: 4300 Epoch 2/50 Iteration: 4300 Avg. Training loss: 2.8910 0.0157 sec/batch\n",
      "Global Step: 4400 Epoch 2/50 Iteration: 4400 Avg. Training loss: 2.8711 0.0152 sec/batch\n",
      "Global Step: 4500 Epoch 2/50 Iteration: 4500 Avg. Training loss: 2.8581 0.0151 sec/batch\n",
      "Global Step: 4600 Epoch 2/50 Iteration: 4600 Avg. Training loss: 2.8675 0.0169 sec/batch\n",
      "Global Step: 4700 Epoch 2/50 Iteration: 4700 Avg. Training loss: 2.8752 0.0175 sec/batch\n",
      "Epoch 3/50 Threshold: 0.08060656649666245 Length of Training words: 2388627\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 2.8617 0.0085 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 2.8708 0.0134 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 2.8717 0.0140 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 2.8455 0.0136 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 2.8450 0.0147 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 2.8402 0.0151 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 2.8491 0.0145 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 2.8555 0.0156 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 2.8543 0.0142 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 2.8610 0.0135 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 2.8374 0.0157 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 2.8408 0.0182 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 2.8511 0.0186 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 2.8511 0.0152 sec/batch\n",
      "Global Step: 6200 Epoch 3/50 Iteration: 6200 Avg. Training loss: 2.8521 0.0143 sec/batch\n",
      "Global Step: 6300 Epoch 3/50 Iteration: 6300 Avg. Training loss: 2.8438 0.0152 sec/batch\n",
      "Global Step: 6400 Epoch 3/50 Iteration: 6400 Avg. Training loss: 2.8316 0.0167 sec/batch\n",
      "Global Step: 6500 Epoch 3/50 Iteration: 6500 Avg. Training loss: 2.8734 0.0152 sec/batch\n",
      "Global Step: 6600 Epoch 3/50 Iteration: 6600 Avg. Training loss: 2.8546 0.0148 sec/batch\n",
      "Global Step: 6700 Epoch 3/50 Iteration: 6700 Avg. Training loss: 2.8733 0.0171 sec/batch\n",
      "Global Step: 6800 Epoch 3/50 Iteration: 6800 Avg. Training loss: 2.8315 0.0166 sec/batch\n",
      "Global Step: 6900 Epoch 3/50 Iteration: 6900 Avg. Training loss: 2.8233 0.0167 sec/batch\n",
      "Global Step: 7000 Epoch 3/50 Iteration: 7000 Avg. Training loss: 2.8571 0.0148 sec/batch\n",
      "Global Step: 7100 Epoch 3/50 Iteration: 7100 Avg. Training loss: 2.8430 0.0178 sec/batch\n",
      "Epoch 4/50 Threshold: 0.0859893440396867 Length of Training words: 2421835\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 2.8485 0.0097 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 2.8605 0.0137 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 2.8588 0.0150 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 2.8267 0.0182 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 2.8364 0.0151 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 2.8293 0.0154 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 2.8401 0.0170 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 2.8421 0.0135 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 2.8450 0.0133 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 2.8486 0.0130 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 2.8212 0.0177 sec/batch\n",
      "Global Step: 8300 Epoch 4/50 Iteration: 8300 Avg. Training loss: 2.8276 0.0144 sec/batch\n",
      "Global Step: 8400 Epoch 4/50 Iteration: 8400 Avg. Training loss: 2.8445 0.0146 sec/batch\n",
      "Global Step: 8500 Epoch 4/50 Iteration: 8500 Avg. Training loss: 2.8315 0.0155 sec/batch\n",
      "Global Step: 8600 Epoch 4/50 Iteration: 8600 Avg. Training loss: 2.8451 0.0154 sec/batch\n",
      "Global Step: 8700 Epoch 4/50 Iteration: 8700 Avg. Training loss: 2.8367 0.0142 sec/batch\n",
      "Global Step: 8800 Epoch 4/50 Iteration: 8800 Avg. Training loss: 2.8189 0.0149 sec/batch\n",
      "Global Step: 8900 Epoch 4/50 Iteration: 8900 Avg. Training loss: 2.8636 0.0132 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9000 Epoch 4/50 Iteration: 9000 Avg. Training loss: 2.8367 0.0152 sec/batch\n",
      "Global Step: 9100 Epoch 4/50 Iteration: 9100 Avg. Training loss: 2.8509 0.0162 sec/batch\n",
      "Global Step: 9200 Epoch 4/50 Iteration: 9200 Avg. Training loss: 2.8298 0.0165 sec/batch\n",
      "Global Step: 9300 Epoch 4/50 Iteration: 9300 Avg. Training loss: 2.8201 0.0155 sec/batch\n",
      "Global Step: 9400 Epoch 4/50 Iteration: 9400 Avg. Training loss: 2.8320 0.0141 sec/batch\n",
      "Global Step: 9500 Epoch 4/50 Iteration: 9500 Avg. Training loss: 2.8372 0.0174 sec/batch\n",
      "Epoch 5/50 Threshold: 0.0848752245043963 Length of Training words: 2415331\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 2.8358 0.0085 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 2.8597 0.0174 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 2.8593 0.0166 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 2.8449 0.0134 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 2.8310 0.0141 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 2.8239 0.0136 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 2.8519 0.0145 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 10400 Epoch 5/50 Iteration: 10400 Avg. Training loss: 2.8355 0.0140 sec/batch\n",
      "Global Step: 10500 Epoch 5/50 Iteration: 10500 Avg. Training loss: 2.8666 0.0180 sec/batch\n",
      "Global Step: 10600 Epoch 5/50 Iteration: 10600 Avg. Training loss: 2.8232 0.0172 sec/batch\n",
      "Global Step: 10700 Epoch 5/50 Iteration: 10700 Avg. Training loss: 2.8223 0.0182 sec/batch\n",
      "Global Step: 10800 Epoch 5/50 Iteration: 10800 Avg. Training loss: 2.8563 0.0133 sec/batch\n",
      "Global Step: 10900 Epoch 5/50 Iteration: 10900 Avg. Training loss: 2.8248 0.0176 sec/batch\n",
      "Global Step: 11000 Epoch 5/50 Iteration: 11000 Avg. Training loss: 2.8444 0.0150 sec/batch\n",
      "Global Step: 11100 Epoch 5/50 Iteration: 11100 Avg. Training loss: 2.8403 0.0157 sec/batch\n",
      "Global Step: 11200 Epoch 5/50 Iteration: 11200 Avg. Training loss: 2.8208 0.0142 sec/batch\n",
      "Global Step: 11300 Epoch 5/50 Iteration: 11300 Avg. Training loss: 2.8621 0.0165 sec/batch\n",
      "Global Step: 11400 Epoch 5/50 Iteration: 11400 Avg. Training loss: 2.8416 0.0128 sec/batch\n",
      "Global Step: 11500 Epoch 5/50 Iteration: 11500 Avg. Training loss: 2.8561 0.0143 sec/batch\n",
      "Global Step: 11600 Epoch 5/50 Iteration: 11600 Avg. Training loss: 2.8463 0.0141 sec/batch\n",
      "Global Step: 11700 Epoch 5/50 Iteration: 11700 Avg. Training loss: 2.8133 0.0172 sec/batch\n",
      "Global Step: 11800 Epoch 5/50 Iteration: 11800 Avg. Training loss: 2.8268 0.0172 sec/batch\n",
      "Global Step: 11900 Epoch 5/50 Iteration: 11900 Avg. Training loss: 2.8307 0.0157 sec/batch\n",
      "Epoch 6/50 Threshold: 0.06057974082760326 Length of Training words: 2234998\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 2.8612 0.0057 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 2.9153 0.0152 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 2.9147 0.0160 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 2.8999 0.0145 sec/batch\n",
      "Global Step: 12400 Epoch 6/50 Iteration: 12400 Avg. Training loss: 2.8952 0.0145 sec/batch\n",
      "Global Step: 12500 Epoch 6/50 Iteration: 12500 Avg. Training loss: 2.8881 0.0160 sec/batch\n",
      "Global Step: 12600 Epoch 6/50 Iteration: 12600 Avg. Training loss: 2.8990 0.0178 sec/batch\n",
      "Global Step: 12700 Epoch 6/50 Iteration: 12700 Avg. Training loss: 2.9125 0.0155 sec/batch\n",
      "Global Step: 12800 Epoch 6/50 Iteration: 12800 Avg. Training loss: 2.8985 0.0142 sec/batch\n",
      "Global Step: 12900 Epoch 6/50 Iteration: 12900 Avg. Training loss: 2.8875 0.0160 sec/batch\n",
      "Global Step: 13000 Epoch 6/50 Iteration: 13000 Avg. Training loss: 2.8875 0.0151 sec/batch\n",
      "Global Step: 13100 Epoch 6/50 Iteration: 13100 Avg. Training loss: 2.9001 0.0158 sec/batch\n",
      "Global Step: 13200 Epoch 6/50 Iteration: 13200 Avg. Training loss: 2.8940 0.0142 sec/batch\n",
      "Global Step: 13300 Epoch 6/50 Iteration: 13300 Avg. Training loss: 2.8987 0.0155 sec/batch\n",
      "Global Step: 13400 Epoch 6/50 Iteration: 13400 Avg. Training loss: 2.8988 0.0147 sec/batch\n",
      "Global Step: 13500 Epoch 6/50 Iteration: 13500 Avg. Training loss: 2.8796 0.0146 sec/batch\n",
      "Global Step: 13600 Epoch 6/50 Iteration: 13600 Avg. Training loss: 2.9229 0.0173 sec/batch\n",
      "Global Step: 13700 Epoch 6/50 Iteration: 13700 Avg. Training loss: 2.8978 0.0188 sec/batch\n",
      "Global Step: 13800 Epoch 6/50 Iteration: 13800 Avg. Training loss: 2.9174 0.0175 sec/batch\n",
      "Global Step: 13900 Epoch 6/50 Iteration: 13900 Avg. Training loss: 2.8748 0.0151 sec/batch\n",
      "Global Step: 14000 Epoch 6/50 Iteration: 14000 Avg. Training loss: 2.8746 0.0157 sec/batch\n",
      "Global Step: 14100 Epoch 6/50 Iteration: 14100 Avg. Training loss: 2.8978 0.0157 sec/batch\n",
      "Epoch 7/50 Threshold: 0.0850077106535117 Length of Training words: 2416591\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 2.8935 0.0005 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 2.8553 0.0173 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 2.8703 0.0138 sec/batch\n",
      "Global Step: 14500 Epoch 7/50 Iteration: 14500 Avg. Training loss: 2.8535 0.0124 sec/batch\n",
      "Global Step: 14600 Epoch 7/50 Iteration: 14600 Avg. Training loss: 2.8206 0.0168 sec/batch\n",
      "Global Step: 14700 Epoch 7/50 Iteration: 14700 Avg. Training loss: 2.8345 0.0153 sec/batch\n",
      "Global Step: 14800 Epoch 7/50 Iteration: 14800 Avg. Training loss: 2.8407 0.0142 sec/batch\n",
      "Global Step: 14900 Epoch 7/50 Iteration: 14900 Avg. Training loss: 2.8304 0.0154 sec/batch\n",
      "Global Step: 15000 Epoch 7/50 Iteration: 15000 Avg. Training loss: 2.8644 0.0141 sec/batch\n",
      "Global Step: 15100 Epoch 7/50 Iteration: 15100 Avg. Training loss: 2.8404 0.0161 sec/batch\n",
      "Global Step: 15200 Epoch 7/50 Iteration: 15200 Avg. Training loss: 2.8351 0.0156 sec/batch\n",
      "Global Step: 15300 Epoch 7/50 Iteration: 15300 Avg. Training loss: 2.8273 0.0154 sec/batch\n",
      "Global Step: 15400 Epoch 7/50 Iteration: 15400 Avg. Training loss: 2.8339 0.0151 sec/batch\n",
      "Global Step: 15500 Epoch 7/50 Iteration: 15500 Avg. Training loss: 2.8274 0.0165 sec/batch\n",
      "Global Step: 15600 Epoch 7/50 Iteration: 15600 Avg. Training loss: 2.8442 0.0156 sec/batch\n",
      "Global Step: 15700 Epoch 7/50 Iteration: 15700 Avg. Training loss: 2.8390 0.0145 sec/batch\n",
      "Global Step: 15800 Epoch 7/50 Iteration: 15800 Avg. Training loss: 2.8466 0.0144 sec/batch\n",
      "Global Step: 15900 Epoch 7/50 Iteration: 15900 Avg. Training loss: 2.8240 0.0137 sec/batch\n",
      "Global Step: 16000 Epoch 7/50 Iteration: 16000 Avg. Training loss: 2.8630 0.0160 sec/batch\n",
      "Global Step: 16100 Epoch 7/50 Iteration: 16100 Avg. Training loss: 2.8418 0.0159 sec/batch\n",
      "Global Step: 16200 Epoch 7/50 Iteration: 16200 Avg. Training loss: 2.8544 0.0152 sec/batch\n",
      "Global Step: 16300 Epoch 7/50 Iteration: 16300 Avg. Training loss: 2.8151 0.0183 sec/batch\n",
      "Global Step: 16400 Epoch 7/50 Iteration: 16400 Avg. Training loss: 2.8179 0.0167 sec/batch\n",
      "Global Step: 16500 Epoch 7/50 Iteration: 16500 Avg. Training loss: 2.8445 0.0160 sec/batch\n",
      "Global Step: 16600 Epoch 7/50 Iteration: 16600 Avg. Training loss: 2.8349 0.0171 sec/batch\n",
      "Epoch 8/50 Threshold: 0.08467242047489035 Length of Training words: 2414420\n",
      "Global Step: 16700 Epoch 8/50 Iteration: 16700 Avg. Training loss: 2.8454 0.0163 sec/batch\n",
      "Global Step: 16800 Epoch 8/50 Iteration: 16800 Avg. Training loss: 2.8691 0.0147 sec/batch\n",
      "Global Step: 16900 Epoch 8/50 Iteration: 16900 Avg. Training loss: 2.8577 0.0149 sec/batch\n",
      "Global Step: 17000 Epoch 8/50 Iteration: 17000 Avg. Training loss: 2.8350 0.0139 sec/batch\n",
      "Global Step: 17100 Epoch 8/50 Iteration: 17100 Avg. Training loss: 2.8341 0.0148 sec/batch\n",
      "Global Step: 17200 Epoch 8/50 Iteration: 17200 Avg. Training loss: 2.8312 0.0130 sec/batch\n",
      "Global Step: 17300 Epoch 8/50 Iteration: 17300 Avg. Training loss: 2.8392 0.0140 sec/batch\n",
      "Global Step: 17400 Epoch 8/50 Iteration: 17400 Avg. Training loss: 2.8545 0.0137 sec/batch\n",
      "Global Step: 17500 Epoch 8/50 Iteration: 17500 Avg. Training loss: 2.8421 0.0141 sec/batch\n",
      "Global Step: 17600 Epoch 8/50 Iteration: 17600 Avg. Training loss: 2.8445 0.0170 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17700 Epoch 8/50 Iteration: 17700 Avg. Training loss: 2.8237 0.0144 sec/batch\n",
      "Global Step: 17800 Epoch 8/50 Iteration: 17800 Avg. Training loss: 2.8300 0.0127 sec/batch\n",
      "Global Step: 17900 Epoch 8/50 Iteration: 17900 Avg. Training loss: 2.8458 0.0158 sec/batch\n",
      "Global Step: 18000 Epoch 8/50 Iteration: 18000 Avg. Training loss: 2.8464 0.0136 sec/batch\n",
      "Global Step: 18100 Epoch 8/50 Iteration: 18100 Avg. Training loss: 2.8346 0.0166 sec/batch\n",
      "Global Step: 18200 Epoch 8/50 Iteration: 18200 Avg. Training loss: 2.8472 0.0168 sec/batch\n",
      "Global Step: 18300 Epoch 8/50 Iteration: 18300 Avg. Training loss: 2.8252 0.0158 sec/batch\n",
      "Global Step: 18400 Epoch 8/50 Iteration: 18400 Avg. Training loss: 2.8632 0.0176 sec/batch\n",
      "Global Step: 18500 Epoch 8/50 Iteration: 18500 Avg. Training loss: 2.8403 0.0161 sec/batch\n",
      "Global Step: 18600 Epoch 8/50 Iteration: 18600 Avg. Training loss: 2.8588 0.0135 sec/batch\n",
      "Global Step: 18700 Epoch 8/50 Iteration: 18700 Avg. Training loss: 2.8214 0.0137 sec/batch\n",
      "Global Step: 18800 Epoch 8/50 Iteration: 18800 Avg. Training loss: 2.8139 0.0155 sec/batch\n",
      "Global Step: 18900 Epoch 8/50 Iteration: 18900 Avg. Training loss: 2.8470 0.0151 sec/batch\n",
      "Global Step: 19000 Epoch 8/50 Iteration: 19000 Avg. Training loss: 2.8332 0.0172 sec/batch\n",
      "Epoch 9/50 Threshold: 0.08782582800636445 Length of Training words: 2434382\n",
      "Global Step: 19100 Epoch 9/50 Iteration: 19100 Avg. Training loss: 2.8397 0.0130 sec/batch\n",
      "Global Step: 19200 Epoch 9/50 Iteration: 19200 Avg. Training loss: 2.8512 0.0173 sec/batch\n",
      "Global Step: 19300 Epoch 9/50 Iteration: 19300 Avg. Training loss: 2.8553 0.0164 sec/batch\n",
      "Global Step: 19400 Epoch 9/50 Iteration: 19400 Avg. Training loss: 2.8236 0.0150 sec/batch\n",
      "Global Step: 19500 Epoch 9/50 Iteration: 19500 Avg. Training loss: 2.8288 0.0146 sec/batch\n",
      "Global Step: 19600 Epoch 9/50 Iteration: 19600 Avg. Training loss: 2.8261 0.0143 sec/batch\n",
      "Global Step: 19700 Epoch 9/50 Iteration: 19700 Avg. Training loss: 2.8345 0.0138 sec/batch\n",
      "Global Step: 19800 Epoch 9/50 Iteration: 19800 Avg. Training loss: 2.8411 0.0150 sec/batch\n",
      "Global Step: 19900 Epoch 9/50 Iteration: 19900 Avg. Training loss: 2.8350 0.0155 sec/batch\n",
      "Global Step: 20000 Epoch 9/50 Iteration: 20000 Avg. Training loss: 2.8528 0.0161 sec/batch\n",
      "Global Step: 20100 Epoch 9/50 Iteration: 20100 Avg. Training loss: 2.8137 0.0150 sec/batch\n",
      "Global Step: 20200 Epoch 9/50 Iteration: 20200 Avg. Training loss: 2.8243 0.0162 sec/batch\n",
      "Global Step: 20300 Epoch 9/50 Iteration: 20300 Avg. Training loss: 2.8447 0.0161 sec/batch\n",
      "Global Step: 20400 Epoch 9/50 Iteration: 20400 Avg. Training loss: 2.8207 0.0160 sec/batch\n",
      "Global Step: 20500 Epoch 9/50 Iteration: 20500 Avg. Training loss: 2.8408 0.0153 sec/batch\n",
      "Global Step: 20600 Epoch 9/50 Iteration: 20600 Avg. Training loss: 2.8344 0.0136 sec/batch\n",
      "Global Step: 20700 Epoch 9/50 Iteration: 20700 Avg. Training loss: 2.8122 0.0140 sec/batch\n",
      "Global Step: 20800 Epoch 9/50 Iteration: 20800 Avg. Training loss: 2.8549 0.0149 sec/batch\n",
      "Global Step: 20900 Epoch 9/50 Iteration: 20900 Avg. Training loss: 2.8309 0.0150 sec/batch\n",
      "Global Step: 21000 Epoch 9/50 Iteration: 21000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 21100 Epoch 9/50 Iteration: 21100 Avg. Training loss: 2.8344 0.0172 sec/batch\n",
      "Global Step: 21200 Epoch 9/50 Iteration: 21200 Avg. Training loss: 2.8136 0.0154 sec/batch\n",
      "Global Step: 21300 Epoch 9/50 Iteration: 21300 Avg. Training loss: 2.8192 0.0151 sec/batch\n",
      "Global Step: 21400 Epoch 9/50 Iteration: 21400 Avg. Training loss: 2.8302 0.0147 sec/batch\n",
      "Epoch 10/50 Threshold: 0.08681111929218609 Length of Training words: 2427436\n",
      "Global Step: 21500 Epoch 10/50 Iteration: 21500 Avg. Training loss: 2.8344 0.0059 sec/batch\n",
      "Global Step: 21600 Epoch 10/50 Iteration: 21600 Avg. Training loss: 2.8455 0.0158 sec/batch\n",
      "Global Step: 21700 Epoch 10/50 Iteration: 21700 Avg. Training loss: 2.8662 0.0176 sec/batch\n",
      "Global Step: 21800 Epoch 10/50 Iteration: 21800 Avg. Training loss: 2.8459 0.0143 sec/batch\n",
      "Global Step: 21900 Epoch 10/50 Iteration: 21900 Avg. Training loss: 2.8286 0.0145 sec/batch\n",
      "Global Step: 22000 Epoch 10/50 Iteration: 22000 Avg. Training loss: 2.8245 0.0144 sec/batch\n",
      "Global Step: 22100 Epoch 10/50 Iteration: 22100 Avg. Training loss: 2.8291 0.0187 sec/batch\n",
      "Global Step: 22200 Epoch 10/50 Iteration: 22200 Avg. Training loss: 2.8406 0.0187 sec/batch\n",
      "Global Step: 22300 Epoch 10/50 Iteration: 22300 Avg. Training loss: 2.8366 0.0160 sec/batch\n",
      "Global Step: 22400 Epoch 10/50 Iteration: 22400 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 22500 Epoch 10/50 Iteration: 22500 Avg. Training loss: 2.8220 0.0154 sec/batch\n",
      "Global Step: 22600 Epoch 10/50 Iteration: 22600 Avg. Training loss: 2.8159 0.0148 sec/batch\n",
      "Global Step: 22700 Epoch 10/50 Iteration: 22700 Avg. Training loss: 2.8466 0.0155 sec/batch\n",
      "Global Step: 22800 Epoch 10/50 Iteration: 22800 Avg. Training loss: 2.8275 0.0137 sec/batch\n",
      "Global Step: 22900 Epoch 10/50 Iteration: 22900 Avg. Training loss: 2.8319 0.0152 sec/batch\n",
      "Global Step: 23000 Epoch 10/50 Iteration: 23000 Avg. Training loss: 2.8433 0.0153 sec/batch\n",
      "Global Step: 23100 Epoch 10/50 Iteration: 23100 Avg. Training loss: 2.8259 0.0154 sec/batch\n",
      "Global Step: 23200 Epoch 10/50 Iteration: 23200 Avg. Training loss: 2.8376 0.0145 sec/batch\n",
      "Global Step: 23300 Epoch 10/50 Iteration: 23300 Avg. Training loss: 2.8581 0.0167 sec/batch\n",
      "Global Step: 23400 Epoch 10/50 Iteration: 23400 Avg. Training loss: 2.8369 0.0155 sec/batch\n",
      "Global Step: 23500 Epoch 10/50 Iteration: 23500 Avg. Training loss: 2.8404 0.0117 sec/batch\n",
      "Global Step: 23600 Epoch 10/50 Iteration: 23600 Avg. Training loss: 2.8159 0.0146 sec/batch\n",
      "Global Step: 23700 Epoch 10/50 Iteration: 23700 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 23800 Epoch 10/50 Iteration: 23800 Avg. Training loss: 2.8264 0.0151 sec/batch\n",
      "Epoch 11/50 Threshold: 0.08986173649493497 Length of Training words: 2445824\n",
      "Global Step: 23900 Epoch 11/50 Iteration: 23900 Avg. Training loss: 2.8282 0.0015 sec/batch\n",
      "Global Step: 24000 Epoch 11/50 Iteration: 24000 Avg. Training loss: 2.8505 0.0152 sec/batch\n",
      "Global Step: 24100 Epoch 11/50 Iteration: 24100 Avg. Training loss: 2.8602 0.0140 sec/batch\n",
      "Global Step: 24200 Epoch 11/50 Iteration: 24200 Avg. Training loss: 2.8411 0.0137 sec/batch\n",
      "Global Step: 24300 Epoch 11/50 Iteration: 24300 Avg. Training loss: 2.8116 0.0134 sec/batch\n",
      "Global Step: 24400 Epoch 11/50 Iteration: 24400 Avg. Training loss: 2.8237 0.0154 sec/batch\n",
      "Global Step: 24500 Epoch 11/50 Iteration: 24500 Avg. Training loss: 2.8330 0.0149 sec/batch\n",
      "Global Step: 24600 Epoch 11/50 Iteration: 24600 Avg. Training loss: 2.8196 0.0159 sec/batch\n",
      "Global Step: 24700 Epoch 11/50 Iteration: 24700 Avg. Training loss: 2.8562 0.0134 sec/batch\n",
      "Global Step: 24800 Epoch 11/50 Iteration: 24800 Avg. Training loss: 2.8342 0.0162 sec/batch\n",
      "Global Step: 24900 Epoch 11/50 Iteration: 24900 Avg. Training loss: 2.8242 0.0138 sec/batch\n",
      "Global Step: 25000 Epoch 11/50 Iteration: 25000 Avg. Training loss: 2.8181 0.0175 sec/batch\n",
      "Global Step: 25100 Epoch 11/50 Iteration: 25100 Avg. Training loss: 2.8224 0.0222 sec/batch\n",
      "Global Step: 25200 Epoch 11/50 Iteration: 25200 Avg. Training loss: 2.8228 0.0155 sec/batch\n",
      "Global Step: 25300 Epoch 11/50 Iteration: 25300 Avg. Training loss: 2.8374 0.0185 sec/batch\n",
      "Global Step: 25400 Epoch 11/50 Iteration: 25400 Avg. Training loss: 2.8314 0.0150 sec/batch\n",
      "Global Step: 25500 Epoch 11/50 Iteration: 25500 Avg. Training loss: 2.8323 0.0132 sec/batch\n",
      "Global Step: 25600 Epoch 11/50 Iteration: 25600 Avg. Training loss: 2.8125 0.0152 sec/batch\n",
      "Global Step: 25700 Epoch 11/50 Iteration: 25700 Avg. Training loss: 2.8541 0.0151 sec/batch\n",
      "Global Step: 25800 Epoch 11/50 Iteration: 25800 Avg. Training loss: 2.8278 0.0160 sec/batch\n",
      "Global Step: 25900 Epoch 11/50 Iteration: 25900 Avg. Training loss: 2.8493 0.0146 sec/batch\n",
      "Global Step: 26000 Epoch 11/50 Iteration: 26000 Avg. Training loss: 2.8121 0.0156 sec/batch\n",
      "Global Step: 26100 Epoch 11/50 Iteration: 26100 Avg. Training loss: 2.8029 0.0142 sec/batch\n",
      "Global Step: 26200 Epoch 11/50 Iteration: 26200 Avg. Training loss: 2.8370 0.0149 sec/batch\n",
      "Global Step: 26300 Epoch 11/50 Iteration: 26300 Avg. Training loss: 2.8203 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Threshold: 0.08110778818964093 Length of Training words: 2392894\n",
      "Global Step: 26400 Epoch 12/50 Iteration: 26400 Avg. Training loss: 2.8435 0.0091 sec/batch\n",
      "Global Step: 26500 Epoch 12/50 Iteration: 26500 Avg. Training loss: 2.8679 0.0172 sec/batch\n",
      "Global Step: 26600 Epoch 12/50 Iteration: 26600 Avg. Training loss: 2.8693 0.0159 sec/batch\n",
      "Global Step: 26700 Epoch 12/50 Iteration: 26700 Avg. Training loss: 2.8375 0.0153 sec/batch\n",
      "Global Step: 26800 Epoch 12/50 Iteration: 26800 Avg. Training loss: 2.8435 0.0164 sec/batch\n",
      "Global Step: 26900 Epoch 12/50 Iteration: 26900 Avg. Training loss: 2.8369 0.0145 sec/batch\n",
      "Global Step: 27000 Epoch 12/50 Iteration: 27000 Avg. Training loss: 2.8479 0.0164 sec/batch\n",
      "Global Step: 27100 Epoch 12/50 Iteration: 27100 Avg. Training loss: 2.8547 0.0155 sec/batch\n",
      "Global Step: 27200 Epoch 12/50 Iteration: 27200 Avg. Training loss: 2.8543 0.0150 sec/batch\n",
      "Global Step: 27300 Epoch 12/50 Iteration: 27300 Avg. Training loss: 2.8543 0.0140 sec/batch\n",
      "Global Step: 27400 Epoch 12/50 Iteration: 27400 Avg. Training loss: 2.8292 0.0176 sec/batch\n",
      "Global Step: 27500 Epoch 12/50 Iteration: 27500 Avg. Training loss: 2.8413 0.0163 sec/batch\n",
      "Global Step: 27600 Epoch 12/50 Iteration: 27600 Avg. Training loss: 2.8461 0.0153 sec/batch\n",
      "Global Step: 27700 Epoch 12/50 Iteration: 27700 Avg. Training loss: 2.8538 0.0149 sec/batch\n",
      "Global Step: 27800 Epoch 12/50 Iteration: 27800 Avg. Training loss: 2.8460 0.0162 sec/batch\n",
      "Global Step: 27900 Epoch 12/50 Iteration: 27900 Avg. Training loss: 2.8436 0.0162 sec/batch\n",
      "Global Step: 28000 Epoch 12/50 Iteration: 28000 Avg. Training loss: 2.8351 0.0163 sec/batch\n",
      "Global Step: 28100 Epoch 12/50 Iteration: 28100 Avg. Training loss: 2.8640 0.0164 sec/batch\n",
      "Global Step: 28200 Epoch 12/50 Iteration: 28200 Avg. Training loss: 2.8487 0.0128 sec/batch\n",
      "Global Step: 28300 Epoch 12/50 Iteration: 28300 Avg. Training loss: 2.8673 0.0152 sec/batch\n",
      "Global Step: 28400 Epoch 12/50 Iteration: 28400 Avg. Training loss: 2.8257 0.0147 sec/batch\n",
      "Global Step: 28500 Epoch 12/50 Iteration: 28500 Avg. Training loss: 2.8200 0.0152 sec/batch\n",
      "Global Step: 28600 Epoch 12/50 Iteration: 28600 Avg. Training loss: 2.8543 0.0156 sec/batch\n",
      "Global Step: 28700 Epoch 12/50 Iteration: 28700 Avg. Training loss: 2.8426 0.0189 sec/batch\n",
      "Epoch 13/50 Threshold: 0.08717083632765375 Length of Training words: 2430009\n",
      "Global Step: 28800 Epoch 13/50 Iteration: 28800 Avg. Training loss: 2.8433 0.0119 sec/batch\n",
      "Global Step: 28900 Epoch 13/50 Iteration: 28900 Avg. Training loss: 2.8544 0.0155 sec/batch\n",
      "Global Step: 29000 Epoch 13/50 Iteration: 29000 Avg. Training loss: 2.8561 0.0183 sec/batch\n",
      "Global Step: 29100 Epoch 13/50 Iteration: 29100 Avg. Training loss: 2.8250 0.0165 sec/batch\n",
      "Global Step: 29200 Epoch 13/50 Iteration: 29200 Avg. Training loss: 2.8316 0.0147 sec/batch\n",
      "Global Step: 29300 Epoch 13/50 Iteration: 29300 Avg. Training loss: 2.8254 0.0164 sec/batch\n",
      "Global Step: 29400 Epoch 13/50 Iteration: 29400 Avg. Training loss: 2.8380 0.0132 sec/batch\n",
      "Global Step: 29500 Epoch 13/50 Iteration: 29500 Avg. Training loss: 2.8419 0.0152 sec/batch\n",
      "Global Step: 29600 Epoch 13/50 Iteration: 29600 Avg. Training loss: 2.8412 0.0165 sec/batch\n",
      "Global Step: 29700 Epoch 13/50 Iteration: 29700 Avg. Training loss: 2.8453 0.0150 sec/batch\n",
      "Global Step: 29800 Epoch 13/50 Iteration: 29800 Avg. Training loss: 2.8164 0.0160 sec/batch\n",
      "Global Step: 29900 Epoch 13/50 Iteration: 29900 Avg. Training loss: 2.8203 0.0165 sec/batch\n",
      "Global Step: 30000 Epoch 13/50 Iteration: 30000 Avg. Training loss: 2.8475 0.0163 sec/batch\n",
      "Global Step: 30100 Epoch 13/50 Iteration: 30100 Avg. Training loss: 2.8233 0.0131 sec/batch\n",
      "Global Step: 30200 Epoch 13/50 Iteration: 30200 Avg. Training loss: 2.8436 0.0148 sec/batch\n",
      "Global Step: 30300 Epoch 13/50 Iteration: 30300 Avg. Training loss: 2.8325 0.0129 sec/batch\n",
      "Global Step: 30400 Epoch 13/50 Iteration: 30400 Avg. Training loss: 2.8146 0.0169 sec/batch\n",
      "Global Step: 30500 Epoch 13/50 Iteration: 30500 Avg. Training loss: 2.8589 0.0126 sec/batch\n",
      "Global Step: 30600 Epoch 13/50 Iteration: 30600 Avg. Training loss: 2.8321 0.0148 sec/batch\n",
      "Global Step: 30700 Epoch 13/50 Iteration: 30700 Avg. Training loss: 2.8510 0.0167 sec/batch\n",
      "Global Step: 30800 Epoch 13/50 Iteration: 30800 Avg. Training loss: 2.8292 0.0177 sec/batch\n",
      "Global Step: 30900 Epoch 13/50 Iteration: 30900 Avg. Training loss: 2.8177 0.0145 sec/batch\n",
      "Global Step: 31000 Epoch 13/50 Iteration: 31000 Avg. Training loss: 2.8219 0.0189 sec/batch\n",
      "Global Step: 31100 Epoch 13/50 Iteration: 31100 Avg. Training loss: 2.8284 0.0155 sec/batch\n",
      "Epoch 14/50 Threshold: 0.07736795802688062 Length of Training words: 2369330\n",
      "Global Step: 31200 Epoch 14/50 Iteration: 31200 Avg. Training loss: 2.8465 0.0079 sec/batch\n",
      "Global Step: 31300 Epoch 14/50 Iteration: 31300 Avg. Training loss: 2.8703 0.0140 sec/batch\n",
      "Global Step: 31400 Epoch 14/50 Iteration: 31400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 31500 Epoch 14/50 Iteration: 31500 Avg. Training loss: 2.8589 0.0144 sec/batch\n",
      "Global Step: 31600 Epoch 14/50 Iteration: 31600 Avg. Training loss: 2.8476 0.0167 sec/batch\n",
      "Global Step: 31700 Epoch 14/50 Iteration: 31700 Avg. Training loss: 2.8344 0.0144 sec/batch\n",
      "Global Step: 31800 Epoch 14/50 Iteration: 31800 Avg. Training loss: 2.8710 0.0176 sec/batch\n",
      "Global Step: 31900 Epoch 14/50 Iteration: 31900 Avg. Training loss: 2.8550 0.0190 sec/batch\n",
      "Global Step: 32000 Epoch 14/50 Iteration: 32000 Avg. Training loss: 2.8547 0.0152 sec/batch\n",
      "Global Step: 32100 Epoch 14/50 Iteration: 32100 Avg. Training loss: 2.8728 0.0167 sec/batch\n",
      "Global Step: 32200 Epoch 14/50 Iteration: 32200 Avg. Training loss: 2.8362 0.0161 sec/batch\n",
      "Global Step: 32300 Epoch 14/50 Iteration: 32300 Avg. Training loss: 2.8446 0.0142 sec/batch\n",
      "Global Step: 32400 Epoch 14/50 Iteration: 32400 Avg. Training loss: 2.8629 0.0148 sec/batch\n",
      "Global Step: 32500 Epoch 14/50 Iteration: 32500 Avg. Training loss: 2.8484 0.0145 sec/batch\n",
      "Global Step: 32600 Epoch 14/50 Iteration: 32600 Avg. Training loss: 2.8589 0.0150 sec/batch\n",
      "Global Step: 32700 Epoch 14/50 Iteration: 32700 Avg. Training loss: 2.8483 0.0118 sec/batch\n",
      "Global Step: 32800 Epoch 14/50 Iteration: 32800 Avg. Training loss: 2.8380 0.0158 sec/batch\n",
      "Global Step: 32900 Epoch 14/50 Iteration: 32900 Avg. Training loss: 2.8775 0.0144 sec/batch\n",
      "Global Step: 33000 Epoch 14/50 Iteration: 33000 Avg. Training loss: 2.8572 0.0178 sec/batch\n",
      "Global Step: 33100 Epoch 14/50 Iteration: 33100 Avg. Training loss: 2.8785 0.0160 sec/batch\n",
      "Global Step: 33200 Epoch 14/50 Iteration: 33200 Avg. Training loss: 2.8371 0.0187 sec/batch\n",
      "Global Step: 33300 Epoch 14/50 Iteration: 33300 Avg. Training loss: 2.8276 0.0170 sec/batch\n",
      "Global Step: 33400 Epoch 14/50 Iteration: 33400 Avg. Training loss: 2.8591 0.0198 sec/batch\n",
      "Global Step: 33500 Epoch 14/50 Iteration: 33500 Avg. Training loss: 2.8482 0.0157 sec/batch\n",
      "Epoch 15/50 Threshold: 0.07867502562390928 Length of Training words: 2377258\n",
      "Global Step: 33600 Epoch 15/50 Iteration: 33600 Avg. Training loss: 2.8611 0.0127 sec/batch\n",
      "Global Step: 33700 Epoch 15/50 Iteration: 33700 Avg. Training loss: 2.8720 0.0163 sec/batch\n",
      "Global Step: 33800 Epoch 15/50 Iteration: 33800 Avg. Training loss: 2.8717 0.0160 sec/batch\n",
      "Global Step: 33900 Epoch 15/50 Iteration: 33900 Avg. Training loss: 2.8478 0.0143 sec/batch\n",
      "Global Step: 34000 Epoch 15/50 Iteration: 34000 Avg. Training loss: 2.8473 0.0149 sec/batch\n",
      "Global Step: 34100 Epoch 15/50 Iteration: 34100 Avg. Training loss: 2.8435 0.0159 sec/batch\n",
      "Global Step: 34200 Epoch 15/50 Iteration: 34200 Avg. Training loss: 2.8486 0.0162 sec/batch\n",
      "Global Step: 34300 Epoch 15/50 Iteration: 34300 Avg. Training loss: 2.8660 0.0167 sec/batch\n",
      "Global Step: 34400 Epoch 15/50 Iteration: 34400 Avg. Training loss: 2.8560 0.0142 sec/batch\n",
      "Global Step: 34500 Epoch 15/50 Iteration: 34500 Avg. Training loss: 2.8551 0.0142 sec/batch\n",
      "Global Step: 34600 Epoch 15/50 Iteration: 34600 Avg. Training loss: 2.8281 0.0140 sec/batch\n",
      "Global Step: 34700 Epoch 15/50 Iteration: 34700 Avg. Training loss: 2.8506 0.0139 sec/batch\n",
      "Global Step: 34800 Epoch 15/50 Iteration: 34800 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 34900 Epoch 15/50 Iteration: 34900 Avg. Training loss: 2.8578 0.0156 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 35000 Epoch 15/50 Iteration: 35000 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 35100 Epoch 15/50 Iteration: 35100 Avg. Training loss: 2.8581 0.0157 sec/batch\n",
      "Global Step: 35200 Epoch 15/50 Iteration: 35200 Avg. Training loss: 2.8403 0.0138 sec/batch\n",
      "Global Step: 35300 Epoch 15/50 Iteration: 35300 Avg. Training loss: 2.8762 0.0164 sec/batch\n",
      "Global Step: 35400 Epoch 15/50 Iteration: 35400 Avg. Training loss: 2.8581 0.0161 sec/batch\n",
      "Global Step: 35500 Epoch 15/50 Iteration: 35500 Avg. Training loss: 2.8638 0.0139 sec/batch\n",
      "Global Step: 35600 Epoch 15/50 Iteration: 35600 Avg. Training loss: 2.8266 0.0152 sec/batch\n",
      "Global Step: 35700 Epoch 15/50 Iteration: 35700 Avg. Training loss: 2.8329 0.0153 sec/batch\n",
      "Global Step: 35800 Epoch 15/50 Iteration: 35800 Avg. Training loss: 2.8430 0.0177 sec/batch\n",
      "Global Step: 35900 Epoch 15/50 Iteration: 35900 Avg. Training loss: 2.8503 0.0130 sec/batch\n",
      "Epoch 16/50 Threshold: 0.0720019299680255 Length of Training words: 2329165\n",
      "Global Step: 36000 Epoch 16/50 Iteration: 36000 Avg. Training loss: 2.8825 0.0148 sec/batch\n",
      "Global Step: 36100 Epoch 16/50 Iteration: 36100 Avg. Training loss: 2.8978 0.0151 sec/batch\n",
      "Global Step: 36200 Epoch 16/50 Iteration: 36200 Avg. Training loss: 2.8758 0.0147 sec/batch\n",
      "Global Step: 36300 Epoch 16/50 Iteration: 36300 Avg. Training loss: 2.8573 0.0144 sec/batch\n",
      "Global Step: 36400 Epoch 16/50 Iteration: 36400 Avg. Training loss: 2.8587 0.0150 sec/batch\n",
      "Global Step: 36500 Epoch 16/50 Iteration: 36500 Avg. Training loss: 2.8645 0.0132 sec/batch\n",
      "Global Step: 36600 Epoch 16/50 Iteration: 36600 Avg. Training loss: 2.8690 0.0175 sec/batch\n",
      "Global Step: 36700 Epoch 16/50 Iteration: 36700 Avg. Training loss: 2.8724 0.0186 sec/batch\n",
      "Global Step: 36800 Epoch 16/50 Iteration: 36800 Avg. Training loss: 2.8878 0.0156 sec/batch\n",
      "Global Step: 36900 Epoch 16/50 Iteration: 36900 Avg. Training loss: 2.8542 0.0145 sec/batch\n",
      "Global Step: 37000 Epoch 16/50 Iteration: 37000 Avg. Training loss: 2.8491 0.0160 sec/batch\n",
      "Global Step: 37100 Epoch 16/50 Iteration: 37100 Avg. Training loss: 2.8816 0.0162 sec/batch\n",
      "Global Step: 37200 Epoch 16/50 Iteration: 37200 Avg. Training loss: 2.8555 0.0143 sec/batch\n",
      "Global Step: 37300 Epoch 16/50 Iteration: 37300 Avg. Training loss: 2.8749 0.0158 sec/batch\n",
      "Global Step: 37400 Epoch 16/50 Iteration: 37400 Avg. Training loss: 2.8679 0.0150 sec/batch\n",
      "Global Step: 37500 Epoch 16/50 Iteration: 37500 Avg. Training loss: 2.8492 0.0180 sec/batch\n",
      "Global Step: 37600 Epoch 16/50 Iteration: 37600 Avg. Training loss: 2.8883 0.0165 sec/batch\n",
      "Global Step: 37700 Epoch 16/50 Iteration: 37700 Avg. Training loss: 2.8683 0.0145 sec/batch\n",
      "Global Step: 37800 Epoch 16/50 Iteration: 37800 Avg. Training loss: 2.8865 0.0156 sec/batch\n",
      "Global Step: 37900 Epoch 16/50 Iteration: 37900 Avg. Training loss: 2.8610 0.0161 sec/batch\n",
      "Global Step: 38000 Epoch 16/50 Iteration: 38000 Avg. Training loss: 2.8436 0.0163 sec/batch\n",
      "Global Step: 38100 Epoch 16/50 Iteration: 38100 Avg. Training loss: 2.8677 0.0152 sec/batch\n",
      "Global Step: 38200 Epoch 16/50 Iteration: 38200 Avg. Training loss: 2.8611 0.0151 sec/batch\n",
      "Epoch 17/50 Threshold: 0.08720321176489212 Length of Training words: 2429828\n",
      "Global Step: 38300 Epoch 17/50 Iteration: 38300 Avg. Training loss: 2.8484 0.0092 sec/batch\n",
      "Global Step: 38400 Epoch 17/50 Iteration: 38400 Avg. Training loss: 2.8508 0.0141 sec/batch\n",
      "Global Step: 38500 Epoch 17/50 Iteration: 38500 Avg. Training loss: 2.8600 0.0147 sec/batch\n",
      "Global Step: 38600 Epoch 17/50 Iteration: 38600 Avg. Training loss: 2.8243 0.0113 sec/batch\n",
      "Global Step: 38700 Epoch 17/50 Iteration: 38700 Avg. Training loss: 2.8304 0.0119 sec/batch\n",
      "Global Step: 38800 Epoch 17/50 Iteration: 38800 Avg. Training loss: 2.8248 0.0125 sec/batch\n",
      "Global Step: 38900 Epoch 17/50 Iteration: 38900 Avg. Training loss: 2.8357 0.0140 sec/batch\n",
      "Global Step: 39000 Epoch 17/50 Iteration: 39000 Avg. Training loss: 2.8408 0.0191 sec/batch\n",
      "Global Step: 39100 Epoch 17/50 Iteration: 39100 Avg. Training loss: 2.8364 0.0169 sec/batch\n",
      "Global Step: 39200 Epoch 17/50 Iteration: 39200 Avg. Training loss: 2.8535 0.0160 sec/batch\n",
      "Global Step: 39300 Epoch 17/50 Iteration: 39300 Avg. Training loss: 2.8166 0.0167 sec/batch\n",
      "Global Step: 39400 Epoch 17/50 Iteration: 39400 Avg. Training loss: 2.8239 0.0158 sec/batch\n",
      "Global Step: 39500 Epoch 17/50 Iteration: 39500 Avg. Training loss: 2.8466 0.0130 sec/batch\n",
      "Global Step: 39600 Epoch 17/50 Iteration: 39600 Avg. Training loss: 2.8177 0.0142 sec/batch\n",
      "Global Step: 39700 Epoch 17/50 Iteration: 39700 Avg. Training loss: 2.8436 0.0166 sec/batch\n",
      "Global Step: 39800 Epoch 17/50 Iteration: 39800 Avg. Training loss: 2.8375 0.0138 sec/batch\n",
      "Global Step: 39900 Epoch 17/50 Iteration: 39900 Avg. Training loss: 2.8115 0.0134 sec/batch\n",
      "Global Step: 40000 Epoch 17/50 Iteration: 40000 Avg. Training loss: 2.8589 0.0158 sec/batch\n",
      "Global Step: 40100 Epoch 17/50 Iteration: 40100 Avg. Training loss: 2.8330 0.0148 sec/batch\n",
      "Global Step: 40200 Epoch 17/50 Iteration: 40200 Avg. Training loss: 2.8518 0.0157 sec/batch\n",
      "Global Step: 40300 Epoch 17/50 Iteration: 40300 Avg. Training loss: 2.8338 0.0133 sec/batch\n",
      "Global Step: 40400 Epoch 17/50 Iteration: 40400 Avg. Training loss: 2.8156 0.0137 sec/batch\n",
      "Global Step: 40500 Epoch 17/50 Iteration: 40500 Avg. Training loss: 2.8206 0.0139 sec/batch\n",
      "Global Step: 40600 Epoch 17/50 Iteration: 40600 Avg. Training loss: 2.8285 0.0118 sec/batch\n",
      "Epoch 18/50 Threshold: 0.07863276721785589 Length of Training words: 2376633\n",
      "Global Step: 40700 Epoch 18/50 Iteration: 40700 Avg. Training loss: 2.8469 0.0056 sec/batch\n",
      "Global Step: 40800 Epoch 18/50 Iteration: 40800 Avg. Training loss: 2.8651 0.0117 sec/batch\n",
      "Global Step: 40900 Epoch 18/50 Iteration: 40900 Avg. Training loss: 2.8781 0.0132 sec/batch\n",
      "Global Step: 41000 Epoch 18/50 Iteration: 41000 Avg. Training loss: 2.8563 0.0117 sec/batch\n",
      "Global Step: 41100 Epoch 18/50 Iteration: 41100 Avg. Training loss: 2.8441 0.0126 sec/batch\n",
      "Global Step: 41200 Epoch 18/50 Iteration: 41200 Avg. Training loss: 2.8358 0.0129 sec/batch\n",
      "Global Step: 41300 Epoch 18/50 Iteration: 41300 Avg. Training loss: 2.8593 0.0126 sec/batch\n",
      "Global Step: 41400 Epoch 18/50 Iteration: 41400 Avg. Training loss: 2.8597 0.0126 sec/batch\n",
      "Global Step: 41500 Epoch 18/50 Iteration: 41500 Avg. Training loss: 2.8469 0.0143 sec/batch\n",
      "Global Step: 41600 Epoch 18/50 Iteration: 41600 Avg. Training loss: 2.8755 0.0138 sec/batch\n",
      "Global Step: 41700 Epoch 18/50 Iteration: 41700 Avg. Training loss: 2.8338 0.0155 sec/batch\n",
      "Global Step: 41800 Epoch 18/50 Iteration: 41800 Avg. Training loss: 2.8415 0.0143 sec/batch\n",
      "Global Step: 41900 Epoch 18/50 Iteration: 41900 Avg. Training loss: 2.8631 0.0147 sec/batch\n",
      "Global Step: 42000 Epoch 18/50 Iteration: 42000 Avg. Training loss: 2.8376 0.0160 sec/batch\n",
      "Global Step: 42100 Epoch 18/50 Iteration: 42100 Avg. Training loss: 2.8589 0.0148 sec/batch\n",
      "Global Step: 42200 Epoch 18/50 Iteration: 42200 Avg. Training loss: 2.8495 0.0156 sec/batch\n",
      "Global Step: 42300 Epoch 18/50 Iteration: 42300 Avg. Training loss: 2.8306 0.0156 sec/batch\n",
      "Global Step: 42400 Epoch 18/50 Iteration: 42400 Avg. Training loss: 2.8768 0.0153 sec/batch\n",
      "Global Step: 42500 Epoch 18/50 Iteration: 42500 Avg. Training loss: 2.8515 0.0144 sec/batch\n",
      "Global Step: 42600 Epoch 18/50 Iteration: 42600 Avg. Training loss: 2.8700 0.0133 sec/batch\n",
      "Global Step: 42700 Epoch 18/50 Iteration: 42700 Avg. Training loss: 2.8418 0.0129 sec/batch\n",
      "Global Step: 42800 Epoch 18/50 Iteration: 42800 Avg. Training loss: 2.8286 0.0127 sec/batch\n",
      "Global Step: 42900 Epoch 18/50 Iteration: 42900 Avg. Training loss: 2.8542 0.0137 sec/batch\n",
      "Global Step: 43000 Epoch 18/50 Iteration: 43000 Avg. Training loss: 2.8467 0.0126 sec/batch\n",
      "Epoch 19/50 Threshold: 0.084364621536794 Length of Training words: 2411957\n",
      "Global Step: 43100 Epoch 19/50 Iteration: 43100 Avg. Training loss: 2.8475 0.0077 sec/batch\n",
      "Global Step: 43200 Epoch 19/50 Iteration: 43200 Avg. Training loss: 2.8590 0.0144 sec/batch\n",
      "Global Step: 43300 Epoch 19/50 Iteration: 43300 Avg. Training loss: 2.8653 0.0121 sec/batch\n",
      "Global Step: 43400 Epoch 19/50 Iteration: 43400 Avg. Training loss: 2.8360 0.0134 sec/batch\n",
      "Global Step: 43500 Epoch 19/50 Iteration: 43500 Avg. Training loss: 2.8348 0.0142 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 43600 Epoch 19/50 Iteration: 43600 Avg. Training loss: 2.8298 0.0130 sec/batch\n",
      "Global Step: 43700 Epoch 19/50 Iteration: 43700 Avg. Training loss: 2.8439 0.0126 sec/batch\n",
      "Global Step: 43800 Epoch 19/50 Iteration: 43800 Avg. Training loss: 2.8480 0.0170 sec/batch\n",
      "Global Step: 43900 Epoch 19/50 Iteration: 43900 Avg. Training loss: 2.8422 0.0172 sec/batch\n",
      "Global Step: 44000 Epoch 19/50 Iteration: 44000 Avg. Training loss: 2.8582 0.0153 sec/batch\n",
      "Global Step: 44100 Epoch 19/50 Iteration: 44100 Avg. Training loss: 2.8216 0.0152 sec/batch\n",
      "Global Step: 44200 Epoch 19/50 Iteration: 44200 Avg. Training loss: 2.8287 0.0126 sec/batch\n",
      "Global Step: 44300 Epoch 19/50 Iteration: 44300 Avg. Training loss: 2.8507 0.0135 sec/batch\n",
      "Global Step: 44400 Epoch 19/50 Iteration: 44400 Avg. Training loss: 2.8275 0.0149 sec/batch\n",
      "Global Step: 44500 Epoch 19/50 Iteration: 44500 Avg. Training loss: 2.8478 0.0157 sec/batch\n",
      "Global Step: 44600 Epoch 19/50 Iteration: 44600 Avg. Training loss: 2.8371 0.0156 sec/batch\n",
      "Global Step: 44700 Epoch 19/50 Iteration: 44700 Avg. Training loss: 2.8216 0.0141 sec/batch\n",
      "Global Step: 44800 Epoch 19/50 Iteration: 44800 Avg. Training loss: 2.8652 0.0153 sec/batch\n",
      "Global Step: 44900 Epoch 19/50 Iteration: 44900 Avg. Training loss: 2.8342 0.0136 sec/batch\n",
      "Global Step: 45000 Epoch 19/50 Iteration: 45000 Avg. Training loss: 2.8565 0.0120 sec/batch\n",
      "Global Step: 45100 Epoch 19/50 Iteration: 45100 Avg. Training loss: 2.8320 0.0129 sec/batch\n",
      "Global Step: 45200 Epoch 19/50 Iteration: 45200 Avg. Training loss: 2.8217 0.0132 sec/batch\n",
      "Global Step: 45300 Epoch 19/50 Iteration: 45300 Avg. Training loss: 2.8350 0.0144 sec/batch\n",
      "Global Step: 45400 Epoch 19/50 Iteration: 45400 Avg. Training loss: 2.8393 0.0121 sec/batch\n",
      "Epoch 20/50 Threshold: 0.06092402220480751 Length of Training words: 2238418\n",
      "Global Step: 45500 Epoch 20/50 Iteration: 45500 Avg. Training loss: 2.8707 0.0072 sec/batch\n",
      "Global Step: 45600 Epoch 20/50 Iteration: 45600 Avg. Training loss: 2.9149 0.0124 sec/batch\n",
      "Global Step: 45700 Epoch 20/50 Iteration: 45700 Avg. Training loss: 2.9167 0.0126 sec/batch\n",
      "Global Step: 45800 Epoch 20/50 Iteration: 45800 Avg. Training loss: 2.8921 0.0117 sec/batch\n",
      "Global Step: 45900 Epoch 20/50 Iteration: 45900 Avg. Training loss: 2.8866 0.0116 sec/batch\n",
      "Global Step: 46000 Epoch 20/50 Iteration: 46000 Avg. Training loss: 2.8914 0.0139 sec/batch\n",
      "Global Step: 46100 Epoch 20/50 Iteration: 46100 Avg. Training loss: 2.8929 0.0158 sec/batch\n",
      "Global Step: 46200 Epoch 20/50 Iteration: 46200 Avg. Training loss: 2.9094 0.0138 sec/batch\n",
      "Global Step: 46300 Epoch 20/50 Iteration: 46300 Avg. Training loss: 2.9070 0.0130 sec/batch\n",
      "Global Step: 46400 Epoch 20/50 Iteration: 46400 Avg. Training loss: 2.8830 0.0156 sec/batch\n",
      "Global Step: 46500 Epoch 20/50 Iteration: 46500 Avg. Training loss: 2.8782 0.0147 sec/batch\n",
      "Global Step: 46600 Epoch 20/50 Iteration: 46600 Avg. Training loss: 2.9095 0.0153 sec/batch\n",
      "Global Step: 46700 Epoch 20/50 Iteration: 46700 Avg. Training loss: 2.8839 0.0135 sec/batch\n",
      "Global Step: 46800 Epoch 20/50 Iteration: 46800 Avg. Training loss: 2.9048 0.0163 sec/batch\n",
      "Global Step: 46900 Epoch 20/50 Iteration: 46900 Avg. Training loss: 2.8951 0.0152 sec/batch\n",
      "Global Step: 47000 Epoch 20/50 Iteration: 47000 Avg. Training loss: 2.8764 0.0155 sec/batch\n",
      "Global Step: 47100 Epoch 20/50 Iteration: 47100 Avg. Training loss: 2.9205 0.0149 sec/batch\n",
      "Global Step: 47200 Epoch 20/50 Iteration: 47200 Avg. Training loss: 2.8997 0.0120 sec/batch\n",
      "Global Step: 47300 Epoch 20/50 Iteration: 47300 Avg. Training loss: 2.9113 0.0122 sec/batch\n",
      "Global Step: 47400 Epoch 20/50 Iteration: 47400 Avg. Training loss: 2.8768 0.0139 sec/batch\n",
      "Global Step: 47500 Epoch 20/50 Iteration: 47500 Avg. Training loss: 2.8812 0.0124 sec/batch\n",
      "Global Step: 47600 Epoch 20/50 Iteration: 47600 Avg. Training loss: 2.8851 0.0131 sec/batch\n",
      "Epoch 21/50 Threshold: 0.06706143014707991 Length of Training words: 2289788\n",
      "Global Step: 47700 Epoch 21/50 Iteration: 47700 Avg. Training loss: 2.8957 0.0020 sec/batch\n",
      "Global Step: 47800 Epoch 21/50 Iteration: 47800 Avg. Training loss: 2.8904 0.0126 sec/batch\n",
      "Global Step: 47900 Epoch 21/50 Iteration: 47900 Avg. Training loss: 2.9109 0.0124 sec/batch\n",
      "Global Step: 48000 Epoch 21/50 Iteration: 48000 Avg. Training loss: 2.8883 0.0127 sec/batch\n",
      "Global Step: 48100 Epoch 21/50 Iteration: 48100 Avg. Training loss: 2.8717 0.0141 sec/batch\n",
      "Global Step: 48200 Epoch 21/50 Iteration: 48200 Avg. Training loss: 2.8689 0.0142 sec/batch\n",
      "Global Step: 48300 Epoch 21/50 Iteration: 48300 Avg. Training loss: 2.8858 0.0149 sec/batch\n",
      "Global Step: 48400 Epoch 21/50 Iteration: 48400 Avg. Training loss: 2.8819 0.0152 sec/batch\n",
      "Global Step: 48500 Epoch 21/50 Iteration: 48500 Avg. Training loss: 2.8791 0.0160 sec/batch\n",
      "Global Step: 48600 Epoch 21/50 Iteration: 48600 Avg. Training loss: 2.8976 0.0150 sec/batch\n",
      "Global Step: 48700 Epoch 21/50 Iteration: 48700 Avg. Training loss: 2.8656 0.0150 sec/batch\n",
      "Global Step: 48800 Epoch 21/50 Iteration: 48800 Avg. Training loss: 2.8699 0.0140 sec/batch\n",
      "Global Step: 48900 Epoch 21/50 Iteration: 48900 Avg. Training loss: 2.8773 0.0165 sec/batch\n",
      "Global Step: 49000 Epoch 21/50 Iteration: 49000 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 49100 Epoch 21/50 Iteration: 49100 Avg. Training loss: 2.8780 0.0171 sec/batch\n",
      "Global Step: 49200 Epoch 21/50 Iteration: 49200 Avg. Training loss: 2.8829 0.0150 sec/batch\n",
      "Global Step: 49300 Epoch 21/50 Iteration: 49300 Avg. Training loss: 2.8680 0.0130 sec/batch\n",
      "Global Step: 49400 Epoch 21/50 Iteration: 49400 Avg. Training loss: 2.9020 0.0130 sec/batch\n",
      "Global Step: 49500 Epoch 21/50 Iteration: 49500 Avg. Training loss: 2.8804 0.0127 sec/batch\n",
      "Global Step: 49600 Epoch 21/50 Iteration: 49600 Avg. Training loss: 2.8893 0.0136 sec/batch\n",
      "Global Step: 49700 Epoch 21/50 Iteration: 49700 Avg. Training loss: 2.8556 0.0127 sec/batch\n",
      "Global Step: 49800 Epoch 21/50 Iteration: 49800 Avg. Training loss: 2.8666 0.0138 sec/batch\n",
      "Global Step: 49900 Epoch 21/50 Iteration: 49900 Avg. Training loss: 2.8745 0.0135 sec/batch\n",
      "Epoch 22/50 Threshold: 0.07384798892845924 Length of Training words: 2344481\n",
      "Global Step: 50000 Epoch 22/50 Iteration: 50000 Avg. Training loss: 2.8710 0.0043 sec/batch\n",
      "Global Step: 50100 Epoch 22/50 Iteration: 50100 Avg. Training loss: 2.8709 0.0139 sec/batch\n",
      "Global Step: 50200 Epoch 22/50 Iteration: 50200 Avg. Training loss: 2.8957 0.0150 sec/batch\n",
      "Global Step: 50300 Epoch 22/50 Iteration: 50300 Avg. Training loss: 2.8718 0.0127 sec/batch\n",
      "Global Step: 50400 Epoch 22/50 Iteration: 50400 Avg. Training loss: 2.8550 0.0156 sec/batch\n",
      "Global Step: 50500 Epoch 22/50 Iteration: 50500 Avg. Training loss: 2.8489 0.0155 sec/batch\n",
      "Global Step: 50600 Epoch 22/50 Iteration: 50600 Avg. Training loss: 2.8670 0.0153 sec/batch\n",
      "Global Step: 50700 Epoch 22/50 Iteration: 50700 Avg. Training loss: 2.8698 0.0156 sec/batch\n",
      "Global Step: 50800 Epoch 22/50 Iteration: 50800 Avg. Training loss: 2.8572 0.0148 sec/batch\n",
      "Global Step: 50900 Epoch 22/50 Iteration: 50900 Avg. Training loss: 2.8870 0.0133 sec/batch\n",
      "Global Step: 51000 Epoch 22/50 Iteration: 51000 Avg. Training loss: 2.8440 0.0124 sec/batch\n",
      "Global Step: 51100 Epoch 22/50 Iteration: 51100 Avg. Training loss: 2.8533 0.0152 sec/batch\n",
      "Global Step: 51200 Epoch 22/50 Iteration: 51200 Avg. Training loss: 2.8699 0.0147 sec/batch\n",
      "Global Step: 51300 Epoch 22/50 Iteration: 51300 Avg. Training loss: 2.8494 0.0143 sec/batch\n",
      "Global Step: 51400 Epoch 22/50 Iteration: 51400 Avg. Training loss: 2.8701 0.0146 sec/batch\n",
      "Global Step: 51500 Epoch 22/50 Iteration: 51500 Avg. Training loss: 2.8587 0.0146 sec/batch\n",
      "Global Step: 51600 Epoch 22/50 Iteration: 51600 Avg. Training loss: 2.8490 0.0124 sec/batch\n",
      "Global Step: 51700 Epoch 22/50 Iteration: 51700 Avg. Training loss: 2.8839 0.0129 sec/batch\n",
      "Global Step: 51800 Epoch 22/50 Iteration: 51800 Avg. Training loss: 2.8656 0.0130 sec/batch\n",
      "Global Step: 51900 Epoch 22/50 Iteration: 51900 Avg. Training loss: 2.8848 0.0125 sec/batch\n",
      "Global Step: 52000 Epoch 22/50 Iteration: 52000 Avg. Training loss: 2.8423 0.0138 sec/batch\n",
      "Global Step: 52100 Epoch 22/50 Iteration: 52100 Avg. Training loss: 2.8388 0.0139 sec/batch\n",
      "Global Step: 52200 Epoch 22/50 Iteration: 52200 Avg. Training loss: 2.8699 0.0129 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 52300 Epoch 22/50 Iteration: 52300 Avg. Training loss: 2.8550 0.0132 sec/batch\n",
      "Epoch 23/50 Threshold: 0.07978773587287813 Length of Training words: 2383586\n",
      "Global Step: 52400 Epoch 23/50 Iteration: 52400 Avg. Training loss: 2.8550 0.0097 sec/batch\n",
      "Global Step: 52500 Epoch 23/50 Iteration: 52500 Avg. Training loss: 2.8775 0.0147 sec/batch\n",
      "Global Step: 52600 Epoch 23/50 Iteration: 52600 Avg. Training loss: 2.8682 0.0155 sec/batch\n",
      "Global Step: 52700 Epoch 23/50 Iteration: 52700 Avg. Training loss: 2.8443 0.0162 sec/batch\n",
      "Global Step: 52800 Epoch 23/50 Iteration: 52800 Avg. Training loss: 2.8399 0.0147 sec/batch\n",
      "Global Step: 52900 Epoch 23/50 Iteration: 52900 Avg. Training loss: 2.8438 0.0173 sec/batch\n",
      "Global Step: 53000 Epoch 23/50 Iteration: 53000 Avg. Training loss: 2.8473 0.0147 sec/batch\n",
      "Global Step: 53100 Epoch 23/50 Iteration: 53100 Avg. Training loss: 2.8656 0.0167 sec/batch\n",
      "Global Step: 53200 Epoch 23/50 Iteration: 53200 Avg. Training loss: 2.8504 0.0156 sec/batch\n",
      "Global Step: 53300 Epoch 23/50 Iteration: 53300 Avg. Training loss: 2.8504 0.0166 sec/batch\n",
      "Global Step: 53400 Epoch 23/50 Iteration: 53400 Avg. Training loss: 2.8326 0.0156 sec/batch\n",
      "Global Step: 53500 Epoch 23/50 Iteration: 53500 Avg. Training loss: 2.8474 0.0140 sec/batch\n",
      "Global Step: 53600 Epoch 23/50 Iteration: 53600 Avg. Training loss: 2.8433 0.0134 sec/batch\n",
      "Global Step: 53700 Epoch 23/50 Iteration: 53700 Avg. Training loss: 2.8552 0.0142 sec/batch\n",
      "Global Step: 53800 Epoch 23/50 Iteration: 53800 Avg. Training loss: 2.8489 0.0139 sec/batch\n",
      "Global Step: 53900 Epoch 23/50 Iteration: 53900 Avg. Training loss: 2.8549 0.0100 sec/batch\n",
      "Global Step: 54000 Epoch 23/50 Iteration: 54000 Avg. Training loss: 2.8393 0.0120 sec/batch\n",
      "Global Step: 54100 Epoch 23/50 Iteration: 54100 Avg. Training loss: 2.8720 0.0134 sec/batch\n",
      "Global Step: 54200 Epoch 23/50 Iteration: 54200 Avg. Training loss: 2.8509 0.0123 sec/batch\n",
      "Global Step: 54300 Epoch 23/50 Iteration: 54300 Avg. Training loss: 2.8595 0.0141 sec/batch\n",
      "Global Step: 54400 Epoch 23/50 Iteration: 54400 Avg. Training loss: 2.8235 0.0159 sec/batch\n",
      "Global Step: 54500 Epoch 23/50 Iteration: 54500 Avg. Training loss: 2.8347 0.0173 sec/batch\n",
      "Global Step: 54600 Epoch 23/50 Iteration: 54600 Avg. Training loss: 2.8431 0.0149 sec/batch\n",
      "Global Step: 54700 Epoch 23/50 Iteration: 54700 Avg. Training loss: 2.8479 0.0154 sec/batch\n",
      "Epoch 24/50 Threshold: 0.0780015896553169 Length of Training words: 2372332\n",
      "Global Step: 54800 Epoch 24/50 Iteration: 54800 Avg. Training loss: 2.8681 0.0129 sec/batch\n",
      "Global Step: 54900 Epoch 24/50 Iteration: 54900 Avg. Training loss: 2.8848 0.0151 sec/batch\n",
      "Global Step: 55000 Epoch 24/50 Iteration: 55000 Avg. Training loss: 2.8635 0.0135 sec/batch\n",
      "Global Step: 55100 Epoch 24/50 Iteration: 55100 Avg. Training loss: 2.8363 0.0169 sec/batch\n",
      "Global Step: 55200 Epoch 24/50 Iteration: 55200 Avg. Training loss: 2.8517 0.0175 sec/batch\n",
      "Global Step: 55300 Epoch 24/50 Iteration: 55300 Avg. Training loss: 2.8504 0.0152 sec/batch\n",
      "Global Step: 55400 Epoch 24/50 Iteration: 55400 Avg. Training loss: 2.8488 0.0162 sec/batch\n",
      "Global Step: 55500 Epoch 24/50 Iteration: 55500 Avg. Training loss: 2.8712 0.0149 sec/batch\n",
      "Global Step: 55600 Epoch 24/50 Iteration: 55600 Avg. Training loss: 2.8663 0.0151 sec/batch\n",
      "Global Step: 55700 Epoch 24/50 Iteration: 55700 Avg. Training loss: 2.8392 0.0144 sec/batch\n",
      "Global Step: 55800 Epoch 24/50 Iteration: 55800 Avg. Training loss: 2.8393 0.0137 sec/batch\n",
      "Global Step: 55900 Epoch 24/50 Iteration: 55900 Avg. Training loss: 2.8572 0.0121 sec/batch\n",
      "Global Step: 56000 Epoch 24/50 Iteration: 56000 Avg. Training loss: 2.8480 0.0133 sec/batch\n",
      "Global Step: 56100 Epoch 24/50 Iteration: 56100 Avg. Training loss: 2.8465 0.0165 sec/batch\n",
      "Global Step: 56200 Epoch 24/50 Iteration: 56200 Avg. Training loss: 2.8622 0.0161 sec/batch\n",
      "Global Step: 56300 Epoch 24/50 Iteration: 56300 Avg. Training loss: 2.8479 0.0135 sec/batch\n",
      "Global Step: 56400 Epoch 24/50 Iteration: 56400 Avg. Training loss: 2.8543 0.0144 sec/batch\n",
      "Global Step: 56500 Epoch 24/50 Iteration: 56500 Avg. Training loss: 2.8700 0.0149 sec/batch\n",
      "Global Step: 56600 Epoch 24/50 Iteration: 56600 Avg. Training loss: 2.8564 0.0142 sec/batch\n",
      "Global Step: 56700 Epoch 24/50 Iteration: 56700 Avg. Training loss: 2.8645 0.0153 sec/batch\n",
      "Global Step: 56800 Epoch 24/50 Iteration: 56800 Avg. Training loss: 2.8281 0.0164 sec/batch\n",
      "Global Step: 56900 Epoch 24/50 Iteration: 56900 Avg. Training loss: 2.8405 0.0150 sec/batch\n",
      "Global Step: 57000 Epoch 24/50 Iteration: 57000 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Epoch 25/50 Threshold: 0.08888214480232033 Length of Training words: 2439982\n",
      "Global Step: 57100 Epoch 25/50 Iteration: 57100 Avg. Training loss: 2.8459 0.0050 sec/batch\n",
      "Global Step: 57200 Epoch 25/50 Iteration: 57200 Avg. Training loss: 2.8390 0.0158 sec/batch\n",
      "Global Step: 57300 Epoch 25/50 Iteration: 57300 Avg. Training loss: 2.8665 0.0159 sec/batch\n",
      "Global Step: 57400 Epoch 25/50 Iteration: 57400 Avg. Training loss: 2.8340 0.0161 sec/batch\n",
      "Global Step: 57500 Epoch 25/50 Iteration: 57500 Avg. Training loss: 2.8204 0.0156 sec/batch\n",
      "Global Step: 57600 Epoch 25/50 Iteration: 57600 Avg. Training loss: 2.8238 0.0147 sec/batch\n",
      "Global Step: 57700 Epoch 25/50 Iteration: 57700 Avg. Training loss: 2.8264 0.0164 sec/batch\n",
      "Global Step: 57800 Epoch 25/50 Iteration: 57800 Avg. Training loss: 2.8334 0.0129 sec/batch\n",
      "Global Step: 57900 Epoch 25/50 Iteration: 57900 Avg. Training loss: 2.8425 0.0142 sec/batch\n",
      "Global Step: 58000 Epoch 25/50 Iteration: 58000 Avg. Training loss: 2.8515 0.0137 sec/batch\n",
      "Global Step: 58100 Epoch 25/50 Iteration: 58100 Avg. Training loss: 2.8138 0.0134 sec/batch\n",
      "Global Step: 58200 Epoch 25/50 Iteration: 58200 Avg. Training loss: 2.8194 0.0123 sec/batch\n",
      "Global Step: 58300 Epoch 25/50 Iteration: 58300 Avg. Training loss: 2.8313 0.0131 sec/batch\n",
      "Global Step: 58400 Epoch 25/50 Iteration: 58400 Avg. Training loss: 2.8262 0.0141 sec/batch\n",
      "Global Step: 58500 Epoch 25/50 Iteration: 58500 Avg. Training loss: 2.8276 0.0120 sec/batch\n",
      "Global Step: 58600 Epoch 25/50 Iteration: 58600 Avg. Training loss: 2.8294 0.0125 sec/batch\n",
      "Global Step: 58700 Epoch 25/50 Iteration: 58700 Avg. Training loss: 2.8341 0.0139 sec/batch\n",
      "Global Step: 58800 Epoch 25/50 Iteration: 58800 Avg. Training loss: 2.8255 0.0140 sec/batch\n",
      "Global Step: 58900 Epoch 25/50 Iteration: 58900 Avg. Training loss: 2.8509 0.0144 sec/batch\n",
      "Global Step: 59000 Epoch 25/50 Iteration: 59000 Avg. Training loss: 2.8332 0.0173 sec/batch\n",
      "Global Step: 59100 Epoch 25/50 Iteration: 59100 Avg. Training loss: 2.8422 0.0174 sec/batch\n",
      "Global Step: 59200 Epoch 25/50 Iteration: 59200 Avg. Training loss: 2.8070 0.0156 sec/batch\n",
      "Global Step: 59300 Epoch 25/50 Iteration: 59300 Avg. Training loss: 2.8117 0.0152 sec/batch\n",
      "Global Step: 59400 Epoch 25/50 Iteration: 59400 Avg. Training loss: 2.8299 0.0140 sec/batch\n",
      "Global Step: 59500 Epoch 25/50 Iteration: 59500 Avg. Training loss: 2.8283 0.0153 sec/batch\n",
      "Epoch 26/50 Threshold: 0.07345678619703419 Length of Training words: 2341446\n",
      "Global Step: 59600 Epoch 26/50 Iteration: 59600 Avg. Training loss: 2.8632 0.0132 sec/batch\n",
      "Global Step: 59700 Epoch 26/50 Iteration: 59700 Avg. Training loss: 2.9014 0.0175 sec/batch\n",
      "Global Step: 59800 Epoch 26/50 Iteration: 59800 Avg. Training loss: 2.8739 0.0173 sec/batch\n",
      "Global Step: 59900 Epoch 26/50 Iteration: 59900 Avg. Training loss: 2.8502 0.0133 sec/batch\n",
      "Global Step: 60000 Epoch 26/50 Iteration: 60000 Avg. Training loss: 2.8588 0.0154 sec/batch\n",
      "Global Step: 60100 Epoch 26/50 Iteration: 60100 Avg. Training loss: 2.8628 0.0153 sec/batch\n",
      "Global Step: 60200 Epoch 26/50 Iteration: 60200 Avg. Training loss: 2.8578 0.0140 sec/batch\n",
      "Global Step: 60300 Epoch 26/50 Iteration: 60300 Avg. Training loss: 2.8807 0.0139 sec/batch\n",
      "Global Step: 60400 Epoch 26/50 Iteration: 60400 Avg. Training loss: 2.8789 0.0155 sec/batch\n",
      "Global Step: 60500 Epoch 26/50 Iteration: 60500 Avg. Training loss: 2.8458 0.0132 sec/batch\n",
      "Global Step: 60600 Epoch 26/50 Iteration: 60600 Avg. Training loss: 2.8506 0.0154 sec/batch\n",
      "Global Step: 60700 Epoch 26/50 Iteration: 60700 Avg. Training loss: 2.8664 0.0173 sec/batch\n",
      "Global Step: 60800 Epoch 26/50 Iteration: 60800 Avg. Training loss: 2.8588 0.0144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 60900 Epoch 26/50 Iteration: 60900 Avg. Training loss: 2.8592 0.0140 sec/batch\n",
      "Global Step: 61000 Epoch 26/50 Iteration: 61000 Avg. Training loss: 2.8718 0.0146 sec/batch\n",
      "Global Step: 61100 Epoch 26/50 Iteration: 61100 Avg. Training loss: 2.8490 0.0119 sec/batch\n",
      "Global Step: 61200 Epoch 26/50 Iteration: 61200 Avg. Training loss: 2.8788 0.0142 sec/batch\n",
      "Global Step: 61300 Epoch 26/50 Iteration: 61300 Avg. Training loss: 2.8717 0.0144 sec/batch\n",
      "Global Step: 61400 Epoch 26/50 Iteration: 61400 Avg. Training loss: 2.8806 0.0137 sec/batch\n",
      "Global Step: 61500 Epoch 26/50 Iteration: 61500 Avg. Training loss: 2.8656 0.0122 sec/batch\n",
      "Global Step: 61600 Epoch 26/50 Iteration: 61600 Avg. Training loss: 2.8393 0.0152 sec/batch\n",
      "Global Step: 61700 Epoch 26/50 Iteration: 61700 Avg. Training loss: 2.8497 0.0163 sec/batch\n",
      "Global Step: 61800 Epoch 26/50 Iteration: 61800 Avg. Training loss: 2.8591 0.0134 sec/batch\n",
      "Epoch 27/50 Threshold: 0.06887011513088591 Length of Training words: 2304434\n",
      "Global Step: 61900 Epoch 27/50 Iteration: 61900 Avg. Training loss: 2.8719 0.0065 sec/batch\n",
      "Global Step: 62000 Epoch 27/50 Iteration: 62000 Avg. Training loss: 2.8947 0.0164 sec/batch\n",
      "Global Step: 62100 Epoch 27/50 Iteration: 62100 Avg. Training loss: 2.8940 0.0152 sec/batch\n",
      "Global Step: 62200 Epoch 27/50 Iteration: 62200 Avg. Training loss: 2.8749 0.0151 sec/batch\n",
      "Global Step: 62300 Epoch 27/50 Iteration: 62300 Avg. Training loss: 2.8666 0.0146 sec/batch\n",
      "Global Step: 62400 Epoch 27/50 Iteration: 62400 Avg. Training loss: 2.8653 0.0144 sec/batch\n",
      "Global Step: 62500 Epoch 27/50 Iteration: 62500 Avg. Training loss: 2.8751 0.0145 sec/batch\n",
      "Global Step: 62600 Epoch 27/50 Iteration: 62600 Avg. Training loss: 2.8865 0.0162 sec/batch\n",
      "Global Step: 62700 Epoch 27/50 Iteration: 62700 Avg. Training loss: 2.8800 0.0163 sec/batch\n",
      "Global Step: 62800 Epoch 27/50 Iteration: 62800 Avg. Training loss: 2.8802 0.0151 sec/batch\n",
      "Global Step: 62900 Epoch 27/50 Iteration: 62900 Avg. Training loss: 2.8586 0.0163 sec/batch\n",
      "Global Step: 63000 Epoch 27/50 Iteration: 63000 Avg. Training loss: 2.8810 0.0153 sec/batch\n",
      "Global Step: 63100 Epoch 27/50 Iteration: 63100 Avg. Training loss: 2.8715 0.0163 sec/batch\n",
      "Global Step: 63200 Epoch 27/50 Iteration: 63200 Avg. Training loss: 2.8706 0.0136 sec/batch\n",
      "Global Step: 63300 Epoch 27/50 Iteration: 63300 Avg. Training loss: 2.8690 0.0142 sec/batch\n",
      "Global Step: 63400 Epoch 27/50 Iteration: 63400 Avg. Training loss: 2.8779 0.0153 sec/batch\n",
      "Global Step: 63500 Epoch 27/50 Iteration: 63500 Avg. Training loss: 2.8745 0.0145 sec/batch\n",
      "Global Step: 63600 Epoch 27/50 Iteration: 63600 Avg. Training loss: 2.8927 0.0141 sec/batch\n",
      "Global Step: 63700 Epoch 27/50 Iteration: 63700 Avg. Training loss: 2.8815 0.0149 sec/batch\n",
      "Global Step: 63800 Epoch 27/50 Iteration: 63800 Avg. Training loss: 2.8809 0.0149 sec/batch\n",
      "Global Step: 63900 Epoch 27/50 Iteration: 63900 Avg. Training loss: 2.8529 0.0154 sec/batch\n",
      "Global Step: 64000 Epoch 27/50 Iteration: 64000 Avg. Training loss: 2.8606 0.0159 sec/batch\n",
      "Global Step: 64100 Epoch 27/50 Iteration: 64100 Avg. Training loss: 2.8647 0.0147 sec/batch\n",
      "Epoch 28/50 Threshold: 0.08345389394934948 Length of Training words: 2406327\n",
      "Global Step: 64200 Epoch 28/50 Iteration: 64200 Avg. Training loss: 2.8669 0.0068 sec/batch\n",
      "Global Step: 64300 Epoch 28/50 Iteration: 64300 Avg. Training loss: 2.8533 0.0153 sec/batch\n",
      "Global Step: 64400 Epoch 28/50 Iteration: 64400 Avg. Training loss: 2.8678 0.0171 sec/batch\n",
      "Global Step: 64500 Epoch 28/50 Iteration: 64500 Avg. Training loss: 2.8451 0.0160 sec/batch\n",
      "Global Step: 64600 Epoch 28/50 Iteration: 64600 Avg. Training loss: 2.8374 0.0173 sec/batch\n",
      "Global Step: 64700 Epoch 28/50 Iteration: 64700 Avg. Training loss: 2.8261 0.0156 sec/batch\n",
      "Global Step: 64800 Epoch 28/50 Iteration: 64800 Avg. Training loss: 2.8465 0.0147 sec/batch\n",
      "Global Step: 64900 Epoch 28/50 Iteration: 64900 Avg. Training loss: 2.8482 0.0152 sec/batch\n",
      "Global Step: 65000 Epoch 28/50 Iteration: 65000 Avg. Training loss: 2.8399 0.0148 sec/batch\n",
      "Global Step: 65100 Epoch 28/50 Iteration: 65100 Avg. Training loss: 2.8677 0.0125 sec/batch\n",
      "Global Step: 65200 Epoch 28/50 Iteration: 65200 Avg. Training loss: 2.8238 0.0138 sec/batch\n",
      "Global Step: 65300 Epoch 28/50 Iteration: 65300 Avg. Training loss: 2.8220 0.0142 sec/batch\n",
      "Global Step: 65400 Epoch 28/50 Iteration: 65400 Avg. Training loss: 2.8588 0.0149 sec/batch\n",
      "Global Step: 65500 Epoch 28/50 Iteration: 65500 Avg. Training loss: 2.8271 0.0172 sec/batch\n",
      "Global Step: 65600 Epoch 28/50 Iteration: 65600 Avg. Training loss: 2.8475 0.0139 sec/batch\n",
      "Global Step: 65700 Epoch 28/50 Iteration: 65700 Avg. Training loss: 2.8460 0.0169 sec/batch\n",
      "Global Step: 65800 Epoch 28/50 Iteration: 65800 Avg. Training loss: 2.8226 0.0142 sec/batch\n",
      "Global Step: 65900 Epoch 28/50 Iteration: 65900 Avg. Training loss: 2.8628 0.0144 sec/batch\n",
      "Global Step: 66000 Epoch 28/50 Iteration: 66000 Avg. Training loss: 2.8420 0.0164 sec/batch\n",
      "Global Step: 66100 Epoch 28/50 Iteration: 66100 Avg. Training loss: 2.8640 0.0122 sec/batch\n",
      "Global Step: 66200 Epoch 28/50 Iteration: 66200 Avg. Training loss: 2.8489 0.0126 sec/batch\n",
      "Global Step: 66300 Epoch 28/50 Iteration: 66300 Avg. Training loss: 2.8178 0.0141 sec/batch\n",
      "Global Step: 66400 Epoch 28/50 Iteration: 66400 Avg. Training loss: 2.8261 0.0148 sec/batch\n",
      "Global Step: 66500 Epoch 28/50 Iteration: 66500 Avg. Training loss: 2.8332 0.0136 sec/batch\n",
      "Epoch 29/50 Threshold: 0.07780942376694977 Length of Training words: 2371334\n",
      "Global Step: 66600 Epoch 29/50 Iteration: 66600 Avg. Training loss: 2.8515 0.0051 sec/batch\n",
      "Global Step: 66700 Epoch 29/50 Iteration: 66700 Avg. Training loss: 2.8699 0.0144 sec/batch\n",
      "Global Step: 66800 Epoch 29/50 Iteration: 66800 Avg. Training loss: 2.8806 0.0142 sec/batch\n",
      "Global Step: 66900 Epoch 29/50 Iteration: 66900 Avg. Training loss: 2.8606 0.0174 sec/batch\n",
      "Global Step: 67000 Epoch 29/50 Iteration: 67000 Avg. Training loss: 2.8471 0.0158 sec/batch\n",
      "Global Step: 67100 Epoch 29/50 Iteration: 67100 Avg. Training loss: 2.8366 0.0155 sec/batch\n",
      "Global Step: 67200 Epoch 29/50 Iteration: 67200 Avg. Training loss: 2.8587 0.0172 sec/batch\n",
      "Global Step: 67300 Epoch 29/50 Iteration: 67300 Avg. Training loss: 2.8587 0.0173 sec/batch\n",
      "Global Step: 67400 Epoch 29/50 Iteration: 67400 Avg. Training loss: 2.8475 0.0143 sec/batch\n",
      "Global Step: 67500 Epoch 29/50 Iteration: 67500 Avg. Training loss: 2.8783 0.0162 sec/batch\n",
      "Global Step: 67600 Epoch 29/50 Iteration: 67600 Avg. Training loss: 2.8358 0.0146 sec/batch\n",
      "Global Step: 67700 Epoch 29/50 Iteration: 67700 Avg. Training loss: 2.8432 0.0171 sec/batch\n",
      "Global Step: 67800 Epoch 29/50 Iteration: 67800 Avg. Training loss: 2.8625 0.0133 sec/batch\n",
      "Global Step: 67900 Epoch 29/50 Iteration: 67900 Avg. Training loss: 2.8385 0.0154 sec/batch\n",
      "Global Step: 68000 Epoch 29/50 Iteration: 68000 Avg. Training loss: 2.8606 0.0161 sec/batch\n",
      "Global Step: 68100 Epoch 29/50 Iteration: 68100 Avg. Training loss: 2.8533 0.0178 sec/batch\n",
      "Global Step: 68200 Epoch 29/50 Iteration: 68200 Avg. Training loss: 2.8337 0.0147 sec/batch\n",
      "Global Step: 68300 Epoch 29/50 Iteration: 68300 Avg. Training loss: 2.8782 0.0142 sec/batch\n",
      "Global Step: 68400 Epoch 29/50 Iteration: 68400 Avg. Training loss: 2.8537 0.0148 sec/batch\n",
      "Global Step: 68500 Epoch 29/50 Iteration: 68500 Avg. Training loss: 2.8706 0.0155 sec/batch\n",
      "Global Step: 68600 Epoch 29/50 Iteration: 68600 Avg. Training loss: 2.8400 0.0144 sec/batch\n",
      "Global Step: 68700 Epoch 29/50 Iteration: 68700 Avg. Training loss: 2.8288 0.0144 sec/batch\n",
      "Global Step: 68800 Epoch 29/50 Iteration: 68800 Avg. Training loss: 2.8570 0.0160 sec/batch\n",
      "Global Step: 68900 Epoch 29/50 Iteration: 68900 Avg. Training loss: 2.8468 0.0142 sec/batch\n",
      "Epoch 30/50 Threshold: 0.0654034721364846 Length of Training words: 2275887\n",
      "Global Step: 69000 Epoch 30/50 Iteration: 69000 Avg. Training loss: 2.8809 0.0074 sec/batch\n",
      "Global Step: 69100 Epoch 30/50 Iteration: 69100 Avg. Training loss: 2.9058 0.0126 sec/batch\n",
      "Global Step: 69200 Epoch 30/50 Iteration: 69200 Avg. Training loss: 2.9044 0.0157 sec/batch\n",
      "Global Step: 69300 Epoch 30/50 Iteration: 69300 Avg. Training loss: 2.8774 0.0135 sec/batch\n",
      "Global Step: 69400 Epoch 30/50 Iteration: 69400 Avg. Training loss: 2.8757 0.0148 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 69500 Epoch 30/50 Iteration: 69500 Avg. Training loss: 2.8781 0.0181 sec/batch\n",
      "Global Step: 69600 Epoch 30/50 Iteration: 69600 Avg. Training loss: 2.8759 0.0168 sec/batch\n",
      "Global Step: 69700 Epoch 30/50 Iteration: 69700 Avg. Training loss: 2.8965 0.0163 sec/batch\n",
      "Global Step: 69800 Epoch 30/50 Iteration: 69800 Avg. Training loss: 2.8986 0.0151 sec/batch\n",
      "Global Step: 69900 Epoch 30/50 Iteration: 69900 Avg. Training loss: 2.8659 0.0150 sec/batch\n",
      "Global Step: 70000 Epoch 30/50 Iteration: 70000 Avg. Training loss: 2.8674 0.0157 sec/batch\n",
      "Global Step: 70100 Epoch 30/50 Iteration: 70100 Avg. Training loss: 2.8952 0.0150 sec/batch\n",
      "Global Step: 70200 Epoch 30/50 Iteration: 70200 Avg. Training loss: 2.8774 0.0138 sec/batch\n",
      "Global Step: 70300 Epoch 30/50 Iteration: 70300 Avg. Training loss: 2.8865 0.0144 sec/batch\n",
      "Global Step: 70400 Epoch 30/50 Iteration: 70400 Avg. Training loss: 2.8862 0.0161 sec/batch\n",
      "Global Step: 70500 Epoch 30/50 Iteration: 70500 Avg. Training loss: 2.8653 0.0143 sec/batch\n",
      "Global Step: 70600 Epoch 30/50 Iteration: 70600 Avg. Training loss: 2.9084 0.0151 sec/batch\n",
      "Global Step: 70700 Epoch 30/50 Iteration: 70700 Avg. Training loss: 2.8830 0.0147 sec/batch\n",
      "Global Step: 70800 Epoch 30/50 Iteration: 70800 Avg. Training loss: 2.9020 0.0159 sec/batch\n",
      "Global Step: 70900 Epoch 30/50 Iteration: 70900 Avg. Training loss: 2.8643 0.0157 sec/batch\n",
      "Global Step: 71000 Epoch 30/50 Iteration: 71000 Avg. Training loss: 2.8571 0.0135 sec/batch\n",
      "Global Step: 71100 Epoch 30/50 Iteration: 71100 Avg. Training loss: 2.8914 0.0143 sec/batch\n",
      "Global Step: 71200 Epoch 30/50 Iteration: 71200 Avg. Training loss: 2.8760 0.0159 sec/batch\n",
      "Epoch 31/50 Threshold: 0.060578662616402365 Length of Training words: 2234747\n",
      "Global Step: 71300 Epoch 31/50 Iteration: 71300 Avg. Training loss: 2.9042 0.0133 sec/batch\n",
      "Global Step: 71400 Epoch 31/50 Iteration: 71400 Avg. Training loss: 2.9262 0.0137 sec/batch\n",
      "Global Step: 71500 Epoch 31/50 Iteration: 71500 Avg. Training loss: 2.9054 0.0148 sec/batch\n",
      "Global Step: 71600 Epoch 31/50 Iteration: 71600 Avg. Training loss: 2.8830 0.0137 sec/batch\n",
      "Global Step: 71700 Epoch 31/50 Iteration: 71700 Avg. Training loss: 2.8938 0.0169 sec/batch\n",
      "Global Step: 71800 Epoch 31/50 Iteration: 71800 Avg. Training loss: 2.8912 0.0160 sec/batch\n",
      "Global Step: 71900 Epoch 31/50 Iteration: 71900 Avg. Training loss: 2.9045 0.0142 sec/batch\n",
      "Global Step: 72000 Epoch 31/50 Iteration: 72000 Avg. Training loss: 2.8956 0.0136 sec/batch\n",
      "Global Step: 72100 Epoch 31/50 Iteration: 72100 Avg. Training loss: 2.9144 0.0149 sec/batch\n",
      "Global Step: 72200 Epoch 31/50 Iteration: 72200 Avg. Training loss: 2.8830 0.0165 sec/batch\n",
      "Global Step: 72300 Epoch 31/50 Iteration: 72300 Avg. Training loss: 2.8844 0.0176 sec/batch\n",
      "Global Step: 72400 Epoch 31/50 Iteration: 72400 Avg. Training loss: 2.8942 0.0157 sec/batch\n",
      "Global Step: 72500 Epoch 31/50 Iteration: 72500 Avg. Training loss: 2.8993 0.0172 sec/batch\n",
      "Global Step: 72600 Epoch 31/50 Iteration: 72600 Avg. Training loss: 2.8914 0.0152 sec/batch\n",
      "Global Step: 72700 Epoch 31/50 Iteration: 72700 Avg. Training loss: 2.9023 0.0141 sec/batch\n",
      "Global Step: 72800 Epoch 31/50 Iteration: 72800 Avg. Training loss: 2.8960 0.0148 sec/batch\n",
      "Global Step: 72900 Epoch 31/50 Iteration: 72900 Avg. Training loss: 2.9102 0.0161 sec/batch\n",
      "Global Step: 73000 Epoch 31/50 Iteration: 73000 Avg. Training loss: 2.9006 0.0173 sec/batch\n",
      "Global Step: 73100 Epoch 31/50 Iteration: 73100 Avg. Training loss: 2.9065 0.0148 sec/batch\n",
      "Global Step: 73200 Epoch 31/50 Iteration: 73200 Avg. Training loss: 2.8709 0.0145 sec/batch\n",
      "Global Step: 73300 Epoch 31/50 Iteration: 73300 Avg. Training loss: 2.8839 0.0138 sec/batch\n",
      "Global Step: 73400 Epoch 31/50 Iteration: 73400 Avg. Training loss: 2.8975 0.0167 sec/batch\n",
      "Epoch 32/50 Threshold: 0.0802312071663469 Length of Training words: 2387534\n",
      "Global Step: 73500 Epoch 32/50 Iteration: 73500 Avg. Training loss: 2.8713 0.0091 sec/batch\n",
      "Global Step: 73600 Epoch 32/50 Iteration: 73600 Avg. Training loss: 2.8711 0.0160 sec/batch\n",
      "Global Step: 73700 Epoch 32/50 Iteration: 73700 Avg. Training loss: 2.8705 0.0147 sec/batch\n",
      "Global Step: 73800 Epoch 32/50 Iteration: 73800 Avg. Training loss: 2.8468 0.0126 sec/batch\n",
      "Global Step: 73900 Epoch 32/50 Iteration: 73900 Avg. Training loss: 2.8437 0.0126 sec/batch\n",
      "Global Step: 74000 Epoch 32/50 Iteration: 74000 Avg. Training loss: 2.8388 0.0149 sec/batch\n",
      "Global Step: 74100 Epoch 32/50 Iteration: 74100 Avg. Training loss: 2.8522 0.0127 sec/batch\n",
      "Global Step: 74200 Epoch 32/50 Iteration: 74200 Avg. Training loss: 2.8525 0.0155 sec/batch\n",
      "Global Step: 74300 Epoch 32/50 Iteration: 74300 Avg. Training loss: 2.8508 0.0142 sec/batch\n",
      "Global Step: 74400 Epoch 32/50 Iteration: 74400 Avg. Training loss: 2.8595 0.0144 sec/batch\n",
      "Global Step: 74500 Epoch 32/50 Iteration: 74500 Avg. Training loss: 2.8337 0.0141 sec/batch\n",
      "Global Step: 74600 Epoch 32/50 Iteration: 74600 Avg. Training loss: 2.8405 0.0152 sec/batch\n",
      "Global Step: 74700 Epoch 32/50 Iteration: 74700 Avg. Training loss: 2.8522 0.0141 sec/batch\n",
      "Global Step: 74800 Epoch 32/50 Iteration: 74800 Avg. Training loss: 2.8450 0.0175 sec/batch\n",
      "Global Step: 74900 Epoch 32/50 Iteration: 74900 Avg. Training loss: 2.8539 0.0170 sec/batch\n",
      "Global Step: 75000 Epoch 32/50 Iteration: 75000 Avg. Training loss: 2.8420 0.0170 sec/batch\n",
      "Global Step: 75100 Epoch 32/50 Iteration: 75100 Avg. Training loss: 2.8333 0.0126 sec/batch\n",
      "Global Step: 75200 Epoch 32/50 Iteration: 75200 Avg. Training loss: 2.8720 0.0178 sec/batch\n",
      "Global Step: 75300 Epoch 32/50 Iteration: 75300 Avg. Training loss: 2.8506 0.0153 sec/batch\n",
      "Global Step: 75400 Epoch 32/50 Iteration: 75400 Avg. Training loss: 2.8680 0.0153 sec/batch\n",
      "Global Step: 75500 Epoch 32/50 Iteration: 75500 Avg. Training loss: 2.8328 0.0161 sec/batch\n",
      "Global Step: 75600 Epoch 32/50 Iteration: 75600 Avg. Training loss: 2.8223 0.0147 sec/batch\n",
      "Global Step: 75700 Epoch 32/50 Iteration: 75700 Avg. Training loss: 2.8545 0.0159 sec/batch\n",
      "Global Step: 75800 Epoch 32/50 Iteration: 75800 Avg. Training loss: 2.8407 0.0147 sec/batch\n",
      "Epoch 33/50 Threshold: 0.06324289916713363 Length of Training words: 2258450\n",
      "Global Step: 75900 Epoch 33/50 Iteration: 75900 Avg. Training loss: 2.8857 0.0120 sec/batch\n",
      "Global Step: 76000 Epoch 33/50 Iteration: 76000 Avg. Training loss: 2.9106 0.0153 sec/batch\n",
      "Global Step: 76100 Epoch 33/50 Iteration: 76100 Avg. Training loss: 2.9095 0.0159 sec/batch\n",
      "Global Step: 76200 Epoch 33/50 Iteration: 76200 Avg. Training loss: 2.8801 0.0162 sec/batch\n",
      "Global Step: 76300 Epoch 33/50 Iteration: 76300 Avg. Training loss: 2.8828 0.0148 sec/batch\n",
      "Global Step: 76400 Epoch 33/50 Iteration: 76400 Avg. Training loss: 2.8858 0.0148 sec/batch\n",
      "Global Step: 76500 Epoch 33/50 Iteration: 76500 Avg. Training loss: 2.8860 0.0133 sec/batch\n",
      "Global Step: 76600 Epoch 33/50 Iteration: 76600 Avg. Training loss: 2.8963 0.0137 sec/batch\n",
      "Global Step: 76700 Epoch 33/50 Iteration: 76700 Avg. Training loss: 2.9017 0.0129 sec/batch\n",
      "Global Step: 76800 Epoch 33/50 Iteration: 76800 Avg. Training loss: 2.8796 0.0135 sec/batch\n",
      "Global Step: 76900 Epoch 33/50 Iteration: 76900 Avg. Training loss: 2.8701 0.0151 sec/batch\n",
      "Global Step: 77000 Epoch 33/50 Iteration: 77000 Avg. Training loss: 2.9061 0.0164 sec/batch\n",
      "Global Step: 77100 Epoch 33/50 Iteration: 77100 Avg. Training loss: 2.8756 0.0163 sec/batch\n",
      "Global Step: 77200 Epoch 33/50 Iteration: 77200 Avg. Training loss: 2.8963 0.0188 sec/batch\n",
      "Global Step: 77300 Epoch 33/50 Iteration: 77300 Avg. Training loss: 2.8865 0.0152 sec/batch\n",
      "Global Step: 77400 Epoch 33/50 Iteration: 77400 Avg. Training loss: 2.8772 0.0142 sec/batch\n",
      "Global Step: 77500 Epoch 33/50 Iteration: 77500 Avg. Training loss: 2.9122 0.0154 sec/batch\n",
      "Global Step: 77600 Epoch 33/50 Iteration: 77600 Avg. Training loss: 2.8915 0.0157 sec/batch\n",
      "Global Step: 77700 Epoch 33/50 Iteration: 77700 Avg. Training loss: 2.9027 0.0142 sec/batch\n",
      "Global Step: 77800 Epoch 33/50 Iteration: 77800 Avg. Training loss: 2.8703 0.0156 sec/batch\n",
      "Global Step: 77900 Epoch 33/50 Iteration: 77900 Avg. Training loss: 2.8723 0.0163 sec/batch\n",
      "Global Step: 78000 Epoch 33/50 Iteration: 78000 Avg. Training loss: 2.8794 0.0150 sec/batch\n",
      "Epoch 34/50 Threshold: 0.07746417489937713 Length of Training words: 2369277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 78100 Epoch 34/50 Iteration: 78100 Avg. Training loss: 2.8833 0.0020 sec/batch\n",
      "Global Step: 78200 Epoch 34/50 Iteration: 78200 Avg. Training loss: 2.8695 0.0169 sec/batch\n",
      "Global Step: 78300 Epoch 34/50 Iteration: 78300 Avg. Training loss: 2.8849 0.0144 sec/batch\n",
      "Global Step: 78400 Epoch 34/50 Iteration: 78400 Avg. Training loss: 2.8621 0.0124 sec/batch\n",
      "Global Step: 78500 Epoch 34/50 Iteration: 78500 Avg. Training loss: 2.8413 0.0143 sec/batch\n",
      "Global Step: 78600 Epoch 34/50 Iteration: 78600 Avg. Training loss: 2.8482 0.0140 sec/batch\n",
      "Global Step: 78700 Epoch 34/50 Iteration: 78700 Avg. Training loss: 2.8491 0.0151 sec/batch\n",
      "Global Step: 78800 Epoch 34/50 Iteration: 78800 Avg. Training loss: 2.8581 0.0144 sec/batch\n",
      "Global Step: 78900 Epoch 34/50 Iteration: 78900 Avg. Training loss: 2.8598 0.0135 sec/batch\n",
      "Global Step: 79000 Epoch 34/50 Iteration: 79000 Avg. Training loss: 2.8737 0.0139 sec/batch\n",
      "Global Step: 79100 Epoch 34/50 Iteration: 79100 Avg. Training loss: 2.8411 0.0155 sec/batch\n",
      "Global Step: 79200 Epoch 34/50 Iteration: 79200 Avg. Training loss: 2.8360 0.0127 sec/batch\n",
      "Global Step: 79300 Epoch 34/50 Iteration: 79300 Avg. Training loss: 2.8640 0.0158 sec/batch\n",
      "Global Step: 79400 Epoch 34/50 Iteration: 79400 Avg. Training loss: 2.8485 0.0161 sec/batch\n",
      "Global Step: 79500 Epoch 34/50 Iteration: 79500 Avg. Training loss: 2.8551 0.0152 sec/batch\n",
      "Global Step: 79600 Epoch 34/50 Iteration: 79600 Avg. Training loss: 2.8634 0.0156 sec/batch\n",
      "Global Step: 79700 Epoch 34/50 Iteration: 79700 Avg. Training loss: 2.8329 0.0141 sec/batch\n",
      "Global Step: 79800 Epoch 34/50 Iteration: 79800 Avg. Training loss: 2.8742 0.0139 sec/batch\n",
      "Global Step: 79900 Epoch 34/50 Iteration: 79900 Avg. Training loss: 2.8585 0.0163 sec/batch\n",
      "Global Step: 80000 Epoch 34/50 Iteration: 80000 Avg. Training loss: 2.8706 0.0178 sec/batch\n",
      "Global Step: 80100 Epoch 34/50 Iteration: 80100 Avg. Training loss: 2.8558 0.0120 sec/batch\n",
      "Global Step: 80200 Epoch 34/50 Iteration: 80200 Avg. Training loss: 2.8265 0.0132 sec/batch\n",
      "Global Step: 80300 Epoch 34/50 Iteration: 80300 Avg. Training loss: 2.8422 0.0160 sec/batch\n",
      "Global Step: 80400 Epoch 34/50 Iteration: 80400 Avg. Training loss: 2.8493 0.0157 sec/batch\n",
      "Epoch 35/50 Threshold: 0.08898776615666429 Length of Training words: 2440799\n",
      "Global Step: 80500 Epoch 35/50 Iteration: 80500 Avg. Training loss: 2.8503 0.0068 sec/batch\n",
      "Global Step: 80600 Epoch 35/50 Iteration: 80600 Avg. Training loss: 2.8414 0.0151 sec/batch\n",
      "Global Step: 80700 Epoch 35/50 Iteration: 80700 Avg. Training loss: 2.8569 0.0150 sec/batch\n",
      "Global Step: 80800 Epoch 35/50 Iteration: 80800 Avg. Training loss: 2.8380 0.0153 sec/batch\n",
      "Global Step: 80900 Epoch 35/50 Iteration: 80900 Avg. Training loss: 2.8241 0.0176 sec/batch\n",
      "Global Step: 81000 Epoch 35/50 Iteration: 81000 Avg. Training loss: 2.8170 0.0160 sec/batch\n",
      "Global Step: 81100 Epoch 35/50 Iteration: 81100 Avg. Training loss: 2.8271 0.0163 sec/batch\n",
      "Global Step: 81200 Epoch 35/50 Iteration: 81200 Avg. Training loss: 2.8338 0.0138 sec/batch\n",
      "Global Step: 81300 Epoch 35/50 Iteration: 81300 Avg. Training loss: 2.8322 0.0136 sec/batch\n",
      "Global Step: 81400 Epoch 35/50 Iteration: 81400 Avg. Training loss: 2.8520 0.0137 sec/batch\n",
      "Global Step: 81500 Epoch 35/50 Iteration: 81500 Avg. Training loss: 2.8154 0.0151 sec/batch\n",
      "Global Step: 81600 Epoch 35/50 Iteration: 81600 Avg. Training loss: 2.8102 0.0130 sec/batch\n",
      "Global Step: 81700 Epoch 35/50 Iteration: 81700 Avg. Training loss: 2.8400 0.0128 sec/batch\n",
      "Global Step: 81800 Epoch 35/50 Iteration: 81800 Avg. Training loss: 2.8253 0.0143 sec/batch\n",
      "Global Step: 81900 Epoch 35/50 Iteration: 81900 Avg. Training loss: 2.8221 0.0161 sec/batch\n",
      "Global Step: 82000 Epoch 35/50 Iteration: 82000 Avg. Training loss: 2.8401 0.0177 sec/batch\n",
      "Global Step: 82100 Epoch 35/50 Iteration: 82100 Avg. Training loss: 2.8228 0.0163 sec/batch\n",
      "Global Step: 82200 Epoch 35/50 Iteration: 82200 Avg. Training loss: 2.8296 0.0148 sec/batch\n",
      "Global Step: 82300 Epoch 35/50 Iteration: 82300 Avg. Training loss: 2.8532 0.0159 sec/batch\n",
      "Global Step: 82400 Epoch 35/50 Iteration: 82400 Avg. Training loss: 2.8319 0.0166 sec/batch\n",
      "Global Step: 82500 Epoch 35/50 Iteration: 82500 Avg. Training loss: 2.8408 0.0160 sec/batch\n",
      "Global Step: 82600 Epoch 35/50 Iteration: 82600 Avg. Training loss: 2.8031 0.0148 sec/batch\n",
      "Global Step: 82700 Epoch 35/50 Iteration: 82700 Avg. Training loss: 2.8184 0.0164 sec/batch\n",
      "Global Step: 82800 Epoch 35/50 Iteration: 82800 Avg. Training loss: 2.8197 0.0151 sec/batch\n",
      "Epoch 36/50 Threshold: 0.07534745402699021 Length of Training words: 2355673\n",
      "Global Step: 82900 Epoch 36/50 Iteration: 82900 Avg. Training loss: 2.8268 0.0005 sec/batch\n",
      "Global Step: 83000 Epoch 36/50 Iteration: 83000 Avg. Training loss: 2.8803 0.0144 sec/batch\n",
      "Global Step: 83100 Epoch 36/50 Iteration: 83100 Avg. Training loss: 2.8871 0.0165 sec/batch\n",
      "Global Step: 83200 Epoch 36/50 Iteration: 83200 Avg. Training loss: 2.8660 0.0155 sec/batch\n",
      "Global Step: 83300 Epoch 36/50 Iteration: 83300 Avg. Training loss: 2.8487 0.0161 sec/batch\n",
      "Global Step: 83400 Epoch 36/50 Iteration: 83400 Avg. Training loss: 2.8498 0.0166 sec/batch\n",
      "Global Step: 83500 Epoch 36/50 Iteration: 83500 Avg. Training loss: 2.8536 0.0141 sec/batch\n",
      "Global Step: 83600 Epoch 36/50 Iteration: 83600 Avg. Training loss: 2.8602 0.0123 sec/batch\n",
      "Global Step: 83700 Epoch 36/50 Iteration: 83700 Avg. Training loss: 2.8686 0.0147 sec/batch\n",
      "Global Step: 83800 Epoch 36/50 Iteration: 83800 Avg. Training loss: 2.8764 0.0137 sec/batch\n",
      "Global Step: 83900 Epoch 36/50 Iteration: 83900 Avg. Training loss: 2.8483 0.0134 sec/batch\n",
      "Global Step: 84000 Epoch 36/50 Iteration: 84000 Avg. Training loss: 2.8404 0.0154 sec/batch\n",
      "Global Step: 84100 Epoch 36/50 Iteration: 84100 Avg. Training loss: 2.8691 0.0132 sec/batch\n",
      "Global Step: 84200 Epoch 36/50 Iteration: 84200 Avg. Training loss: 2.8506 0.0136 sec/batch\n",
      "Global Step: 84300 Epoch 36/50 Iteration: 84300 Avg. Training loss: 2.8566 0.0149 sec/batch\n",
      "Global Step: 84400 Epoch 36/50 Iteration: 84400 Avg. Training loss: 2.8662 0.0173 sec/batch\n",
      "Global Step: 84500 Epoch 36/50 Iteration: 84500 Avg. Training loss: 2.8357 0.0162 sec/batch\n",
      "Global Step: 84600 Epoch 36/50 Iteration: 84600 Avg. Training loss: 2.8759 0.0141 sec/batch\n",
      "Global Step: 84700 Epoch 36/50 Iteration: 84700 Avg. Training loss: 2.8624 0.0146 sec/batch\n",
      "Global Step: 84800 Epoch 36/50 Iteration: 84800 Avg. Training loss: 2.8741 0.0152 sec/batch\n",
      "Global Step: 84900 Epoch 36/50 Iteration: 84900 Avg. Training loss: 2.8591 0.0141 sec/batch\n",
      "Global Step: 85000 Epoch 36/50 Iteration: 85000 Avg. Training loss: 2.8377 0.0164 sec/batch\n",
      "Global Step: 85100 Epoch 36/50 Iteration: 85100 Avg. Training loss: 2.8441 0.0161 sec/batch\n",
      "Global Step: 85200 Epoch 36/50 Iteration: 85200 Avg. Training loss: 2.8583 0.0158 sec/batch\n",
      "Epoch 37/50 Threshold: 0.08809174339175921 Length of Training words: 2435024\n",
      "Global Step: 85300 Epoch 37/50 Iteration: 85300 Avg. Training loss: 2.8461 0.0072 sec/batch\n",
      "Global Step: 85400 Epoch 37/50 Iteration: 85400 Avg. Training loss: 2.8475 0.0144 sec/batch\n",
      "Global Step: 85500 Epoch 37/50 Iteration: 85500 Avg. Training loss: 2.8557 0.0151 sec/batch\n",
      "Global Step: 85600 Epoch 37/50 Iteration: 85600 Avg. Training loss: 2.8360 0.0150 sec/batch\n",
      "Global Step: 85700 Epoch 37/50 Iteration: 85700 Avg. Training loss: 2.8275 0.0140 sec/batch\n",
      "Global Step: 85800 Epoch 37/50 Iteration: 85800 Avg. Training loss: 2.8168 0.0154 sec/batch\n",
      "Global Step: 85900 Epoch 37/50 Iteration: 85900 Avg. Training loss: 2.8360 0.0179 sec/batch\n",
      "Global Step: 86000 Epoch 37/50 Iteration: 86000 Avg. Training loss: 2.8351 0.0162 sec/batch\n",
      "Global Step: 86100 Epoch 37/50 Iteration: 86100 Avg. Training loss: 2.8330 0.0163 sec/batch\n",
      "Global Step: 86200 Epoch 37/50 Iteration: 86200 Avg. Training loss: 2.8584 0.0152 sec/batch\n",
      "Global Step: 86300 Epoch 37/50 Iteration: 86300 Avg. Training loss: 2.8175 0.0149 sec/batch\n",
      "Global Step: 86400 Epoch 37/50 Iteration: 86400 Avg. Training loss: 2.8073 0.0147 sec/batch\n",
      "Global Step: 86500 Epoch 37/50 Iteration: 86500 Avg. Training loss: 2.8441 0.0155 sec/batch\n",
      "Global Step: 86600 Epoch 37/50 Iteration: 86600 Avg. Training loss: 2.8234 0.0148 sec/batch\n",
      "Global Step: 86700 Epoch 37/50 Iteration: 86700 Avg. Training loss: 2.8302 0.0146 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 86800 Epoch 37/50 Iteration: 86800 Avg. Training loss: 2.8378 0.0138 sec/batch\n",
      "Global Step: 86900 Epoch 37/50 Iteration: 86900 Avg. Training loss: 2.8186 0.0152 sec/batch\n",
      "Global Step: 87000 Epoch 37/50 Iteration: 87000 Avg. Training loss: 2.8400 0.0166 sec/batch\n",
      "Global Step: 87100 Epoch 37/50 Iteration: 87100 Avg. Training loss: 2.8483 0.0149 sec/batch\n",
      "Global Step: 87200 Epoch 37/50 Iteration: 87200 Avg. Training loss: 2.8330 0.0185 sec/batch\n",
      "Global Step: 87300 Epoch 37/50 Iteration: 87300 Avg. Training loss: 2.8375 0.0140 sec/batch\n",
      "Global Step: 87400 Epoch 37/50 Iteration: 87400 Avg. Training loss: 2.8104 0.0163 sec/batch\n",
      "Global Step: 87500 Epoch 37/50 Iteration: 87500 Avg. Training loss: 2.8165 0.0159 sec/batch\n",
      "Global Step: 87600 Epoch 37/50 Iteration: 87600 Avg. Training loss: 2.8289 0.0159 sec/batch\n",
      "Epoch 38/50 Threshold: 0.07277438964797873 Length of Training words: 2336721\n",
      "Global Step: 87700 Epoch 38/50 Iteration: 87700 Avg. Training loss: 2.8265 0.0021 sec/batch\n",
      "Global Step: 87800 Epoch 38/50 Iteration: 87800 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 87900 Epoch 38/50 Iteration: 87900 Avg. Training loss: 2.8937 0.0167 sec/batch\n",
      "Global Step: 88000 Epoch 38/50 Iteration: 88000 Avg. Training loss: 2.8696 0.0152 sec/batch\n",
      "Global Step: 88100 Epoch 38/50 Iteration: 88100 Avg. Training loss: 2.8591 0.0154 sec/batch\n",
      "Global Step: 88200 Epoch 38/50 Iteration: 88200 Avg. Training loss: 2.8545 0.0150 sec/batch\n",
      "Global Step: 88300 Epoch 38/50 Iteration: 88300 Avg. Training loss: 2.8603 0.0136 sec/batch\n",
      "Global Step: 88400 Epoch 38/50 Iteration: 88400 Avg. Training loss: 2.8678 0.0148 sec/batch\n",
      "Global Step: 88500 Epoch 38/50 Iteration: 88500 Avg. Training loss: 2.8669 0.0148 sec/batch\n",
      "Global Step: 88600 Epoch 38/50 Iteration: 88600 Avg. Training loss: 2.8888 0.0155 sec/batch\n",
      "Global Step: 88700 Epoch 38/50 Iteration: 88700 Avg. Training loss: 2.8488 0.0144 sec/batch\n",
      "Global Step: 88800 Epoch 38/50 Iteration: 88800 Avg. Training loss: 2.8481 0.0158 sec/batch\n",
      "Global Step: 88900 Epoch 38/50 Iteration: 88900 Avg. Training loss: 2.8802 0.0144 sec/batch\n",
      "Global Step: 89000 Epoch 38/50 Iteration: 89000 Avg. Training loss: 2.8504 0.0139 sec/batch\n",
      "Global Step: 89100 Epoch 38/50 Iteration: 89100 Avg. Training loss: 2.8718 0.0139 sec/batch\n",
      "Global Step: 89200 Epoch 38/50 Iteration: 89200 Avg. Training loss: 2.8640 0.0132 sec/batch\n",
      "Global Step: 89300 Epoch 38/50 Iteration: 89300 Avg. Training loss: 2.8444 0.0161 sec/batch\n",
      "Global Step: 89400 Epoch 38/50 Iteration: 89400 Avg. Training loss: 2.8877 0.0164 sec/batch\n",
      "Global Step: 89500 Epoch 38/50 Iteration: 89500 Avg. Training loss: 2.8663 0.0166 sec/batch\n",
      "Global Step: 89600 Epoch 38/50 Iteration: 89600 Avg. Training loss: 2.8782 0.0177 sec/batch\n",
      "Global Step: 89700 Epoch 38/50 Iteration: 89700 Avg. Training loss: 2.8529 0.0159 sec/batch\n",
      "Global Step: 89800 Epoch 38/50 Iteration: 89800 Avg. Training loss: 2.8370 0.0167 sec/batch\n",
      "Global Step: 89900 Epoch 38/50 Iteration: 89900 Avg. Training loss: 2.8713 0.0162 sec/batch\n",
      "Global Step: 90000 Epoch 38/50 Iteration: 90000 Avg. Training loss: 2.8581 0.0159 sec/batch\n",
      "Epoch 39/50 Threshold: 0.086121294696839 Length of Training words: 2423215\n",
      "Global Step: 90100 Epoch 39/50 Iteration: 90100 Avg. Training loss: 2.8511 0.0120 sec/batch\n",
      "Global Step: 90200 Epoch 39/50 Iteration: 90200 Avg. Training loss: 2.8597 0.0154 sec/batch\n",
      "Global Step: 90300 Epoch 39/50 Iteration: 90300 Avg. Training loss: 2.8542 0.0154 sec/batch\n",
      "Global Step: 90400 Epoch 39/50 Iteration: 90400 Avg. Training loss: 2.8285 0.0158 sec/batch\n",
      "Global Step: 90500 Epoch 39/50 Iteration: 90500 Avg. Training loss: 2.8313 0.0158 sec/batch\n",
      "Global Step: 90600 Epoch 39/50 Iteration: 90600 Avg. Training loss: 2.8206 0.0167 sec/batch\n",
      "Global Step: 90700 Epoch 39/50 Iteration: 90700 Avg. Training loss: 2.8388 0.0134 sec/batch\n",
      "Global Step: 90800 Epoch 39/50 Iteration: 90800 Avg. Training loss: 2.8449 0.0150 sec/batch\n",
      "Global Step: 90900 Epoch 39/50 Iteration: 90900 Avg. Training loss: 2.8452 0.0154 sec/batch\n",
      "Global Step: 91000 Epoch 39/50 Iteration: 91000 Avg. Training loss: 2.8437 0.0156 sec/batch\n",
      "Global Step: 91100 Epoch 39/50 Iteration: 91100 Avg. Training loss: 2.8218 0.0153 sec/batch\n",
      "Global Step: 91200 Epoch 39/50 Iteration: 91200 Avg. Training loss: 2.8275 0.0127 sec/batch\n",
      "Global Step: 91300 Epoch 39/50 Iteration: 91300 Avg. Training loss: 2.8395 0.0145 sec/batch\n",
      "Global Step: 91400 Epoch 39/50 Iteration: 91400 Avg. Training loss: 2.8329 0.0142 sec/batch\n",
      "Global Step: 91500 Epoch 39/50 Iteration: 91500 Avg. Training loss: 2.8406 0.0128 sec/batch\n",
      "Global Step: 91600 Epoch 39/50 Iteration: 91600 Avg. Training loss: 2.8300 0.0147 sec/batch\n",
      "Global Step: 91700 Epoch 39/50 Iteration: 91700 Avg. Training loss: 2.8212 0.0161 sec/batch\n",
      "Global Step: 91800 Epoch 39/50 Iteration: 91800 Avg. Training loss: 2.8601 0.0151 sec/batch\n",
      "Global Step: 91900 Epoch 39/50 Iteration: 91900 Avg. Training loss: 2.8365 0.0163 sec/batch\n",
      "Global Step: 92000 Epoch 39/50 Iteration: 92000 Avg. Training loss: 2.8510 0.0157 sec/batch\n",
      "Global Step: 92100 Epoch 39/50 Iteration: 92100 Avg. Training loss: 2.8259 0.0174 sec/batch\n",
      "Global Step: 92200 Epoch 39/50 Iteration: 92200 Avg. Training loss: 2.8148 0.0173 sec/batch\n",
      "Global Step: 92300 Epoch 39/50 Iteration: 92300 Avg. Training loss: 2.8339 0.0164 sec/batch\n",
      "Global Step: 92400 Epoch 39/50 Iteration: 92400 Avg. Training loss: 2.8303 0.0153 sec/batch\n",
      "Epoch 40/50 Threshold: 0.07775738919778097 Length of Training words: 2370779\n",
      "Global Step: 92500 Epoch 40/50 Iteration: 92500 Avg. Training loss: 2.8442 0.0083 sec/batch\n",
      "Global Step: 92600 Epoch 40/50 Iteration: 92600 Avg. Training loss: 2.8734 0.0159 sec/batch\n",
      "Global Step: 92700 Epoch 40/50 Iteration: 92700 Avg. Training loss: 2.8727 0.0155 sec/batch\n",
      "Global Step: 92800 Epoch 40/50 Iteration: 92800 Avg. Training loss: 2.8527 0.0161 sec/batch\n",
      "Global Step: 92900 Epoch 40/50 Iteration: 92900 Avg. Training loss: 2.8473 0.0153 sec/batch\n",
      "Global Step: 93000 Epoch 40/50 Iteration: 93000 Avg. Training loss: 2.8441 0.0145 sec/batch\n",
      "Global Step: 93100 Epoch 40/50 Iteration: 93100 Avg. Training loss: 2.8565 0.0141 sec/batch\n",
      "Global Step: 93200 Epoch 40/50 Iteration: 93200 Avg. Training loss: 2.8573 0.0162 sec/batch\n",
      "Global Step: 93300 Epoch 40/50 Iteration: 93300 Avg. Training loss: 2.8574 0.0136 sec/batch\n",
      "Global Step: 93400 Epoch 40/50 Iteration: 93400 Avg. Training loss: 2.8654 0.0169 sec/batch\n",
      "Global Step: 93500 Epoch 40/50 Iteration: 93500 Avg. Training loss: 2.8369 0.0162 sec/batch\n",
      "Global Step: 93600 Epoch 40/50 Iteration: 93600 Avg. Training loss: 2.8472 0.0164 sec/batch\n",
      "Global Step: 93700 Epoch 40/50 Iteration: 93700 Avg. Training loss: 2.8550 0.0155 sec/batch\n",
      "Global Step: 93800 Epoch 40/50 Iteration: 93800 Avg. Training loss: 2.8572 0.0164 sec/batch\n",
      "Global Step: 93900 Epoch 40/50 Iteration: 93900 Avg. Training loss: 2.8508 0.0150 sec/batch\n",
      "Global Step: 94000 Epoch 40/50 Iteration: 94000 Avg. Training loss: 2.8506 0.0128 sec/batch\n",
      "Global Step: 94100 Epoch 40/50 Iteration: 94100 Avg. Training loss: 2.8424 0.0131 sec/batch\n",
      "Global Step: 94200 Epoch 40/50 Iteration: 94200 Avg. Training loss: 2.8757 0.0172 sec/batch\n",
      "Global Step: 94300 Epoch 40/50 Iteration: 94300 Avg. Training loss: 2.8537 0.0172 sec/batch\n",
      "Global Step: 94400 Epoch 40/50 Iteration: 94400 Avg. Training loss: 2.8676 0.0167 sec/batch\n",
      "Global Step: 94500 Epoch 40/50 Iteration: 94500 Avg. Training loss: 2.8301 0.0161 sec/batch\n",
      "Global Step: 94600 Epoch 40/50 Iteration: 94600 Avg. Training loss: 2.8309 0.0158 sec/batch\n",
      "Global Step: 94700 Epoch 40/50 Iteration: 94700 Avg. Training loss: 2.8634 0.0155 sec/batch\n",
      "Global Step: 94800 Epoch 40/50 Iteration: 94800 Avg. Training loss: 2.8429 0.0156 sec/batch\n",
      "Epoch 41/50 Threshold: 0.06232496225957763 Length of Training words: 2250332\n",
      "Global Step: 94900 Epoch 41/50 Iteration: 94900 Avg. Training loss: 2.8899 0.0141 sec/batch\n",
      "Global Step: 95000 Epoch 41/50 Iteration: 95000 Avg. Training loss: 2.9321 0.0138 sec/batch\n",
      "Global Step: 95100 Epoch 41/50 Iteration: 95100 Avg. Training loss: 2.9010 0.0171 sec/batch\n",
      "Global Step: 95200 Epoch 41/50 Iteration: 95200 Avg. Training loss: 2.8778 0.0167 sec/batch\n",
      "Global Step: 95300 Epoch 41/50 Iteration: 95300 Avg. Training loss: 2.8830 0.0155 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 95400 Epoch 41/50 Iteration: 95400 Avg. Training loss: 2.8868 0.0145 sec/batch\n",
      "Global Step: 95500 Epoch 41/50 Iteration: 95500 Avg. Training loss: 2.8928 0.0157 sec/batch\n",
      "Global Step: 95600 Epoch 41/50 Iteration: 95600 Avg. Training loss: 2.8940 0.0146 sec/batch\n",
      "Global Step: 95700 Epoch 41/50 Iteration: 95700 Avg. Training loss: 2.9165 0.0161 sec/batch\n",
      "Global Step: 95800 Epoch 41/50 Iteration: 95800 Avg. Training loss: 2.8731 0.0170 sec/batch\n",
      "Global Step: 95900 Epoch 41/50 Iteration: 95900 Avg. Training loss: 2.8816 0.0140 sec/batch\n",
      "Global Step: 96000 Epoch 41/50 Iteration: 96000 Avg. Training loss: 2.8972 0.0143 sec/batch\n",
      "Global Step: 96100 Epoch 41/50 Iteration: 96100 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 96200 Epoch 41/50 Iteration: 96200 Avg. Training loss: 2.8846 0.0137 sec/batch\n",
      "Global Step: 96300 Epoch 41/50 Iteration: 96300 Avg. Training loss: 2.8934 0.0146 sec/batch\n",
      "Global Step: 96400 Epoch 41/50 Iteration: 96400 Avg. Training loss: 2.8802 0.0138 sec/batch\n",
      "Global Step: 96500 Epoch 41/50 Iteration: 96500 Avg. Training loss: 2.9145 0.0160 sec/batch\n",
      "Global Step: 96600 Epoch 41/50 Iteration: 96600 Avg. Training loss: 2.8921 0.0145 sec/batch\n",
      "Global Step: 96700 Epoch 41/50 Iteration: 96700 Avg. Training loss: 2.8998 0.0132 sec/batch\n",
      "Global Step: 96800 Epoch 41/50 Iteration: 96800 Avg. Training loss: 2.8715 0.0166 sec/batch\n",
      "Global Step: 96900 Epoch 41/50 Iteration: 96900 Avg. Training loss: 2.8779 0.0163 sec/batch\n",
      "Global Step: 97000 Epoch 41/50 Iteration: 97000 Avg. Training loss: 2.8849 0.0165 sec/batch\n",
      "Epoch 42/50 Threshold: 0.06905878343510341 Length of Training words: 2305711\n",
      "Global Step: 97100 Epoch 42/50 Iteration: 97100 Avg. Training loss: 2.8886 0.0056 sec/batch\n",
      "Global Step: 97200 Epoch 42/50 Iteration: 97200 Avg. Training loss: 2.8876 0.0161 sec/batch\n",
      "Global Step: 97300 Epoch 42/50 Iteration: 97300 Avg. Training loss: 2.9018 0.0153 sec/batch\n",
      "Global Step: 97400 Epoch 42/50 Iteration: 97400 Avg. Training loss: 2.8790 0.0148 sec/batch\n",
      "Global Step: 97500 Epoch 42/50 Iteration: 97500 Avg. Training loss: 2.8686 0.0143 sec/batch\n",
      "Global Step: 97600 Epoch 42/50 Iteration: 97600 Avg. Training loss: 2.8547 0.0159 sec/batch\n",
      "Global Step: 97700 Epoch 42/50 Iteration: 97700 Avg. Training loss: 2.8821 0.0165 sec/batch\n",
      "Global Step: 97800 Epoch 42/50 Iteration: 97800 Avg. Training loss: 2.8767 0.0151 sec/batch\n",
      "Global Step: 97900 Epoch 42/50 Iteration: 97900 Avg. Training loss: 2.8810 0.0187 sec/batch\n",
      "Global Step: 98000 Epoch 42/50 Iteration: 98000 Avg. Training loss: 2.8814 0.0158 sec/batch\n",
      "Global Step: 98100 Epoch 42/50 Iteration: 98100 Avg. Training loss: 2.8569 0.0162 sec/batch\n",
      "Global Step: 98200 Epoch 42/50 Iteration: 98200 Avg. Training loss: 2.8645 0.0189 sec/batch\n",
      "Global Step: 98300 Epoch 42/50 Iteration: 98300 Avg. Training loss: 2.8741 0.0150 sec/batch\n",
      "Global Step: 98400 Epoch 42/50 Iteration: 98400 Avg. Training loss: 2.8784 0.0151 sec/batch\n",
      "Global Step: 98500 Epoch 42/50 Iteration: 98500 Avg. Training loss: 2.8714 0.0166 sec/batch\n",
      "Global Step: 98600 Epoch 42/50 Iteration: 98600 Avg. Training loss: 2.8792 0.0173 sec/batch\n",
      "Global Step: 98700 Epoch 42/50 Iteration: 98700 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 98800 Epoch 42/50 Iteration: 98800 Avg. Training loss: 2.8923 0.0146 sec/batch\n",
      "Global Step: 98900 Epoch 42/50 Iteration: 98900 Avg. Training loss: 2.8740 0.0138 sec/batch\n",
      "Global Step: 99000 Epoch 42/50 Iteration: 99000 Avg. Training loss: 2.8820 0.0144 sec/batch\n",
      "Global Step: 99100 Epoch 42/50 Iteration: 99100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 99200 Epoch 42/50 Iteration: 99200 Avg. Training loss: 2.8596 0.0159 sec/batch\n",
      "Global Step: 99300 Epoch 42/50 Iteration: 99300 Avg. Training loss: 2.8690 0.0170 sec/batch\n",
      "Epoch 43/50 Threshold: 0.06065749365533461 Length of Training words: 2235476\n",
      "Global Step: 99400 Epoch 43/50 Iteration: 99400 Avg. Training loss: 2.8792 0.0034 sec/batch\n",
      "Global Step: 99500 Epoch 43/50 Iteration: 99500 Avg. Training loss: 2.9161 0.0162 sec/batch\n",
      "Global Step: 99600 Epoch 43/50 Iteration: 99600 Avg. Training loss: 2.9233 0.0153 sec/batch\n",
      "Global Step: 99700 Epoch 43/50 Iteration: 99700 Avg. Training loss: 2.8968 0.0151 sec/batch\n",
      "Global Step: 99800 Epoch 43/50 Iteration: 99800 Avg. Training loss: 2.8904 0.0156 sec/batch\n",
      "Global Step: 99900 Epoch 43/50 Iteration: 99900 Avg. Training loss: 2.8856 0.0143 sec/batch\n",
      "Global Step: 100000 Epoch 43/50 Iteration: 100000 Avg. Training loss: 2.8945 0.0159 sec/batch\n",
      "Global Step: 100100 Epoch 43/50 Iteration: 100100 Avg. Training loss: 2.9092 0.0124 sec/batch\n",
      "Global Step: 100200 Epoch 43/50 Iteration: 100200 Avg. Training loss: 2.9013 0.0134 sec/batch\n",
      "Global Step: 100300 Epoch 43/50 Iteration: 100300 Avg. Training loss: 2.8949 0.0157 sec/batch\n",
      "Global Step: 100400 Epoch 43/50 Iteration: 100400 Avg. Training loss: 2.8842 0.0185 sec/batch\n",
      "Global Step: 100500 Epoch 43/50 Iteration: 100500 Avg. Training loss: 2.8981 0.0157 sec/batch\n",
      "Global Step: 100600 Epoch 43/50 Iteration: 100600 Avg. Training loss: 2.8920 0.0155 sec/batch\n",
      "Global Step: 100700 Epoch 43/50 Iteration: 100700 Avg. Training loss: 2.8917 0.0169 sec/batch\n",
      "Global Step: 100800 Epoch 43/50 Iteration: 100800 Avg. Training loss: 2.9001 0.0136 sec/batch\n",
      "Global Step: 100900 Epoch 43/50 Iteration: 100900 Avg. Training loss: 2.8770 0.0158 sec/batch\n",
      "Global Step: 101000 Epoch 43/50 Iteration: 101000 Avg. Training loss: 2.9147 0.0154 sec/batch\n",
      "Global Step: 101100 Epoch 43/50 Iteration: 101100 Avg. Training loss: 2.8994 0.0128 sec/batch\n",
      "Global Step: 101200 Epoch 43/50 Iteration: 101200 Avg. Training loss: 2.9157 0.0151 sec/batch\n",
      "Global Step: 101300 Epoch 43/50 Iteration: 101300 Avg. Training loss: 2.8816 0.0136 sec/batch\n",
      "Global Step: 101400 Epoch 43/50 Iteration: 101400 Avg. Training loss: 2.8681 0.0128 sec/batch\n",
      "Global Step: 101500 Epoch 43/50 Iteration: 101500 Avg. Training loss: 2.8983 0.0150 sec/batch\n",
      "Global Step: 101600 Epoch 43/50 Iteration: 101600 Avg. Training loss: 2.8918 0.0141 sec/batch\n",
      "Epoch 44/50 Threshold: 0.07064468846220132 Length of Training words: 2319882\n",
      "Global Step: 101700 Epoch 44/50 Iteration: 101700 Avg. Training loss: 2.8802 0.0135 sec/batch\n",
      "Global Step: 101800 Epoch 44/50 Iteration: 101800 Avg. Training loss: 2.9037 0.0139 sec/batch\n",
      "Global Step: 101900 Epoch 44/50 Iteration: 101900 Avg. Training loss: 2.8799 0.0148 sec/batch\n",
      "Global Step: 102000 Epoch 44/50 Iteration: 102000 Avg. Training loss: 2.8559 0.0163 sec/batch\n",
      "Global Step: 102100 Epoch 44/50 Iteration: 102100 Avg. Training loss: 2.8637 0.0165 sec/batch\n",
      "Global Step: 102200 Epoch 44/50 Iteration: 102200 Avg. Training loss: 2.8651 0.0165 sec/batch\n",
      "Global Step: 102300 Epoch 44/50 Iteration: 102300 Avg. Training loss: 2.8706 0.0147 sec/batch\n",
      "Global Step: 102400 Epoch 44/50 Iteration: 102400 Avg. Training loss: 2.8747 0.0145 sec/batch\n",
      "Global Step: 102500 Epoch 44/50 Iteration: 102500 Avg. Training loss: 2.8868 0.0137 sec/batch\n",
      "Global Step: 102600 Epoch 44/50 Iteration: 102600 Avg. Training loss: 2.8564 0.0145 sec/batch\n",
      "Global Step: 102700 Epoch 44/50 Iteration: 102700 Avg. Training loss: 2.8495 0.0140 sec/batch\n",
      "Global Step: 102800 Epoch 44/50 Iteration: 102800 Avg. Training loss: 2.8858 0.0156 sec/batch\n",
      "Global Step: 102900 Epoch 44/50 Iteration: 102900 Avg. Training loss: 2.8561 0.0139 sec/batch\n",
      "Global Step: 103000 Epoch 44/50 Iteration: 103000 Avg. Training loss: 2.8760 0.0147 sec/batch\n",
      "Global Step: 103100 Epoch 44/50 Iteration: 103100 Avg. Training loss: 2.8720 0.0138 sec/batch\n",
      "Global Step: 103200 Epoch 44/50 Iteration: 103200 Avg. Training loss: 2.8514 0.0139 sec/batch\n",
      "Global Step: 103300 Epoch 44/50 Iteration: 103300 Avg. Training loss: 2.8927 0.0158 sec/batch\n",
      "Global Step: 103400 Epoch 44/50 Iteration: 103400 Avg. Training loss: 2.8709 0.0161 sec/batch\n",
      "Global Step: 103500 Epoch 44/50 Iteration: 103500 Avg. Training loss: 2.8870 0.0152 sec/batch\n",
      "Global Step: 103600 Epoch 44/50 Iteration: 103600 Avg. Training loss: 2.8601 0.0167 sec/batch\n",
      "Global Step: 103700 Epoch 44/50 Iteration: 103700 Avg. Training loss: 2.8425 0.0152 sec/batch\n",
      "Global Step: 103800 Epoch 44/50 Iteration: 103800 Avg. Training loss: 2.8710 0.0137 sec/batch\n",
      "Global Step: 103900 Epoch 44/50 Iteration: 103900 Avg. Training loss: 2.8641 0.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Threshold: 0.07593991723833231 Length of Training words: 2358860\n",
      "Global Step: 104000 Epoch 45/50 Iteration: 104000 Avg. Training loss: 2.8658 0.0114 sec/batch\n",
      "Global Step: 104100 Epoch 45/50 Iteration: 104100 Avg. Training loss: 2.8793 0.0146 sec/batch\n",
      "Global Step: 104200 Epoch 45/50 Iteration: 104200 Avg. Training loss: 2.8787 0.0137 sec/batch\n",
      "Global Step: 104300 Epoch 45/50 Iteration: 104300 Avg. Training loss: 2.8509 0.0148 sec/batch\n",
      "Global Step: 104400 Epoch 45/50 Iteration: 104400 Avg. Training loss: 2.8496 0.0143 sec/batch\n",
      "Global Step: 104500 Epoch 45/50 Iteration: 104500 Avg. Training loss: 2.8496 0.0140 sec/batch\n",
      "Global Step: 104600 Epoch 45/50 Iteration: 104600 Avg. Training loss: 2.8555 0.0157 sec/batch\n",
      "Global Step: 104700 Epoch 45/50 Iteration: 104700 Avg. Training loss: 2.8734 0.0144 sec/batch\n",
      "Global Step: 104800 Epoch 45/50 Iteration: 104800 Avg. Training loss: 2.8605 0.0169 sec/batch\n",
      "Global Step: 104900 Epoch 45/50 Iteration: 104900 Avg. Training loss: 2.8560 0.0176 sec/batch\n",
      "Global Step: 105000 Epoch 45/50 Iteration: 105000 Avg. Training loss: 2.8451 0.0167 sec/batch\n",
      "Global Step: 105100 Epoch 45/50 Iteration: 105100 Avg. Training loss: 2.8596 0.0154 sec/batch\n",
      "Global Step: 105200 Epoch 45/50 Iteration: 105200 Avg. Training loss: 2.8482 0.0153 sec/batch\n",
      "Global Step: 105300 Epoch 45/50 Iteration: 105300 Avg. Training loss: 2.8573 0.0152 sec/batch\n",
      "Global Step: 105400 Epoch 45/50 Iteration: 105400 Avg. Training loss: 2.8552 0.0155 sec/batch\n",
      "Global Step: 105500 Epoch 45/50 Iteration: 105500 Avg. Training loss: 2.8594 0.0152 sec/batch\n",
      "Global Step: 105600 Epoch 45/50 Iteration: 105600 Avg. Training loss: 2.8578 0.0136 sec/batch\n",
      "Global Step: 105700 Epoch 45/50 Iteration: 105700 Avg. Training loss: 2.8719 0.0152 sec/batch\n",
      "Global Step: 105800 Epoch 45/50 Iteration: 105800 Avg. Training loss: 2.8567 0.0153 sec/batch\n",
      "Global Step: 105900 Epoch 45/50 Iteration: 105900 Avg. Training loss: 2.8654 0.0128 sec/batch\n",
      "Global Step: 106000 Epoch 45/50 Iteration: 106000 Avg. Training loss: 2.8337 0.0138 sec/batch\n",
      "Global Step: 106100 Epoch 45/50 Iteration: 106100 Avg. Training loss: 2.8447 0.0169 sec/batch\n",
      "Global Step: 106200 Epoch 45/50 Iteration: 106200 Avg. Training loss: 2.8509 0.0156 sec/batch\n",
      "Epoch 46/50 Threshold: 0.07458249463143053 Length of Training words: 2350270\n",
      "Global Step: 106300 Epoch 46/50 Iteration: 106300 Avg. Training loss: 2.8559 0.0024 sec/batch\n",
      "Global Step: 106400 Epoch 46/50 Iteration: 106400 Avg. Training loss: 2.8692 0.0153 sec/batch\n",
      "Global Step: 106500 Epoch 46/50 Iteration: 106500 Avg. Training loss: 2.8916 0.0146 sec/batch\n",
      "Global Step: 106600 Epoch 46/50 Iteration: 106600 Avg. Training loss: 2.8670 0.0152 sec/batch\n",
      "Global Step: 106700 Epoch 46/50 Iteration: 106700 Avg. Training loss: 2.8505 0.0148 sec/batch\n",
      "Global Step: 106800 Epoch 46/50 Iteration: 106800 Avg. Training loss: 2.8512 0.0154 sec/batch\n",
      "Global Step: 106900 Epoch 46/50 Iteration: 106900 Avg. Training loss: 2.8534 0.0144 sec/batch\n",
      "Global Step: 107000 Epoch 46/50 Iteration: 107000 Avg. Training loss: 2.8648 0.0134 sec/batch\n",
      "Global Step: 107100 Epoch 46/50 Iteration: 107100 Avg. Training loss: 2.8622 0.0183 sec/batch\n",
      "Global Step: 107200 Epoch 46/50 Iteration: 107200 Avg. Training loss: 2.8821 0.0146 sec/batch\n",
      "Global Step: 107300 Epoch 46/50 Iteration: 107300 Avg. Training loss: 2.8442 0.0166 sec/batch\n",
      "Global Step: 107400 Epoch 46/50 Iteration: 107400 Avg. Training loss: 2.8414 0.0156 sec/batch\n",
      "Global Step: 107500 Epoch 46/50 Iteration: 107500 Avg. Training loss: 2.8783 0.0160 sec/batch\n",
      "Global Step: 107600 Epoch 46/50 Iteration: 107600 Avg. Training loss: 2.8440 0.0163 sec/batch\n",
      "Global Step: 107700 Epoch 46/50 Iteration: 107700 Avg. Training loss: 2.8680 0.0140 sec/batch\n",
      "Global Step: 107800 Epoch 46/50 Iteration: 107800 Avg. Training loss: 2.8608 0.0161 sec/batch\n",
      "Global Step: 107900 Epoch 46/50 Iteration: 107900 Avg. Training loss: 2.8416 0.0155 sec/batch\n",
      "Global Step: 108000 Epoch 46/50 Iteration: 108000 Avg. Training loss: 2.8843 0.0150 sec/batch\n",
      "Global Step: 108100 Epoch 46/50 Iteration: 108100 Avg. Training loss: 2.8579 0.0149 sec/batch\n",
      "Global Step: 108200 Epoch 46/50 Iteration: 108200 Avg. Training loss: 2.8778 0.0148 sec/batch\n",
      "Global Step: 108300 Epoch 46/50 Iteration: 108300 Avg. Training loss: 2.8508 0.0152 sec/batch\n",
      "Global Step: 108400 Epoch 46/50 Iteration: 108400 Avg. Training loss: 2.8374 0.0154 sec/batch\n",
      "Global Step: 108500 Epoch 46/50 Iteration: 108500 Avg. Training loss: 2.8620 0.0161 sec/batch\n",
      "Global Step: 108600 Epoch 46/50 Iteration: 108600 Avg. Training loss: 2.8523 0.0139 sec/batch\n",
      "Epoch 47/50 Threshold: 0.07293145990513991 Length of Training words: 2337022\n",
      "Global Step: 108700 Epoch 47/50 Iteration: 108700 Avg. Training loss: 2.8708 0.0092 sec/batch\n",
      "Global Step: 108800 Epoch 47/50 Iteration: 108800 Avg. Training loss: 2.8870 0.0129 sec/batch\n",
      "Global Step: 108900 Epoch 47/50 Iteration: 108900 Avg. Training loss: 2.8835 0.0147 sec/batch\n",
      "Global Step: 109000 Epoch 47/50 Iteration: 109000 Avg. Training loss: 2.8593 0.0136 sec/batch\n",
      "Global Step: 109100 Epoch 47/50 Iteration: 109100 Avg. Training loss: 2.8566 0.0138 sec/batch\n",
      "Global Step: 109200 Epoch 47/50 Iteration: 109200 Avg. Training loss: 2.8550 0.0148 sec/batch\n",
      "Global Step: 109300 Epoch 47/50 Iteration: 109300 Avg. Training loss: 2.8643 0.0150 sec/batch\n",
      "Global Step: 109400 Epoch 47/50 Iteration: 109400 Avg. Training loss: 2.8806 0.0133 sec/batch\n",
      "Global Step: 109500 Epoch 47/50 Iteration: 109500 Avg. Training loss: 2.8665 0.0158 sec/batch\n",
      "Global Step: 109600 Epoch 47/50 Iteration: 109600 Avg. Training loss: 2.8622 0.0177 sec/batch\n",
      "Global Step: 109700 Epoch 47/50 Iteration: 109700 Avg. Training loss: 2.8536 0.0176 sec/batch\n",
      "Global Step: 109800 Epoch 47/50 Iteration: 109800 Avg. Training loss: 2.8700 0.0164 sec/batch\n",
      "Global Step: 109900 Epoch 47/50 Iteration: 109900 Avg. Training loss: 2.8559 0.0151 sec/batch\n",
      "Global Step: 110000 Epoch 47/50 Iteration: 110000 Avg. Training loss: 2.8598 0.0162 sec/batch\n",
      "Global Step: 110100 Epoch 47/50 Iteration: 110100 Avg. Training loss: 2.8601 0.0161 sec/batch\n",
      "Global Step: 110200 Epoch 47/50 Iteration: 110200 Avg. Training loss: 2.8660 0.0161 sec/batch\n",
      "Global Step: 110300 Epoch 47/50 Iteration: 110300 Avg. Training loss: 2.8643 0.0162 sec/batch\n",
      "Global Step: 110400 Epoch 47/50 Iteration: 110400 Avg. Training loss: 2.8839 0.0147 sec/batch\n",
      "Global Step: 110500 Epoch 47/50 Iteration: 110500 Avg. Training loss: 2.8634 0.0154 sec/batch\n",
      "Global Step: 110600 Epoch 47/50 Iteration: 110600 Avg. Training loss: 2.8710 0.0156 sec/batch\n",
      "Global Step: 110700 Epoch 47/50 Iteration: 110700 Avg. Training loss: 2.8442 0.0165 sec/batch\n",
      "Global Step: 110800 Epoch 47/50 Iteration: 110800 Avg. Training loss: 2.8501 0.0166 sec/batch\n",
      "Global Step: 110900 Epoch 47/50 Iteration: 110900 Avg. Training loss: 2.8555 0.0148 sec/batch\n",
      "Epoch 48/50 Threshold: 0.06985650474361509 Length of Training words: 2312860\n",
      "Global Step: 111000 Epoch 48/50 Iteration: 111000 Avg. Training loss: 2.8629 0.0047 sec/batch\n",
      "Global Step: 111100 Epoch 48/50 Iteration: 111100 Avg. Training loss: 2.8838 0.0139 sec/batch\n",
      "Global Step: 111200 Epoch 48/50 Iteration: 111200 Avg. Training loss: 2.9036 0.0141 sec/batch\n",
      "Global Step: 111300 Epoch 48/50 Iteration: 111300 Avg. Training loss: 2.8771 0.0161 sec/batch\n",
      "Global Step: 111400 Epoch 48/50 Iteration: 111400 Avg. Training loss: 2.8649 0.0138 sec/batch\n",
      "Global Step: 111500 Epoch 48/50 Iteration: 111500 Avg. Training loss: 2.8544 0.0125 sec/batch\n",
      "Global Step: 111600 Epoch 48/50 Iteration: 111600 Avg. Training loss: 2.8846 0.0161 sec/batch\n",
      "Global Step: 111700 Epoch 48/50 Iteration: 111700 Avg. Training loss: 2.8774 0.0144 sec/batch\n",
      "Global Step: 111800 Epoch 48/50 Iteration: 111800 Avg. Training loss: 2.8724 0.0117 sec/batch\n",
      "Global Step: 111900 Epoch 48/50 Iteration: 111900 Avg. Training loss: 2.8840 0.0141 sec/batch\n",
      "Global Step: 112000 Epoch 48/50 Iteration: 112000 Avg. Training loss: 2.8562 0.0156 sec/batch\n",
      "Global Step: 112100 Epoch 48/50 Iteration: 112100 Avg. Training loss: 2.8637 0.0178 sec/batch\n",
      "Global Step: 112200 Epoch 48/50 Iteration: 112200 Avg. Training loss: 2.8711 0.0175 sec/batch\n",
      "Global Step: 112300 Epoch 48/50 Iteration: 112300 Avg. Training loss: 2.8781 0.0157 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 112400 Epoch 48/50 Iteration: 112400 Avg. Training loss: 2.8706 0.0165 sec/batch\n",
      "Global Step: 112500 Epoch 48/50 Iteration: 112500 Avg. Training loss: 2.8753 0.0185 sec/batch\n",
      "Global Step: 112600 Epoch 48/50 Iteration: 112600 Avg. Training loss: 2.8575 0.0160 sec/batch\n",
      "Global Step: 112700 Epoch 48/50 Iteration: 112700 Avg. Training loss: 2.8922 0.0139 sec/batch\n",
      "Global Step: 112800 Epoch 48/50 Iteration: 112800 Avg. Training loss: 2.8744 0.0159 sec/batch\n",
      "Global Step: 112900 Epoch 48/50 Iteration: 112900 Avg. Training loss: 2.8827 0.0152 sec/batch\n",
      "Global Step: 113000 Epoch 48/50 Iteration: 113000 Avg. Training loss: 2.8446 0.0156 sec/batch\n",
      "Global Step: 113100 Epoch 48/50 Iteration: 113100 Avg. Training loss: 2.8607 0.0175 sec/batch\n",
      "Global Step: 113200 Epoch 48/50 Iteration: 113200 Avg. Training loss: 2.8686 0.0163 sec/batch\n",
      "Epoch 49/50 Threshold: 0.07556541599744111 Length of Training words: 2356231\n",
      "Global Step: 113300 Epoch 49/50 Iteration: 113300 Avg. Training loss: 2.8701 0.0023 sec/batch\n",
      "Global Step: 113400 Epoch 49/50 Iteration: 113400 Avg. Training loss: 2.8662 0.0152 sec/batch\n",
      "Global Step: 113500 Epoch 49/50 Iteration: 113500 Avg. Training loss: 2.8920 0.0169 sec/batch\n",
      "Global Step: 113600 Epoch 49/50 Iteration: 113600 Avg. Training loss: 2.8648 0.0153 sec/batch\n",
      "Global Step: 113700 Epoch 49/50 Iteration: 113700 Avg. Training loss: 2.8487 0.0147 sec/batch\n",
      "Global Step: 113800 Epoch 49/50 Iteration: 113800 Avg. Training loss: 2.8506 0.0156 sec/batch\n",
      "Global Step: 113900 Epoch 49/50 Iteration: 113900 Avg. Training loss: 2.8528 0.0130 sec/batch\n",
      "Global Step: 114000 Epoch 49/50 Iteration: 114000 Avg. Training loss: 2.8622 0.0158 sec/batch\n",
      "Global Step: 114100 Epoch 49/50 Iteration: 114100 Avg. Training loss: 2.8607 0.0137 sec/batch\n",
      "Global Step: 114200 Epoch 49/50 Iteration: 114200 Avg. Training loss: 2.8839 0.0116 sec/batch\n",
      "Global Step: 114300 Epoch 49/50 Iteration: 114300 Avg. Training loss: 2.8428 0.0129 sec/batch\n",
      "Global Step: 114400 Epoch 49/50 Iteration: 114400 Avg. Training loss: 2.8368 0.0153 sec/batch\n",
      "Global Step: 114500 Epoch 49/50 Iteration: 114500 Avg. Training loss: 2.8732 0.0148 sec/batch\n",
      "Global Step: 114600 Epoch 49/50 Iteration: 114600 Avg. Training loss: 2.8440 0.0137 sec/batch\n",
      "Global Step: 114700 Epoch 49/50 Iteration: 114700 Avg. Training loss: 2.8673 0.0145 sec/batch\n",
      "Global Step: 114800 Epoch 49/50 Iteration: 114800 Avg. Training loss: 2.8568 0.0178 sec/batch\n",
      "Global Step: 114900 Epoch 49/50 Iteration: 114900 Avg. Training loss: 2.8395 0.0161 sec/batch\n",
      "Global Step: 115000 Epoch 49/50 Iteration: 115000 Avg. Training loss: 2.8809 0.0165 sec/batch\n",
      "Global Step: 115100 Epoch 49/50 Iteration: 115100 Avg. Training loss: 2.8554 0.0174 sec/batch\n",
      "Global Step: 115200 Epoch 49/50 Iteration: 115200 Avg. Training loss: 2.8734 0.0160 sec/batch\n",
      "Global Step: 115300 Epoch 49/50 Iteration: 115300 Avg. Training loss: 2.8501 0.0163 sec/batch\n",
      "Global Step: 115400 Epoch 49/50 Iteration: 115400 Avg. Training loss: 2.8378 0.0137 sec/batch\n",
      "Global Step: 115500 Epoch 49/50 Iteration: 115500 Avg. Training loss: 2.8551 0.0174 sec/batch\n",
      "Global Step: 115600 Epoch 49/50 Iteration: 115600 Avg. Training loss: 2.8550 0.0152 sec/batch\n",
      "Epoch 50/50 Threshold: 0.0823238264796941 Length of Training words: 2399910\n",
      "Global Step: 115700 Epoch 50/50 Iteration: 115700 Avg. Training loss: 2.8504 0.0103 sec/batch\n",
      "Global Step: 115800 Epoch 50/50 Iteration: 115800 Avg. Training loss: 2.8617 0.0149 sec/batch\n",
      "Global Step: 115900 Epoch 50/50 Iteration: 115900 Avg. Training loss: 2.8669 0.0154 sec/batch\n",
      "Global Step: 116000 Epoch 50/50 Iteration: 116000 Avg. Training loss: 2.8363 0.0146 sec/batch\n",
      "Global Step: 116100 Epoch 50/50 Iteration: 116100 Avg. Training loss: 2.8398 0.0156 sec/batch\n",
      "Global Step: 116200 Epoch 50/50 Iteration: 116200 Avg. Training loss: 2.8349 0.0153 sec/batch\n",
      "Global Step: 116300 Epoch 50/50 Iteration: 116300 Avg. Training loss: 2.8455 0.0153 sec/batch\n",
      "Global Step: 116400 Epoch 50/50 Iteration: 116400 Avg. Training loss: 2.8501 0.0135 sec/batch\n",
      "Global Step: 116500 Epoch 50/50 Iteration: 116500 Avg. Training loss: 2.8486 0.0170 sec/batch\n",
      "Global Step: 116600 Epoch 50/50 Iteration: 116600 Avg. Training loss: 2.8564 0.0153 sec/batch\n",
      "Global Step: 116700 Epoch 50/50 Iteration: 116700 Avg. Training loss: 2.8292 0.0132 sec/batch\n",
      "Global Step: 116800 Epoch 50/50 Iteration: 116800 Avg. Training loss: 2.8331 0.0137 sec/batch\n",
      "Global Step: 116900 Epoch 50/50 Iteration: 116900 Avg. Training loss: 2.8493 0.0138 sec/batch\n",
      "Global Step: 117000 Epoch 50/50 Iteration: 117000 Avg. Training loss: 2.8371 0.0149 sec/batch\n",
      "Global Step: 117100 Epoch 50/50 Iteration: 117100 Avg. Training loss: 2.8497 0.0154 sec/batch\n",
      "Global Step: 117200 Epoch 50/50 Iteration: 117200 Avg. Training loss: 2.8351 0.0131 sec/batch\n",
      "Global Step: 117300 Epoch 50/50 Iteration: 117300 Avg. Training loss: 2.8299 0.0124 sec/batch\n",
      "Global Step: 117400 Epoch 50/50 Iteration: 117400 Avg. Training loss: 2.8669 0.0193 sec/batch\n",
      "Global Step: 117500 Epoch 50/50 Iteration: 117500 Avg. Training loss: 2.8450 0.0135 sec/batch\n",
      "Global Step: 117600 Epoch 50/50 Iteration: 117600 Avg. Training loss: 2.8595 0.0148 sec/batch\n",
      "Global Step: 117700 Epoch 50/50 Iteration: 117700 Avg. Training loss: 2.8310 0.0169 sec/batch\n",
      "Global Step: 117800 Epoch 50/50 Iteration: 117800 Avg. Training loss: 2.8190 0.0164 sec/batch\n",
      "Global Step: 117900 Epoch 50/50 Iteration: 117900 Avg. Training loss: 2.8454 0.0143 sec/batch\n",
      "Global Step: 118000 Epoch 50/50 Iteration: 118000 Avg. Training loss: 2.8372 0.0135 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/pos/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos5'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB14AAAcMCAYAAAAHCCGfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6a8855c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 902,
       "width": 943
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
