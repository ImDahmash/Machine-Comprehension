{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 391092),\n",
       " ('IN', 313428),\n",
       " ('NNP', 286327),\n",
       " ('DT', 246818),\n",
       " ('JJ', 194522),\n",
       " ('NNS', 150057),\n",
       " (',', 138811),\n",
       " ('.', 92716),\n",
       " ('CC', 86433),\n",
       " ('VBD', 83807),\n",
       " ('CD', 77651),\n",
       " ('RB', 71899),\n",
       " ('VBN', 69596),\n",
       " ('TO', 48800),\n",
       " ('VBZ', 41862),\n",
       " ('VB', 40118),\n",
       " ('VBG', 35283),\n",
       " (':', 34343),\n",
       " ('VBP', 28324),\n",
       " ('PRP', 24380),\n",
       " ('FW', 22213),\n",
       " ('PRP$', 18771),\n",
       " ('POS', 14923),\n",
       " ('WDT', 13756),\n",
       " (\"''\", 13344),\n",
       " ('``', 12561),\n",
       " ('MD', 11441),\n",
       " ('NNPS', 10637),\n",
       " ('JJS', 6400),\n",
       " ('JJR', 6373),\n",
       " ('WRB', 5119),\n",
       " ('WP', 4378),\n",
       " ('RP', 3189),\n",
       " ('RBR', 2910),\n",
       " ('OTHER', 2645),\n",
       " ('EX', 2623),\n",
       " ('RBS', 2274)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pos_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if(text[i]=='$'or text[i]=='PDT' or text[i]=='WP$' or text[i]==\"SYM\" or text[i]=='LS' or text[i]=='#' or text[i]=='UH'):\n",
    "        text[i]='OTHER'\n",
    "    \n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609824\n",
      "Unique words: 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 214218),\n",
       " (1, 191830),\n",
       " (2, 183019),\n",
       " (3, 170562),\n",
       " (4, 151288),\n",
       " (5, 132821),\n",
       " (6, 127626),\n",
       " (7, 92716),\n",
       " (8, 86433),\n",
       " (9, 83807),\n",
       " (10, 77651),\n",
       " (11, 71899),\n",
       " (12, 69596),\n",
       " (13, 48800),\n",
       " (14, 41862),\n",
       " (15, 40118),\n",
       " (16, 35283),\n",
       " (17, 34343),\n",
       " (18, 28324),\n",
       " (19, 24380),\n",
       " (20, 22213),\n",
       " (21, 18771),\n",
       " (22, 14923),\n",
       " (23, 13756),\n",
       " (24, 13344),\n",
       " (25, 12561),\n",
       " (26, 11441),\n",
       " (27, 10637),\n",
       " (28, 6400),\n",
       " (29, 6373),\n",
       " (30, 5119),\n",
       " (31, 4378),\n",
       " (32, 3189),\n",
       " (33, 2910),\n",
       " (34, 2645),\n",
       " (35, 2623),\n",
       " (36, 2274)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "## Your code here\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "threshold = 0.045\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "word_counts = Counter(train_words)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    #     labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#         valid_examples = np.append(valid_examples, \n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜checkpoints/posâ€™: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with train_graph.as_default():\n",
    "#     saver = tf.train.Saver()\n",
    "    \n",
    "# with tf.Session(graph=train_graph) as sess:\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "#     embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 6.5519 0.0228 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 4.8493 0.0128 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 3.9669 0.0131 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 3.5058 0.0153 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 3.2970 0.0146 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 3.1752 0.0150 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 3.1270 0.0136 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 3.0838 0.0124 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 3.0234 0.0139 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 3.0056 0.0129 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 2.9958 0.0134 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 2.9992 0.0148 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 2.9831 0.0135 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 2.9656 0.0134 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 2.9946 0.0129 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 2.9778 0.0103 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 2.9822 0.0060 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 2.9454 0.0070 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 2.9490 0.0066 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 2.9526 0.0076 sec/batch\n",
      "Global Step: 2100 Epoch 2/50 Iteration: 2100 Avg. Training loss: 2.9665 0.0017 sec/batch\n",
      "Global Step: 2200 Epoch 2/50 Iteration: 2200 Avg. Training loss: 2.9780 0.0058 sec/batch\n",
      "Global Step: 2300 Epoch 2/50 Iteration: 2300 Avg. Training loss: 2.9784 0.0062 sec/batch\n",
      "Global Step: 2400 Epoch 2/50 Iteration: 2400 Avg. Training loss: 2.9488 0.0069 sec/batch\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 2.9526 0.0051 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 2.9502 0.0110 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 2.9622 0.0131 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 2.9571 0.0131 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 2.9581 0.0128 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 2.9455 0.0128 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 2.9533 0.0139 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 2.9480 0.0148 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 2.9611 0.0128 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 2.9531 0.0130 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 2.9460 0.0136 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 2.9750 0.0123 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 2.9574 0.0131 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 2.9604 0.0147 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 2.9307 0.0128 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 2.9462 0.0120 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 2.9462 0.0125 sec/batch\n",
      "Global Step: 4200 Epoch 3/50 Iteration: 4200 Avg. Training loss: 2.9509 0.0095 sec/batch\n",
      "Global Step: 4300 Epoch 3/50 Iteration: 4300 Avg. Training loss: 2.9871 0.0122 sec/batch\n",
      "Global Step: 4400 Epoch 3/50 Iteration: 4400 Avg. Training loss: 2.9570 0.0130 sec/batch\n",
      "Global Step: 4500 Epoch 3/50 Iteration: 4500 Avg. Training loss: 2.9454 0.0127 sec/batch\n",
      "Global Step: 4600 Epoch 3/50 Iteration: 4600 Avg. Training loss: 2.9384 0.0123 sec/batch\n",
      "Global Step: 4700 Epoch 3/50 Iteration: 4700 Avg. Training loss: 2.9526 0.0123 sec/batch\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 2.9674 0.0137 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 2.9547 0.0115 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 2.9409 0.0117 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 2.9373 0.0129 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 2.9639 0.0132 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 2.9499 0.0131 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 2.9452 0.0132 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 2.9526 0.0136 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 2.9548 0.0127 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 2.9551 0.0119 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 2.9712 0.0133 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 2.9401 0.0120 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 2.9318 0.0136 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 2.9447 0.0134 sec/batch\n",
      "Global Step: 6200 Epoch 4/50 Iteration: 6200 Avg. Training loss: 2.9540 0.0030 sec/batch\n",
      "Global Step: 6300 Epoch 4/50 Iteration: 6300 Avg. Training loss: 2.9634 0.0122 sec/batch\n",
      "Global Step: 6400 Epoch 4/50 Iteration: 6400 Avg. Training loss: 2.9732 0.0130 sec/batch\n",
      "Global Step: 6500 Epoch 4/50 Iteration: 6500 Avg. Training loss: 2.9516 0.0122 sec/batch\n",
      "Global Step: 6600 Epoch 4/50 Iteration: 6600 Avg. Training loss: 2.9480 0.0118 sec/batch\n",
      "Global Step: 6700 Epoch 4/50 Iteration: 6700 Avg. Training loss: 2.9398 0.0119 sec/batch\n",
      "Global Step: 6800 Epoch 4/50 Iteration: 6800 Avg. Training loss: 2.9480 0.0126 sec/batch\n",
      "Global Step: 6900 Epoch 4/50 Iteration: 6900 Avg. Training loss: 2.9542 0.0135 sec/batch\n",
      "Global Step: 7000 Epoch 4/50 Iteration: 7000 Avg. Training loss: 2.9669 0.0121 sec/batch\n",
      "Global Step: 7100 Epoch 4/50 Iteration: 7100 Avg. Training loss: 2.9307 0.0144 sec/batch\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 2.9456 0.0137 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 2.9545 0.0114 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 2.9457 0.0127 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 2.9583 0.0140 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 2.9382 0.0138 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 2.9692 0.0131 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 2.9568 0.0142 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 2.9641 0.0139 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 2.9292 0.0139 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 2.9396 0.0130 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 2.9491 0.0154 sec/batch\n",
      "Global Step: 8300 Epoch 5/50 Iteration: 8300 Avg. Training loss: 2.9538 0.0090 sec/batch\n",
      "Global Step: 8400 Epoch 5/50 Iteration: 8400 Avg. Training loss: 2.9707 0.0148 sec/batch\n",
      "Global Step: 8500 Epoch 5/50 Iteration: 8500 Avg. Training loss: 2.9652 0.0157 sec/batch\n",
      "Global Step: 8600 Epoch 5/50 Iteration: 8600 Avg. Training loss: 2.9371 0.0131 sec/batch\n",
      "Global Step: 8700 Epoch 5/50 Iteration: 8700 Avg. Training loss: 2.9445 0.0153 sec/batch\n",
      "Global Step: 8800 Epoch 5/50 Iteration: 8800 Avg. Training loss: 2.9528 0.0098 sec/batch\n",
      "Global Step: 8900 Epoch 5/50 Iteration: 8900 Avg. Training loss: 2.9503 0.0124 sec/batch\n",
      "Global Step: 9000 Epoch 5/50 Iteration: 9000 Avg. Training loss: 2.9565 0.0128 sec/batch\n",
      "Global Step: 9100 Epoch 5/50 Iteration: 9100 Avg. Training loss: 2.9435 0.0135 sec/batch\n",
      "Global Step: 9200 Epoch 5/50 Iteration: 9200 Avg. Training loss: 2.9385 0.0129 sec/batch\n",
      "Global Step: 9300 Epoch 5/50 Iteration: 9300 Avg. Training loss: 2.9578 0.0131 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9400 Epoch 5/50 Iteration: 9400 Avg. Training loss: 2.9422 0.0139 sec/batch\n",
      "Global Step: 9500 Epoch 5/50 Iteration: 9500 Avg. Training loss: 2.9602 0.0126 sec/batch\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 2.9502 0.0126 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 2.9484 0.0136 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 2.9624 0.0141 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 2.9592 0.0129 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 2.9439 0.0140 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 2.9280 0.0145 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 2.9503 0.0132 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 2.9406 0.0136 sec/batch\n",
      "Global Step: 10400 Epoch 6/50 Iteration: 10400 Avg. Training loss: 2.9635 0.0152 sec/batch\n",
      "Global Step: 10500 Epoch 6/50 Iteration: 10500 Avg. Training loss: 2.9748 0.0153 sec/batch\n",
      "Global Step: 10600 Epoch 6/50 Iteration: 10600 Avg. Training loss: 2.9550 0.0142 sec/batch\n",
      "Global Step: 10700 Epoch 6/50 Iteration: 10700 Avg. Training loss: 2.9432 0.0156 sec/batch\n",
      "Global Step: 10800 Epoch 6/50 Iteration: 10800 Avg. Training loss: 2.9433 0.0145 sec/batch\n",
      "Global Step: 10900 Epoch 6/50 Iteration: 10900 Avg. Training loss: 2.9438 0.0147 sec/batch\n",
      "Global Step: 11000 Epoch 6/50 Iteration: 11000 Avg. Training loss: 2.9608 0.0137 sec/batch\n",
      "Global Step: 11100 Epoch 6/50 Iteration: 11100 Avg. Training loss: 2.9591 0.0133 sec/batch\n",
      "Global Step: 11200 Epoch 6/50 Iteration: 11200 Avg. Training loss: 2.9376 0.0137 sec/batch\n",
      "Global Step: 11300 Epoch 6/50 Iteration: 11300 Avg. Training loss: 2.9429 0.0129 sec/batch\n",
      "Global Step: 11400 Epoch 6/50 Iteration: 11400 Avg. Training loss: 2.9438 0.0149 sec/batch\n",
      "Global Step: 11500 Epoch 6/50 Iteration: 11500 Avg. Training loss: 2.9551 0.0137 sec/batch\n",
      "Global Step: 11600 Epoch 6/50 Iteration: 11600 Avg. Training loss: 2.9490 0.0131 sec/batch\n",
      "Global Step: 11700 Epoch 6/50 Iteration: 11700 Avg. Training loss: 2.9397 0.0141 sec/batch\n",
      "Global Step: 11800 Epoch 6/50 Iteration: 11800 Avg. Training loss: 2.9720 0.0137 sec/batch\n",
      "Global Step: 11900 Epoch 6/50 Iteration: 11900 Avg. Training loss: 2.9535 0.0130 sec/batch\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 2.9665 0.0143 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 2.9314 0.0138 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 2.9351 0.0154 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 2.9419 0.0151 sec/batch\n",
      "Global Step: 12400 Epoch 7/50 Iteration: 12400 Avg. Training loss: 2.9576 0.0055 sec/batch\n",
      "Global Step: 12500 Epoch 7/50 Iteration: 12500 Avg. Training loss: 2.9688 0.0131 sec/batch\n",
      "Global Step: 12600 Epoch 7/50 Iteration: 12600 Avg. Training loss: 2.9704 0.0145 sec/batch\n",
      "Global Step: 12700 Epoch 7/50 Iteration: 12700 Avg. Training loss: 2.9412 0.0143 sec/batch\n",
      "Global Step: 12800 Epoch 7/50 Iteration: 12800 Avg. Training loss: 2.9436 0.0135 sec/batch\n",
      "Global Step: 12900 Epoch 7/50 Iteration: 12900 Avg. Training loss: 2.9445 0.0140 sec/batch\n",
      "Global Step: 13000 Epoch 7/50 Iteration: 13000 Avg. Training loss: 2.9547 0.0132 sec/batch\n",
      "Global Step: 13100 Epoch 7/50 Iteration: 13100 Avg. Training loss: 2.9529 0.0130 sec/batch\n",
      "Global Step: 13200 Epoch 7/50 Iteration: 13200 Avg. Training loss: 2.9509 0.0135 sec/batch\n",
      "Global Step: 13300 Epoch 7/50 Iteration: 13300 Avg. Training loss: 2.9401 0.0131 sec/batch\n",
      "Global Step: 13400 Epoch 7/50 Iteration: 13400 Avg. Training loss: 2.9504 0.0102 sec/batch\n",
      "Global Step: 13500 Epoch 7/50 Iteration: 13500 Avg. Training loss: 2.9443 0.0129 sec/batch\n",
      "Global Step: 13600 Epoch 7/50 Iteration: 13600 Avg. Training loss: 2.9566 0.0147 sec/batch\n",
      "Global Step: 13700 Epoch 7/50 Iteration: 13700 Avg. Training loss: 2.9515 0.0130 sec/batch\n",
      "Global Step: 13800 Epoch 7/50 Iteration: 13800 Avg. Training loss: 2.9408 0.0133 sec/batch\n",
      "Global Step: 13900 Epoch 7/50 Iteration: 13900 Avg. Training loss: 2.9714 0.0151 sec/batch\n",
      "Global Step: 14000 Epoch 7/50 Iteration: 14000 Avg. Training loss: 2.9534 0.0136 sec/batch\n",
      "Global Step: 14100 Epoch 7/50 Iteration: 14100 Avg. Training loss: 2.9573 0.0137 sec/batch\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 2.9297 0.0132 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 2.9434 0.0140 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 2.9431 0.0138 sec/batch\n",
      "Global Step: 14500 Epoch 8/50 Iteration: 14500 Avg. Training loss: 2.9490 0.0115 sec/batch\n",
      "Global Step: 14600 Epoch 8/50 Iteration: 14600 Avg. Training loss: 2.9854 0.0139 sec/batch\n",
      "Global Step: 14700 Epoch 8/50 Iteration: 14700 Avg. Training loss: 2.9552 0.0141 sec/batch\n",
      "Global Step: 14800 Epoch 8/50 Iteration: 14800 Avg. Training loss: 2.9425 0.0140 sec/batch\n",
      "Global Step: 14900 Epoch 8/50 Iteration: 14900 Avg. Training loss: 2.9366 0.0140 sec/batch\n",
      "Global Step: 15000 Epoch 8/50 Iteration: 15000 Avg. Training loss: 2.9488 0.0139 sec/batch\n",
      "Global Step: 15100 Epoch 8/50 Iteration: 15100 Avg. Training loss: 2.9647 0.0140 sec/batch\n",
      "Global Step: 15200 Epoch 8/50 Iteration: 15200 Avg. Training loss: 2.9527 0.0129 sec/batch\n",
      "Global Step: 15300 Epoch 8/50 Iteration: 15300 Avg. Training loss: 2.9385 0.0133 sec/batch\n",
      "Global Step: 15400 Epoch 8/50 Iteration: 15400 Avg. Training loss: 2.9361 0.0142 sec/batch\n",
      "Global Step: 15500 Epoch 8/50 Iteration: 15500 Avg. Training loss: 2.9625 0.0129 sec/batch\n",
      "Global Step: 15600 Epoch 8/50 Iteration: 15600 Avg. Training loss: 2.9479 0.0124 sec/batch\n",
      "Global Step: 15700 Epoch 8/50 Iteration: 15700 Avg. Training loss: 2.9438 0.0132 sec/batch\n",
      "Global Step: 15800 Epoch 8/50 Iteration: 15800 Avg. Training loss: 2.9526 0.0128 sec/batch\n",
      "Global Step: 15900 Epoch 8/50 Iteration: 15900 Avg. Training loss: 2.9523 0.0119 sec/batch\n",
      "Global Step: 16000 Epoch 8/50 Iteration: 16000 Avg. Training loss: 2.9525 0.0122 sec/batch\n",
      "Global Step: 16100 Epoch 8/50 Iteration: 16100 Avg. Training loss: 2.9687 0.0134 sec/batch\n",
      "Global Step: 16200 Epoch 8/50 Iteration: 16200 Avg. Training loss: 2.9356 0.0138 sec/batch\n",
      "Global Step: 16300 Epoch 8/50 Iteration: 16300 Avg. Training loss: 2.9293 0.0128 sec/batch\n",
      "Global Step: 16400 Epoch 8/50 Iteration: 16400 Avg. Training loss: 2.9438 0.0129 sec/batch\n",
      "Global Step: 16500 Epoch 9/50 Iteration: 16500 Avg. Training loss: 2.9530 0.0029 sec/batch\n",
      "Global Step: 16600 Epoch 9/50 Iteration: 16600 Avg. Training loss: 2.9622 0.0121 sec/batch\n",
      "Global Step: 16700 Epoch 9/50 Iteration: 16700 Avg. Training loss: 2.9718 0.0132 sec/batch\n",
      "Global Step: 16800 Epoch 9/50 Iteration: 16800 Avg. Training loss: 2.9526 0.0135 sec/batch\n",
      "Global Step: 16900 Epoch 9/50 Iteration: 16900 Avg. Training loss: 2.9471 0.0126 sec/batch\n",
      "Global Step: 17000 Epoch 9/50 Iteration: 17000 Avg. Training loss: 2.9391 0.0137 sec/batch\n",
      "Global Step: 17100 Epoch 9/50 Iteration: 17100 Avg. Training loss: 2.9468 0.0151 sec/batch\n",
      "Global Step: 17200 Epoch 9/50 Iteration: 17200 Avg. Training loss: 2.9528 0.0155 sec/batch\n",
      "Global Step: 17300 Epoch 9/50 Iteration: 17300 Avg. Training loss: 2.9644 0.0127 sec/batch\n",
      "Global Step: 17400 Epoch 9/50 Iteration: 17400 Avg. Training loss: 2.9306 0.0143 sec/batch\n",
      "Global Step: 17500 Epoch 9/50 Iteration: 17500 Avg. Training loss: 2.9453 0.0151 sec/batch\n",
      "Global Step: 17600 Epoch 9/50 Iteration: 17600 Avg. Training loss: 2.9527 0.0145 sec/batch\n",
      "Global Step: 17700 Epoch 9/50 Iteration: 17700 Avg. Training loss: 2.9448 0.0156 sec/batch\n",
      "Global Step: 17800 Epoch 9/50 Iteration: 17800 Avg. Training loss: 2.9560 0.0135 sec/batch\n",
      "Global Step: 17900 Epoch 9/50 Iteration: 17900 Avg. Training loss: 2.9367 0.0132 sec/batch\n",
      "Global Step: 18000 Epoch 9/50 Iteration: 18000 Avg. Training loss: 2.9682 0.0143 sec/batch\n",
      "Global Step: 18100 Epoch 9/50 Iteration: 18100 Avg. Training loss: 2.9562 0.0065 sec/batch\n",
      "Global Step: 18200 Epoch 9/50 Iteration: 18200 Avg. Training loss: 2.9637 0.0053 sec/batch\n",
      "Global Step: 18300 Epoch 9/50 Iteration: 18300 Avg. Training loss: 2.9277 0.0061 sec/batch\n",
      "Global Step: 18400 Epoch 9/50 Iteration: 18400 Avg. Training loss: 2.9374 0.0074 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 18500 Epoch 9/50 Iteration: 18500 Avg. Training loss: 2.9491 0.0073 sec/batch\n",
      "Global Step: 18600 Epoch 10/50 Iteration: 18600 Avg. Training loss: 2.9538 0.0029 sec/batch\n",
      "Global Step: 18700 Epoch 10/50 Iteration: 18700 Avg. Training loss: 2.9703 0.0051 sec/batch\n",
      "Global Step: 18800 Epoch 10/50 Iteration: 18800 Avg. Training loss: 2.9642 0.0055 sec/batch\n",
      "Global Step: 18900 Epoch 10/50 Iteration: 18900 Avg. Training loss: 2.9364 0.0063 sec/batch\n",
      "Global Step: 19000 Epoch 10/50 Iteration: 19000 Avg. Training loss: 2.9453 0.0059 sec/batch\n",
      "Global Step: 19100 Epoch 10/50 Iteration: 19100 Avg. Training loss: 2.9525 0.0047 sec/batch\n",
      "Global Step: 19200 Epoch 10/50 Iteration: 19200 Avg. Training loss: 2.9486 0.0050 sec/batch\n",
      "Global Step: 19300 Epoch 10/50 Iteration: 19300 Avg. Training loss: 2.9553 0.0054 sec/batch\n",
      "Global Step: 19400 Epoch 10/50 Iteration: 19400 Avg. Training loss: 2.9456 0.0122 sec/batch\n",
      "Global Step: 19500 Epoch 10/50 Iteration: 19500 Avg. Training loss: 2.9379 0.0146 sec/batch\n",
      "Global Step: 19600 Epoch 10/50 Iteration: 19600 Avg. Training loss: 2.9570 0.0138 sec/batch\n",
      "Global Step: 19700 Epoch 10/50 Iteration: 19700 Avg. Training loss: 2.9426 0.0127 sec/batch\n",
      "Global Step: 19800 Epoch 10/50 Iteration: 19800 Avg. Training loss: 2.9584 0.0131 sec/batch\n",
      "Global Step: 19900 Epoch 10/50 Iteration: 19900 Avg. Training loss: 2.9495 0.0146 sec/batch\n",
      "Global Step: 20000 Epoch 10/50 Iteration: 20000 Avg. Training loss: 2.9488 0.0133 sec/batch\n",
      "Global Step: 20100 Epoch 10/50 Iteration: 20100 Avg. Training loss: 2.9620 0.0123 sec/batch\n",
      "Global Step: 20200 Epoch 10/50 Iteration: 20200 Avg. Training loss: 2.9576 0.0130 sec/batch\n",
      "Global Step: 20300 Epoch 10/50 Iteration: 20300 Avg. Training loss: 2.9415 0.0145 sec/batch\n",
      "Global Step: 20400 Epoch 10/50 Iteration: 20400 Avg. Training loss: 2.9264 0.0148 sec/batch\n",
      "Global Step: 20500 Epoch 10/50 Iteration: 20500 Avg. Training loss: 2.9523 0.0153 sec/batch\n",
      "Global Step: 20600 Epoch 10/50 Iteration: 20600 Avg. Training loss: 2.9394 0.0141 sec/batch\n",
      "Global Step: 20700 Epoch 11/50 Iteration: 20700 Avg. Training loss: 2.9636 0.0145 sec/batch\n",
      "Global Step: 20800 Epoch 11/50 Iteration: 20800 Avg. Training loss: 2.9722 0.0148 sec/batch\n",
      "Global Step: 20900 Epoch 11/50 Iteration: 20900 Avg. Training loss: 2.9547 0.0135 sec/batch\n",
      "Global Step: 21000 Epoch 11/50 Iteration: 21000 Avg. Training loss: 2.9444 0.0122 sec/batch\n",
      "Global Step: 21100 Epoch 11/50 Iteration: 21100 Avg. Training loss: 2.9418 0.0130 sec/batch\n",
      "Global Step: 21200 Epoch 11/50 Iteration: 21200 Avg. Training loss: 2.9441 0.0147 sec/batch\n",
      "Global Step: 21300 Epoch 11/50 Iteration: 21300 Avg. Training loss: 2.9611 0.0134 sec/batch\n",
      "Global Step: 21400 Epoch 11/50 Iteration: 21400 Avg. Training loss: 2.9594 0.0120 sec/batch\n",
      "Global Step: 21500 Epoch 11/50 Iteration: 21500 Avg. Training loss: 2.9362 0.0145 sec/batch\n",
      "Global Step: 21600 Epoch 11/50 Iteration: 21600 Avg. Training loss: 2.9431 0.0125 sec/batch\n",
      "Global Step: 21700 Epoch 11/50 Iteration: 21700 Avg. Training loss: 2.9437 0.0127 sec/batch\n",
      "Global Step: 21800 Epoch 11/50 Iteration: 21800 Avg. Training loss: 2.9553 0.0125 sec/batch\n",
      "Global Step: 21900 Epoch 11/50 Iteration: 21900 Avg. Training loss: 2.9495 0.0139 sec/batch\n",
      "Global Step: 22000 Epoch 11/50 Iteration: 22000 Avg. Training loss: 2.9388 0.0125 sec/batch\n",
      "Global Step: 22100 Epoch 11/50 Iteration: 22100 Avg. Training loss: 2.9700 0.0135 sec/batch\n",
      "Global Step: 22200 Epoch 11/50 Iteration: 22200 Avg. Training loss: 2.9540 0.0137 sec/batch\n",
      "Global Step: 22300 Epoch 11/50 Iteration: 22300 Avg. Training loss: 2.9645 0.0140 sec/batch\n",
      "Global Step: 22400 Epoch 11/50 Iteration: 22400 Avg. Training loss: 2.9324 0.0115 sec/batch\n",
      "Global Step: 22500 Epoch 11/50 Iteration: 22500 Avg. Training loss: 2.9345 0.0133 sec/batch\n",
      "Global Step: 22600 Epoch 11/50 Iteration: 22600 Avg. Training loss: 2.9423 0.0147 sec/batch\n",
      "Global Step: 22700 Epoch 12/50 Iteration: 22700 Avg. Training loss: 2.9553 0.0067 sec/batch\n",
      "Global Step: 22800 Epoch 12/50 Iteration: 22800 Avg. Training loss: 2.9678 0.0143 sec/batch\n",
      "Global Step: 22900 Epoch 12/50 Iteration: 22900 Avg. Training loss: 2.9709 0.0140 sec/batch\n",
      "Global Step: 23000 Epoch 12/50 Iteration: 23000 Avg. Training loss: 2.9415 0.0157 sec/batch\n",
      "Global Step: 23100 Epoch 12/50 Iteration: 23100 Avg. Training loss: 2.9429 0.0147 sec/batch\n",
      "Global Step: 23200 Epoch 12/50 Iteration: 23200 Avg. Training loss: 2.9429 0.0130 sec/batch\n",
      "Global Step: 23300 Epoch 12/50 Iteration: 23300 Avg. Training loss: 2.9557 0.0145 sec/batch\n",
      "Global Step: 23400 Epoch 12/50 Iteration: 23400 Avg. Training loss: 2.9524 0.0138 sec/batch\n",
      "Global Step: 23500 Epoch 12/50 Iteration: 23500 Avg. Training loss: 2.9503 0.0139 sec/batch\n",
      "Global Step: 23600 Epoch 12/50 Iteration: 23600 Avg. Training loss: 2.9399 0.0155 sec/batch\n",
      "Global Step: 23700 Epoch 12/50 Iteration: 23700 Avg. Training loss: 2.9491 0.0140 sec/batch\n",
      "Global Step: 23800 Epoch 12/50 Iteration: 23800 Avg. Training loss: 2.9444 0.0130 sec/batch\n",
      "Global Step: 23900 Epoch 12/50 Iteration: 23900 Avg. Training loss: 2.9570 0.0122 sec/batch\n",
      "Global Step: 24000 Epoch 12/50 Iteration: 24000 Avg. Training loss: 2.9507 0.0141 sec/batch\n",
      "Global Step: 24100 Epoch 12/50 Iteration: 24100 Avg. Training loss: 2.9421 0.0142 sec/batch\n",
      "Global Step: 24200 Epoch 12/50 Iteration: 24200 Avg. Training loss: 2.9701 0.0125 sec/batch\n",
      "Global Step: 24300 Epoch 12/50 Iteration: 24300 Avg. Training loss: 2.9533 0.0154 sec/batch\n",
      "Global Step: 24400 Epoch 12/50 Iteration: 24400 Avg. Training loss: 2.9545 0.0146 sec/batch\n",
      "Global Step: 24500 Epoch 12/50 Iteration: 24500 Avg. Training loss: 2.9270 0.0155 sec/batch\n",
      "Global Step: 24600 Epoch 12/50 Iteration: 24600 Avg. Training loss: 2.9414 0.0154 sec/batch\n",
      "Global Step: 24700 Epoch 12/50 Iteration: 24700 Avg. Training loss: 2.9417 0.0132 sec/batch\n",
      "Global Step: 24800 Epoch 13/50 Iteration: 24800 Avg. Training loss: 2.9486 0.0110 sec/batch\n",
      "Global Step: 24900 Epoch 13/50 Iteration: 24900 Avg. Training loss: 2.9844 0.0153 sec/batch\n",
      "Global Step: 25000 Epoch 13/50 Iteration: 25000 Avg. Training loss: 2.9541 0.0158 sec/batch\n",
      "Global Step: 25100 Epoch 13/50 Iteration: 25100 Avg. Training loss: 2.9410 0.0136 sec/batch\n",
      "Global Step: 25200 Epoch 13/50 Iteration: 25200 Avg. Training loss: 2.9358 0.0155 sec/batch\n",
      "Global Step: 25300 Epoch 13/50 Iteration: 25300 Avg. Training loss: 2.9496 0.0142 sec/batch\n",
      "Global Step: 25400 Epoch 13/50 Iteration: 25400 Avg. Training loss: 2.9638 0.0132 sec/batch\n",
      "Global Step: 25500 Epoch 13/50 Iteration: 25500 Avg. Training loss: 2.9524 0.0139 sec/batch\n",
      "Global Step: 25600 Epoch 13/50 Iteration: 25600 Avg. Training loss: 2.9372 0.0135 sec/batch\n",
      "Global Step: 25700 Epoch 13/50 Iteration: 25700 Avg. Training loss: 2.9351 0.0128 sec/batch\n",
      "Global Step: 25800 Epoch 13/50 Iteration: 25800 Avg. Training loss: 2.9612 0.0147 sec/batch\n",
      "Global Step: 25900 Epoch 13/50 Iteration: 25900 Avg. Training loss: 2.9467 0.0140 sec/batch\n",
      "Global Step: 26000 Epoch 13/50 Iteration: 26000 Avg. Training loss: 2.9436 0.0128 sec/batch\n",
      "Global Step: 26100 Epoch 13/50 Iteration: 26100 Avg. Training loss: 2.9535 0.0146 sec/batch\n",
      "Global Step: 26200 Epoch 13/50 Iteration: 26200 Avg. Training loss: 2.9532 0.0141 sec/batch\n",
      "Global Step: 26300 Epoch 13/50 Iteration: 26300 Avg. Training loss: 2.9530 0.0144 sec/batch\n",
      "Global Step: 26400 Epoch 13/50 Iteration: 26400 Avg. Training loss: 2.9698 0.0146 sec/batch\n",
      "Global Step: 26500 Epoch 13/50 Iteration: 26500 Avg. Training loss: 2.9364 0.0155 sec/batch\n",
      "Global Step: 26600 Epoch 13/50 Iteration: 26600 Avg. Training loss: 2.9305 0.0152 sec/batch\n",
      "Global Step: 26700 Epoch 13/50 Iteration: 26700 Avg. Training loss: 2.9443 0.0142 sec/batch\n",
      "Global Step: 26800 Epoch 14/50 Iteration: 26800 Avg. Training loss: 2.9513 0.0033 sec/batch\n",
      "Global Step: 26900 Epoch 14/50 Iteration: 26900 Avg. Training loss: 2.9614 0.0126 sec/batch\n",
      "Global Step: 27000 Epoch 14/50 Iteration: 27000 Avg. Training loss: 2.9714 0.0084 sec/batch\n",
      "Global Step: 27100 Epoch 14/50 Iteration: 27100 Avg. Training loss: 2.9504 0.0085 sec/batch\n",
      "Global Step: 27200 Epoch 14/50 Iteration: 27200 Avg. Training loss: 2.9466 0.0072 sec/batch\n",
      "Global Step: 27300 Epoch 14/50 Iteration: 27300 Avg. Training loss: 2.9399 0.0075 sec/batch\n",
      "Global Step: 27400 Epoch 14/50 Iteration: 27400 Avg. Training loss: 2.9477 0.0062 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 27500 Epoch 14/50 Iteration: 27500 Avg. Training loss: 2.9547 0.0081 sec/batch\n",
      "Global Step: 27600 Epoch 14/50 Iteration: 27600 Avg. Training loss: 2.9648 0.0140 sec/batch\n",
      "Global Step: 27700 Epoch 14/50 Iteration: 27700 Avg. Training loss: 2.9291 0.0144 sec/batch\n",
      "Global Step: 27800 Epoch 14/50 Iteration: 27800 Avg. Training loss: 2.9448 0.0130 sec/batch\n",
      "Global Step: 27900 Epoch 14/50 Iteration: 27900 Avg. Training loss: 2.9541 0.0130 sec/batch\n",
      "Global Step: 28000 Epoch 14/50 Iteration: 28000 Avg. Training loss: 2.9442 0.0130 sec/batch\n",
      "Global Step: 28100 Epoch 14/50 Iteration: 28100 Avg. Training loss: 2.9560 0.0122 sec/batch\n",
      "Global Step: 28200 Epoch 14/50 Iteration: 28200 Avg. Training loss: 2.9384 0.0130 sec/batch\n",
      "Global Step: 28300 Epoch 14/50 Iteration: 28300 Avg. Training loss: 2.9679 0.0123 sec/batch\n",
      "Global Step: 28400 Epoch 14/50 Iteration: 28400 Avg. Training loss: 2.9550 0.0129 sec/batch\n",
      "Global Step: 28500 Epoch 14/50 Iteration: 28500 Avg. Training loss: 2.9639 0.0130 sec/batch\n",
      "Global Step: 28600 Epoch 14/50 Iteration: 28600 Avg. Training loss: 2.9290 0.0140 sec/batch\n",
      "Global Step: 28700 Epoch 14/50 Iteration: 28700 Avg. Training loss: 2.9386 0.0137 sec/batch\n",
      "Global Step: 28800 Epoch 14/50 Iteration: 28800 Avg. Training loss: 2.9507 0.0125 sec/batch\n",
      "Global Step: 28900 Epoch 15/50 Iteration: 28900 Avg. Training loss: 2.9524 0.0074 sec/batch\n",
      "Global Step: 29000 Epoch 15/50 Iteration: 29000 Avg. Training loss: 2.9707 0.0144 sec/batch\n",
      "Global Step: 29100 Epoch 15/50 Iteration: 29100 Avg. Training loss: 2.9664 0.0125 sec/batch\n",
      "Global Step: 29200 Epoch 15/50 Iteration: 29200 Avg. Training loss: 2.9351 0.0129 sec/batch\n",
      "Global Step: 29300 Epoch 15/50 Iteration: 29300 Avg. Training loss: 2.9439 0.0139 sec/batch\n",
      "Global Step: 29400 Epoch 15/50 Iteration: 29400 Avg. Training loss: 2.9517 0.0133 sec/batch\n",
      "Global Step: 29500 Epoch 15/50 Iteration: 29500 Avg. Training loss: 2.9482 0.0164 sec/batch\n",
      "Global Step: 29600 Epoch 15/50 Iteration: 29600 Avg. Training loss: 2.9558 0.0153 sec/batch\n",
      "Global Step: 29700 Epoch 15/50 Iteration: 29700 Avg. Training loss: 2.9430 0.0146 sec/batch\n",
      "Global Step: 29800 Epoch 15/50 Iteration: 29800 Avg. Training loss: 2.9367 0.0150 sec/batch\n",
      "Global Step: 29900 Epoch 15/50 Iteration: 29900 Avg. Training loss: 2.9571 0.0152 sec/batch\n",
      "Global Step: 30000 Epoch 15/50 Iteration: 30000 Avg. Training loss: 2.9410 0.0132 sec/batch\n",
      "Global Step: 30100 Epoch 15/50 Iteration: 30100 Avg. Training loss: 2.9582 0.0159 sec/batch\n",
      "Global Step: 30200 Epoch 15/50 Iteration: 30200 Avg. Training loss: 2.9509 0.0156 sec/batch\n",
      "Global Step: 30300 Epoch 15/50 Iteration: 30300 Avg. Training loss: 2.9479 0.0127 sec/batch\n",
      "Global Step: 30400 Epoch 15/50 Iteration: 30400 Avg. Training loss: 2.9602 0.0148 sec/batch\n",
      "Global Step: 30500 Epoch 15/50 Iteration: 30500 Avg. Training loss: 2.9574 0.0145 sec/batch\n",
      "Global Step: 30600 Epoch 15/50 Iteration: 30600 Avg. Training loss: 2.9408 0.0116 sec/batch\n",
      "Global Step: 30700 Epoch 15/50 Iteration: 30700 Avg. Training loss: 2.9275 0.0138 sec/batch\n",
      "Global Step: 30800 Epoch 15/50 Iteration: 30800 Avg. Training loss: 2.9508 0.0144 sec/batch\n",
      "Global Step: 30900 Epoch 15/50 Iteration: 30900 Avg. Training loss: 2.9404 0.0139 sec/batch\n",
      "Global Step: 31000 Epoch 16/50 Iteration: 31000 Avg. Training loss: 2.9638 0.0134 sec/batch\n",
      "Global Step: 31100 Epoch 16/50 Iteration: 31100 Avg. Training loss: 2.9749 0.0149 sec/batch\n",
      "Global Step: 31200 Epoch 16/50 Iteration: 31200 Avg. Training loss: 2.9543 0.0154 sec/batch\n",
      "Global Step: 31300 Epoch 16/50 Iteration: 31300 Avg. Training loss: 2.9421 0.0137 sec/batch\n",
      "Global Step: 31400 Epoch 16/50 Iteration: 31400 Avg. Training loss: 2.9418 0.0140 sec/batch\n",
      "Global Step: 31500 Epoch 16/50 Iteration: 31500 Avg. Training loss: 2.9438 0.0162 sec/batch\n",
      "Global Step: 31600 Epoch 16/50 Iteration: 31600 Avg. Training loss: 2.9602 0.0151 sec/batch\n",
      "Global Step: 31700 Epoch 16/50 Iteration: 31700 Avg. Training loss: 2.9606 0.0145 sec/batch\n",
      "Global Step: 31800 Epoch 16/50 Iteration: 31800 Avg. Training loss: 2.9361 0.0147 sec/batch\n",
      "Global Step: 31900 Epoch 16/50 Iteration: 31900 Avg. Training loss: 2.9421 0.0151 sec/batch\n",
      "Global Step: 32000 Epoch 16/50 Iteration: 32000 Avg. Training loss: 2.9434 0.0159 sec/batch\n",
      "Global Step: 32100 Epoch 16/50 Iteration: 32100 Avg. Training loss: 2.9550 0.0147 sec/batch\n",
      "Global Step: 32200 Epoch 16/50 Iteration: 32200 Avg. Training loss: 2.9491 0.0143 sec/batch\n",
      "Global Step: 32300 Epoch 16/50 Iteration: 32300 Avg. Training loss: 2.9379 0.0149 sec/batch\n",
      "Global Step: 32400 Epoch 16/50 Iteration: 32400 Avg. Training loss: 2.9692 0.0131 sec/batch\n",
      "Global Step: 32500 Epoch 16/50 Iteration: 32500 Avg. Training loss: 2.9527 0.0143 sec/batch\n",
      "Global Step: 32600 Epoch 16/50 Iteration: 32600 Avg. Training loss: 2.9640 0.0149 sec/batch\n",
      "Global Step: 32700 Epoch 16/50 Iteration: 32700 Avg. Training loss: 2.9317 0.0143 sec/batch\n",
      "Global Step: 32800 Epoch 16/50 Iteration: 32800 Avg. Training loss: 2.9349 0.0126 sec/batch\n",
      "Global Step: 32900 Epoch 16/50 Iteration: 32900 Avg. Training loss: 2.9412 0.0151 sec/batch\n",
      "Global Step: 33000 Epoch 17/50 Iteration: 33000 Avg. Training loss: 2.9550 0.0052 sec/batch\n",
      "Global Step: 33100 Epoch 17/50 Iteration: 33100 Avg. Training loss: 2.9667 0.0136 sec/batch\n",
      "Global Step: 33200 Epoch 17/50 Iteration: 33200 Avg. Training loss: 2.9695 0.0147 sec/batch\n",
      "Global Step: 33300 Epoch 17/50 Iteration: 33300 Avg. Training loss: 2.9388 0.0150 sec/batch\n",
      "Global Step: 33400 Epoch 17/50 Iteration: 33400 Avg. Training loss: 2.9431 0.0144 sec/batch\n",
      "Global Step: 33500 Epoch 17/50 Iteration: 33500 Avg. Training loss: 2.9434 0.0156 sec/batch\n",
      "Global Step: 33600 Epoch 17/50 Iteration: 33600 Avg. Training loss: 2.9556 0.0144 sec/batch\n",
      "Global Step: 33700 Epoch 17/50 Iteration: 33700 Avg. Training loss: 2.9515 0.0147 sec/batch\n",
      "Global Step: 33800 Epoch 17/50 Iteration: 33800 Avg. Training loss: 2.9517 0.0138 sec/batch\n",
      "Global Step: 33900 Epoch 17/50 Iteration: 33900 Avg. Training loss: 2.9393 0.0152 sec/batch\n",
      "Global Step: 34000 Epoch 17/50 Iteration: 34000 Avg. Training loss: 2.9502 0.0149 sec/batch\n",
      "Global Step: 34100 Epoch 17/50 Iteration: 34100 Avg. Training loss: 2.9449 0.0144 sec/batch\n",
      "Global Step: 34200 Epoch 17/50 Iteration: 34200 Avg. Training loss: 2.9550 0.0139 sec/batch\n",
      "Global Step: 34300 Epoch 17/50 Iteration: 34300 Avg. Training loss: 2.9496 0.0141 sec/batch\n",
      "Global Step: 34400 Epoch 17/50 Iteration: 34400 Avg. Training loss: 2.9404 0.0132 sec/batch\n",
      "Global Step: 34500 Epoch 17/50 Iteration: 34500 Avg. Training loss: 2.9689 0.0147 sec/batch\n",
      "Global Step: 34600 Epoch 17/50 Iteration: 34600 Avg. Training loss: 2.9522 0.0150 sec/batch\n",
      "Global Step: 34700 Epoch 17/50 Iteration: 34700 Avg. Training loss: 2.9584 0.0137 sec/batch\n",
      "Global Step: 34800 Epoch 17/50 Iteration: 34800 Avg. Training loss: 2.9290 0.0149 sec/batch\n",
      "Global Step: 34900 Epoch 17/50 Iteration: 34900 Avg. Training loss: 2.9423 0.0142 sec/batch\n",
      "Global Step: 35000 Epoch 17/50 Iteration: 35000 Avg. Training loss: 2.9415 0.0135 sec/batch\n",
      "Global Step: 35100 Epoch 18/50 Iteration: 35100 Avg. Training loss: 2.9499 0.0048 sec/batch\n",
      "Global Step: 35200 Epoch 18/50 Iteration: 35200 Avg. Training loss: 2.9847 0.0070 sec/batch\n",
      "Global Step: 35300 Epoch 18/50 Iteration: 35300 Avg. Training loss: 2.9551 0.0082 sec/batch\n",
      "Global Step: 35400 Epoch 18/50 Iteration: 35400 Avg. Training loss: 2.9419 0.0062 sec/batch\n",
      "Global Step: 35500 Epoch 18/50 Iteration: 35500 Avg. Training loss: 2.9359 0.0054 sec/batch\n",
      "Global Step: 35600 Epoch 18/50 Iteration: 35600 Avg. Training loss: 2.9496 0.0052 sec/batch\n",
      "Global Step: 35700 Epoch 18/50 Iteration: 35700 Avg. Training loss: 2.9647 0.0066 sec/batch\n",
      "Global Step: 35800 Epoch 18/50 Iteration: 35800 Avg. Training loss: 2.9524 0.0058 sec/batch\n",
      "Global Step: 35900 Epoch 18/50 Iteration: 35900 Avg. Training loss: 2.9378 0.0063 sec/batch\n",
      "Global Step: 36000 Epoch 18/50 Iteration: 36000 Avg. Training loss: 2.9358 0.0088 sec/batch\n",
      "Global Step: 36100 Epoch 18/50 Iteration: 36100 Avg. Training loss: 2.9611 0.0134 sec/batch\n",
      "Global Step: 36200 Epoch 18/50 Iteration: 36200 Avg. Training loss: 2.9471 0.0148 sec/batch\n",
      "Global Step: 36300 Epoch 18/50 Iteration: 36300 Avg. Training loss: 2.9440 0.0144 sec/batch\n",
      "Global Step: 36400 Epoch 18/50 Iteration: 36400 Avg. Training loss: 2.9519 0.0133 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 36500 Epoch 18/50 Iteration: 36500 Avg. Training loss: 2.9511 0.0130 sec/batch\n",
      "Global Step: 36600 Epoch 18/50 Iteration: 36600 Avg. Training loss: 2.9537 0.0135 sec/batch\n",
      "Global Step: 36700 Epoch 18/50 Iteration: 36700 Avg. Training loss: 2.9702 0.0144 sec/batch\n",
      "Global Step: 36800 Epoch 18/50 Iteration: 36800 Avg. Training loss: 2.9359 0.0129 sec/batch\n",
      "Global Step: 36900 Epoch 18/50 Iteration: 36900 Avg. Training loss: 2.9287 0.0135 sec/batch\n",
      "Global Step: 37000 Epoch 18/50 Iteration: 37000 Avg. Training loss: 2.9425 0.0150 sec/batch\n",
      "Global Step: 37100 Epoch 19/50 Iteration: 37100 Avg. Training loss: 2.9518 0.0027 sec/batch\n",
      "Global Step: 37200 Epoch 19/50 Iteration: 37200 Avg. Training loss: 2.9629 0.0121 sec/batch\n",
      "Global Step: 37300 Epoch 19/50 Iteration: 37300 Avg. Training loss: 2.9716 0.0128 sec/batch\n",
      "Global Step: 37400 Epoch 19/50 Iteration: 37400 Avg. Training loss: 2.9502 0.0134 sec/batch\n",
      "Global Step: 37500 Epoch 19/50 Iteration: 37500 Avg. Training loss: 2.9479 0.0132 sec/batch\n",
      "Global Step: 37600 Epoch 19/50 Iteration: 37600 Avg. Training loss: 2.9392 0.0127 sec/batch\n",
      "Global Step: 37700 Epoch 19/50 Iteration: 37700 Avg. Training loss: 2.9467 0.0125 sec/batch\n",
      "Global Step: 37800 Epoch 19/50 Iteration: 37800 Avg. Training loss: 2.9549 0.0144 sec/batch\n",
      "Global Step: 37900 Epoch 19/50 Iteration: 37900 Avg. Training loss: 2.9633 0.0127 sec/batch\n",
      "Global Step: 38000 Epoch 19/50 Iteration: 38000 Avg. Training loss: 2.9295 0.0128 sec/batch\n",
      "Global Step: 38100 Epoch 19/50 Iteration: 38100 Avg. Training loss: 2.9439 0.0163 sec/batch\n",
      "Global Step: 38200 Epoch 19/50 Iteration: 38200 Avg. Training loss: 2.9544 0.0146 sec/batch\n",
      "Global Step: 38300 Epoch 19/50 Iteration: 38300 Avg. Training loss: 2.9451 0.0133 sec/batch\n",
      "Global Step: 38400 Epoch 19/50 Iteration: 38400 Avg. Training loss: 2.9573 0.0148 sec/batch\n",
      "Global Step: 38500 Epoch 19/50 Iteration: 38500 Avg. Training loss: 2.9361 0.0152 sec/batch\n",
      "Global Step: 38600 Epoch 19/50 Iteration: 38600 Avg. Training loss: 2.9660 0.0142 sec/batch\n",
      "Global Step: 38700 Epoch 19/50 Iteration: 38700 Avg. Training loss: 2.9557 0.0150 sec/batch\n",
      "Global Step: 38800 Epoch 19/50 Iteration: 38800 Avg. Training loss: 2.9625 0.0146 sec/batch\n",
      "Global Step: 38900 Epoch 19/50 Iteration: 38900 Avg. Training loss: 2.9289 0.0147 sec/batch\n",
      "Global Step: 39000 Epoch 19/50 Iteration: 39000 Avg. Training loss: 2.9380 0.0146 sec/batch\n",
      "Global Step: 39100 Epoch 19/50 Iteration: 39100 Avg. Training loss: 2.9475 0.0142 sec/batch\n",
      "Global Step: 39200 Epoch 20/50 Iteration: 39200 Avg. Training loss: 2.9544 0.0092 sec/batch\n",
      "Global Step: 39300 Epoch 20/50 Iteration: 39300 Avg. Training loss: 2.9699 0.0156 sec/batch\n",
      "Global Step: 39400 Epoch 20/50 Iteration: 39400 Avg. Training loss: 2.9647 0.0142 sec/batch\n",
      "Global Step: 39500 Epoch 20/50 Iteration: 39500 Avg. Training loss: 2.9362 0.0136 sec/batch\n",
      "Global Step: 39600 Epoch 20/50 Iteration: 39600 Avg. Training loss: 2.9431 0.0164 sec/batch\n",
      "Global Step: 39700 Epoch 20/50 Iteration: 39700 Avg. Training loss: 2.9523 0.0136 sec/batch\n",
      "Global Step: 39800 Epoch 20/50 Iteration: 39800 Avg. Training loss: 2.9478 0.0125 sec/batch\n",
      "Global Step: 39900 Epoch 20/50 Iteration: 39900 Avg. Training loss: 2.9555 0.0140 sec/batch\n",
      "Global Step: 40000 Epoch 20/50 Iteration: 40000 Avg. Training loss: 2.9448 0.0139 sec/batch\n",
      "Global Step: 40100 Epoch 20/50 Iteration: 40100 Avg. Training loss: 2.9363 0.0136 sec/batch\n",
      "Global Step: 40200 Epoch 20/50 Iteration: 40200 Avg. Training loss: 2.9578 0.0151 sec/batch\n",
      "Global Step: 40300 Epoch 20/50 Iteration: 40300 Avg. Training loss: 2.9430 0.0138 sec/batch\n",
      "Global Step: 40400 Epoch 20/50 Iteration: 40400 Avg. Training loss: 2.9576 0.0137 sec/batch\n",
      "Global Step: 40500 Epoch 20/50 Iteration: 40500 Avg. Training loss: 2.9500 0.0154 sec/batch\n",
      "Global Step: 40600 Epoch 20/50 Iteration: 40600 Avg. Training loss: 2.9469 0.0137 sec/batch\n",
      "Global Step: 40700 Epoch 20/50 Iteration: 40700 Avg. Training loss: 2.9615 0.0148 sec/batch\n",
      "Global Step: 40800 Epoch 20/50 Iteration: 40800 Avg. Training loss: 2.9592 0.0145 sec/batch\n",
      "Global Step: 40900 Epoch 20/50 Iteration: 40900 Avg. Training loss: 2.9416 0.0140 sec/batch\n",
      "Global Step: 41000 Epoch 20/50 Iteration: 41000 Avg. Training loss: 2.9264 0.0147 sec/batch\n",
      "Global Step: 41100 Epoch 20/50 Iteration: 41100 Avg. Training loss: 2.9498 0.0145 sec/batch\n",
      "Global Step: 41200 Epoch 20/50 Iteration: 41200 Avg. Training loss: 2.9420 0.0149 sec/batch\n",
      "Global Step: 41300 Epoch 21/50 Iteration: 41300 Avg. Training loss: 2.9634 0.0141 sec/batch\n",
      "Global Step: 41400 Epoch 21/50 Iteration: 41400 Avg. Training loss: 2.9732 0.0145 sec/batch\n",
      "Global Step: 41500 Epoch 21/50 Iteration: 41500 Avg. Training loss: 2.9533 0.0156 sec/batch\n",
      "Global Step: 41600 Epoch 21/50 Iteration: 41600 Avg. Training loss: 2.9428 0.0155 sec/batch\n",
      "Global Step: 41700 Epoch 21/50 Iteration: 41700 Avg. Training loss: 2.9407 0.0133 sec/batch\n",
      "Global Step: 41800 Epoch 21/50 Iteration: 41800 Avg. Training loss: 2.9437 0.0136 sec/batch\n",
      "Global Step: 41900 Epoch 21/50 Iteration: 41900 Avg. Training loss: 2.9597 0.0155 sec/batch\n",
      "Global Step: 42000 Epoch 21/50 Iteration: 42000 Avg. Training loss: 2.9604 0.0134 sec/batch\n",
      "Global Step: 42100 Epoch 21/50 Iteration: 42100 Avg. Training loss: 2.9365 0.0121 sec/batch\n",
      "Global Step: 42200 Epoch 21/50 Iteration: 42200 Avg. Training loss: 2.9426 0.0131 sec/batch\n",
      "Global Step: 42300 Epoch 21/50 Iteration: 42300 Avg. Training loss: 2.9420 0.0147 sec/batch\n",
      "Global Step: 42400 Epoch 21/50 Iteration: 42400 Avg. Training loss: 2.9561 0.0136 sec/batch\n",
      "Global Step: 42500 Epoch 21/50 Iteration: 42500 Avg. Training loss: 2.9491 0.0124 sec/batch\n",
      "Global Step: 42600 Epoch 21/50 Iteration: 42600 Avg. Training loss: 2.9386 0.0150 sec/batch\n",
      "Global Step: 42700 Epoch 21/50 Iteration: 42700 Avg. Training loss: 2.9692 0.0152 sec/batch\n",
      "Global Step: 42800 Epoch 21/50 Iteration: 42800 Avg. Training loss: 2.9525 0.0137 sec/batch\n",
      "Global Step: 42900 Epoch 21/50 Iteration: 42900 Avg. Training loss: 2.9649 0.0122 sec/batch\n",
      "Global Step: 43000 Epoch 21/50 Iteration: 43000 Avg. Training loss: 2.9312 0.0139 sec/batch\n",
      "Global Step: 43100 Epoch 21/50 Iteration: 43100 Avg. Training loss: 2.9337 0.0144 sec/batch\n",
      "Global Step: 43200 Epoch 21/50 Iteration: 43200 Avg. Training loss: 2.9412 0.0166 sec/batch\n",
      "Global Step: 43300 Epoch 22/50 Iteration: 43300 Avg. Training loss: 2.9555 0.0059 sec/batch\n",
      "Global Step: 43400 Epoch 22/50 Iteration: 43400 Avg. Training loss: 2.9676 0.0151 sec/batch\n",
      "Global Step: 43500 Epoch 22/50 Iteration: 43500 Avg. Training loss: 2.9687 0.0153 sec/batch\n",
      "Global Step: 43600 Epoch 22/50 Iteration: 43600 Avg. Training loss: 2.9401 0.0128 sec/batch\n",
      "Global Step: 43700 Epoch 22/50 Iteration: 43700 Avg. Training loss: 2.9444 0.0160 sec/batch\n",
      "Global Step: 43800 Epoch 22/50 Iteration: 43800 Avg. Training loss: 2.9423 0.0151 sec/batch\n",
      "Global Step: 43900 Epoch 22/50 Iteration: 43900 Avg. Training loss: 2.9552 0.0139 sec/batch\n",
      "Global Step: 44000 Epoch 22/50 Iteration: 44000 Avg. Training loss: 2.9510 0.0146 sec/batch\n",
      "Global Step: 44100 Epoch 22/50 Iteration: 44100 Avg. Training loss: 2.9507 0.0148 sec/batch\n",
      "Global Step: 44200 Epoch 22/50 Iteration: 44200 Avg. Training loss: 2.9392 0.0149 sec/batch\n",
      "Global Step: 44300 Epoch 22/50 Iteration: 44300 Avg. Training loss: 2.9486 0.0139 sec/batch\n",
      "Global Step: 44400 Epoch 22/50 Iteration: 44400 Avg. Training loss: 2.9439 0.0146 sec/batch\n",
      "Global Step: 44500 Epoch 22/50 Iteration: 44500 Avg. Training loss: 2.9570 0.0156 sec/batch\n",
      "Global Step: 44600 Epoch 22/50 Iteration: 44600 Avg. Training loss: 2.9487 0.0139 sec/batch\n",
      "Global Step: 44700 Epoch 22/50 Iteration: 44700 Avg. Training loss: 2.9407 0.0138 sec/batch\n",
      "Global Step: 44800 Epoch 22/50 Iteration: 44800 Avg. Training loss: 2.9695 0.0141 sec/batch\n",
      "Global Step: 44900 Epoch 22/50 Iteration: 44900 Avg. Training loss: 2.9519 0.0154 sec/batch\n",
      "Global Step: 45000 Epoch 22/50 Iteration: 45000 Avg. Training loss: 2.9565 0.0141 sec/batch\n",
      "Global Step: 45100 Epoch 22/50 Iteration: 45100 Avg. Training loss: 2.9269 0.0128 sec/batch\n",
      "Global Step: 45200 Epoch 22/50 Iteration: 45200 Avg. Training loss: 2.9435 0.0157 sec/batch\n",
      "Global Step: 45300 Epoch 22/50 Iteration: 45300 Avg. Training loss: 2.9413 0.0149 sec/batch\n",
      "Global Step: 45400 Epoch 23/50 Iteration: 45400 Avg. Training loss: 2.9485 0.0119 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 45500 Epoch 23/50 Iteration: 45500 Avg. Training loss: 2.9850 0.0145 sec/batch\n",
      "Global Step: 45600 Epoch 23/50 Iteration: 45600 Avg. Training loss: 2.9552 0.0146 sec/batch\n",
      "Global Step: 45700 Epoch 23/50 Iteration: 45700 Avg. Training loss: 2.9413 0.0149 sec/batch\n",
      "Global Step: 45800 Epoch 23/50 Iteration: 45800 Avg. Training loss: 2.9361 0.0125 sec/batch\n",
      "Global Step: 45900 Epoch 23/50 Iteration: 45900 Avg. Training loss: 2.9488 0.0142 sec/batch\n",
      "Global Step: 46000 Epoch 23/50 Iteration: 46000 Avg. Training loss: 2.9637 0.0137 sec/batch\n",
      "Global Step: 46100 Epoch 23/50 Iteration: 46100 Avg. Training loss: 2.9521 0.0134 sec/batch\n",
      "Global Step: 46200 Epoch 23/50 Iteration: 46200 Avg. Training loss: 2.9369 0.0151 sec/batch\n",
      "Global Step: 46300 Epoch 23/50 Iteration: 46300 Avg. Training loss: 2.9356 0.0148 sec/batch\n",
      "Global Step: 46400 Epoch 23/50 Iteration: 46400 Avg. Training loss: 2.9618 0.0139 sec/batch\n",
      "Global Step: 46500 Epoch 23/50 Iteration: 46500 Avg. Training loss: 2.9473 0.0134 sec/batch\n",
      "Global Step: 46600 Epoch 23/50 Iteration: 46600 Avg. Training loss: 2.9432 0.0120 sec/batch\n",
      "Global Step: 46700 Epoch 23/50 Iteration: 46700 Avg. Training loss: 2.9517 0.0139 sec/batch\n",
      "Global Step: 46800 Epoch 23/50 Iteration: 46800 Avg. Training loss: 2.9512 0.0149 sec/batch\n",
      "Global Step: 46900 Epoch 23/50 Iteration: 46900 Avg. Training loss: 2.9518 0.0138 sec/batch\n",
      "Global Step: 47000 Epoch 23/50 Iteration: 47000 Avg. Training loss: 2.9682 0.0138 sec/batch\n",
      "Global Step: 47100 Epoch 23/50 Iteration: 47100 Avg. Training loss: 2.9371 0.0155 sec/batch\n",
      "Global Step: 47200 Epoch 23/50 Iteration: 47200 Avg. Training loss: 2.9276 0.0143 sec/batch\n",
      "Global Step: 47300 Epoch 23/50 Iteration: 47300 Avg. Training loss: 2.9425 0.0146 sec/batch\n",
      "Global Step: 47400 Epoch 24/50 Iteration: 47400 Avg. Training loss: 2.9522 0.0029 sec/batch\n",
      "Global Step: 47500 Epoch 24/50 Iteration: 47500 Avg. Training loss: 2.9619 0.0139 sec/batch\n",
      "Global Step: 47600 Epoch 24/50 Iteration: 47600 Avg. Training loss: 2.9717 0.0160 sec/batch\n",
      "Global Step: 47700 Epoch 24/50 Iteration: 47700 Avg. Training loss: 2.9496 0.0154 sec/batch\n",
      "Global Step: 47800 Epoch 24/50 Iteration: 47800 Avg. Training loss: 2.9473 0.0142 sec/batch\n",
      "Global Step: 47900 Epoch 24/50 Iteration: 47900 Avg. Training loss: 2.9378 0.0150 sec/batch\n",
      "Global Step: 48000 Epoch 24/50 Iteration: 48000 Avg. Training loss: 2.9464 0.0154 sec/batch\n",
      "Global Step: 48100 Epoch 24/50 Iteration: 48100 Avg. Training loss: 2.9519 0.0108 sec/batch\n",
      "Global Step: 48200 Epoch 24/50 Iteration: 48200 Avg. Training loss: 2.9645 0.0136 sec/batch\n",
      "Global Step: 48300 Epoch 24/50 Iteration: 48300 Avg. Training loss: 2.9285 0.0150 sec/batch\n",
      "Global Step: 48400 Epoch 24/50 Iteration: 48400 Avg. Training loss: 2.9434 0.0150 sec/batch\n",
      "Global Step: 48500 Epoch 24/50 Iteration: 48500 Avg. Training loss: 2.9540 0.0130 sec/batch\n",
      "Global Step: 48600 Epoch 24/50 Iteration: 48600 Avg. Training loss: 2.9443 0.0153 sec/batch\n",
      "Global Step: 48700 Epoch 24/50 Iteration: 48700 Avg. Training loss: 2.9565 0.0143 sec/batch\n",
      "Global Step: 48800 Epoch 24/50 Iteration: 48800 Avg. Training loss: 2.9358 0.0136 sec/batch\n",
      "Global Step: 48900 Epoch 24/50 Iteration: 48900 Avg. Training loss: 2.9661 0.0144 sec/batch\n",
      "Global Step: 49000 Epoch 24/50 Iteration: 49000 Avg. Training loss: 2.9535 0.0135 sec/batch\n",
      "Global Step: 49100 Epoch 24/50 Iteration: 49100 Avg. Training loss: 2.9625 0.0154 sec/batch\n",
      "Global Step: 49200 Epoch 24/50 Iteration: 49200 Avg. Training loss: 2.9285 0.0144 sec/batch\n",
      "Global Step: 49300 Epoch 24/50 Iteration: 49300 Avg. Training loss: 2.9379 0.0144 sec/batch\n",
      "Global Step: 49400 Epoch 24/50 Iteration: 49400 Avg. Training loss: 2.9483 0.0143 sec/batch\n",
      "Global Step: 49500 Epoch 25/50 Iteration: 49500 Avg. Training loss: 2.9522 0.0092 sec/batch\n",
      "Global Step: 49600 Epoch 25/50 Iteration: 49600 Avg. Training loss: 2.9699 0.0125 sec/batch\n",
      "Global Step: 49700 Epoch 25/50 Iteration: 49700 Avg. Training loss: 2.9651 0.0146 sec/batch\n",
      "Global Step: 49800 Epoch 25/50 Iteration: 49800 Avg. Training loss: 2.9367 0.0149 sec/batch\n",
      "Global Step: 49900 Epoch 25/50 Iteration: 49900 Avg. Training loss: 2.9436 0.0139 sec/batch\n",
      "Global Step: 50000 Epoch 25/50 Iteration: 50000 Avg. Training loss: 2.9519 0.0153 sec/batch\n",
      "Global Step: 50100 Epoch 25/50 Iteration: 50100 Avg. Training loss: 2.9472 0.0151 sec/batch\n",
      "Global Step: 50200 Epoch 25/50 Iteration: 50200 Avg. Training loss: 2.9557 0.0144 sec/batch\n",
      "Global Step: 50300 Epoch 25/50 Iteration: 50300 Avg. Training loss: 2.9451 0.0146 sec/batch\n",
      "Global Step: 50400 Epoch 25/50 Iteration: 50400 Avg. Training loss: 2.9377 0.0139 sec/batch\n",
      "Global Step: 50500 Epoch 25/50 Iteration: 50500 Avg. Training loss: 2.9564 0.0147 sec/batch\n",
      "Global Step: 50600 Epoch 25/50 Iteration: 50600 Avg. Training loss: 2.9398 0.0143 sec/batch\n",
      "Global Step: 50700 Epoch 25/50 Iteration: 50700 Avg. Training loss: 2.9584 0.0153 sec/batch\n",
      "Global Step: 50800 Epoch 25/50 Iteration: 50800 Avg. Training loss: 2.9496 0.0136 sec/batch\n",
      "Global Step: 50900 Epoch 25/50 Iteration: 50900 Avg. Training loss: 2.9475 0.0140 sec/batch\n",
      "Global Step: 51000 Epoch 25/50 Iteration: 51000 Avg. Training loss: 2.9609 0.0150 sec/batch\n",
      "Global Step: 51100 Epoch 25/50 Iteration: 51100 Avg. Training loss: 2.9579 0.0119 sec/batch\n",
      "Global Step: 51200 Epoch 25/50 Iteration: 51200 Avg. Training loss: 2.9409 0.0073 sec/batch\n",
      "Global Step: 51300 Epoch 25/50 Iteration: 51300 Avg. Training loss: 2.9254 0.0063 sec/batch\n",
      "Global Step: 51400 Epoch 25/50 Iteration: 51400 Avg. Training loss: 2.9492 0.0082 sec/batch\n",
      "Global Step: 51500 Epoch 25/50 Iteration: 51500 Avg. Training loss: 2.9407 0.0063 sec/batch\n",
      "Global Step: 51600 Epoch 26/50 Iteration: 51600 Avg. Training loss: 2.9629 0.0065 sec/batch\n",
      "Global Step: 51700 Epoch 26/50 Iteration: 51700 Avg. Training loss: 2.9734 0.0078 sec/batch\n",
      "Global Step: 51800 Epoch 26/50 Iteration: 51800 Avg. Training loss: 2.9523 0.0076 sec/batch\n",
      "Global Step: 51900 Epoch 26/50 Iteration: 51900 Avg. Training loss: 2.9434 0.0080 sec/batch\n",
      "Global Step: 52000 Epoch 26/50 Iteration: 52000 Avg. Training loss: 2.9420 0.0067 sec/batch\n",
      "Global Step: 52100 Epoch 26/50 Iteration: 52100 Avg. Training loss: 2.9443 0.0082 sec/batch\n",
      "Global Step: 52200 Epoch 26/50 Iteration: 52200 Avg. Training loss: 2.9602 0.0062 sec/batch\n",
      "Global Step: 52300 Epoch 26/50 Iteration: 52300 Avg. Training loss: 2.9580 0.0118 sec/batch\n",
      "Global Step: 52400 Epoch 26/50 Iteration: 52400 Avg. Training loss: 2.9359 0.0133 sec/batch\n",
      "Global Step: 52500 Epoch 26/50 Iteration: 52500 Avg. Training loss: 2.9428 0.0137 sec/batch\n",
      "Global Step: 52600 Epoch 26/50 Iteration: 52600 Avg. Training loss: 2.9416 0.0150 sec/batch\n",
      "Global Step: 52700 Epoch 26/50 Iteration: 52700 Avg. Training loss: 2.9558 0.0134 sec/batch\n",
      "Global Step: 52800 Epoch 26/50 Iteration: 52800 Avg. Training loss: 2.9489 0.0132 sec/batch\n",
      "Global Step: 52900 Epoch 26/50 Iteration: 52900 Avg. Training loss: 2.9375 0.0148 sec/batch\n",
      "Global Step: 53000 Epoch 26/50 Iteration: 53000 Avg. Training loss: 2.9688 0.0145 sec/batch\n",
      "Global Step: 53100 Epoch 26/50 Iteration: 53100 Avg. Training loss: 2.9515 0.0133 sec/batch\n",
      "Global Step: 53200 Epoch 26/50 Iteration: 53200 Avg. Training loss: 2.9646 0.0123 sec/batch\n",
      "Global Step: 53300 Epoch 26/50 Iteration: 53300 Avg. Training loss: 2.9313 0.0129 sec/batch\n",
      "Global Step: 53400 Epoch 26/50 Iteration: 53400 Avg. Training loss: 2.9349 0.0132 sec/batch\n",
      "Global Step: 53500 Epoch 26/50 Iteration: 53500 Avg. Training loss: 2.9404 0.0130 sec/batch\n",
      "Global Step: 53600 Epoch 27/50 Iteration: 53600 Avg. Training loss: 2.9564 0.0051 sec/batch\n",
      "Global Step: 53700 Epoch 27/50 Iteration: 53700 Avg. Training loss: 2.9656 0.0151 sec/batch\n",
      "Global Step: 53800 Epoch 27/50 Iteration: 53800 Avg. Training loss: 2.9694 0.0132 sec/batch\n",
      "Global Step: 53900 Epoch 27/50 Iteration: 53900 Avg. Training loss: 2.9396 0.0142 sec/batch\n",
      "Global Step: 54000 Epoch 27/50 Iteration: 54000 Avg. Training loss: 2.9438 0.0149 sec/batch\n",
      "Global Step: 54100 Epoch 27/50 Iteration: 54100 Avg. Training loss: 2.9429 0.0138 sec/batch\n",
      "Global Step: 54200 Epoch 27/50 Iteration: 54200 Avg. Training loss: 2.9543 0.0156 sec/batch\n",
      "Global Step: 54300 Epoch 27/50 Iteration: 54300 Avg. Training loss: 2.9514 0.0138 sec/batch\n",
      "Global Step: 54400 Epoch 27/50 Iteration: 54400 Avg. Training loss: 2.9499 0.0135 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 54500 Epoch 27/50 Iteration: 54500 Avg. Training loss: 2.9391 0.0149 sec/batch\n",
      "Global Step: 54600 Epoch 27/50 Iteration: 54600 Avg. Training loss: 2.9506 0.0127 sec/batch\n",
      "Global Step: 54700 Epoch 27/50 Iteration: 54700 Avg. Training loss: 2.9433 0.0154 sec/batch\n",
      "Global Step: 54800 Epoch 27/50 Iteration: 54800 Avg. Training loss: 2.9558 0.0140 sec/batch\n",
      "Global Step: 54900 Epoch 27/50 Iteration: 54900 Avg. Training loss: 2.9492 0.0141 sec/batch\n",
      "Global Step: 55000 Epoch 27/50 Iteration: 55000 Avg. Training loss: 2.9400 0.0145 sec/batch\n",
      "Global Step: 55100 Epoch 27/50 Iteration: 55100 Avg. Training loss: 2.9689 0.0131 sec/batch\n",
      "Global Step: 55200 Epoch 27/50 Iteration: 55200 Avg. Training loss: 2.9512 0.0140 sec/batch\n",
      "Global Step: 55300 Epoch 27/50 Iteration: 55300 Avg. Training loss: 2.9563 0.0136 sec/batch\n",
      "Global Step: 55400 Epoch 27/50 Iteration: 55400 Avg. Training loss: 2.9275 0.0139 sec/batch\n",
      "Global Step: 55500 Epoch 27/50 Iteration: 55500 Avg. Training loss: 2.9413 0.0140 sec/batch\n",
      "Global Step: 55600 Epoch 27/50 Iteration: 55600 Avg. Training loss: 2.9416 0.0141 sec/batch\n",
      "Global Step: 55700 Epoch 28/50 Iteration: 55700 Avg. Training loss: 2.9480 0.0120 sec/batch\n",
      "Global Step: 55800 Epoch 28/50 Iteration: 55800 Avg. Training loss: 2.9852 0.0141 sec/batch\n",
      "Global Step: 55900 Epoch 28/50 Iteration: 55900 Avg. Training loss: 2.9539 0.0159 sec/batch\n",
      "Global Step: 56000 Epoch 28/50 Iteration: 56000 Avg. Training loss: 2.9410 0.0136 sec/batch\n",
      "Global Step: 56100 Epoch 28/50 Iteration: 56100 Avg. Training loss: 2.9354 0.0125 sec/batch\n",
      "Global Step: 56200 Epoch 28/50 Iteration: 56200 Avg. Training loss: 2.9482 0.0150 sec/batch\n",
      "Global Step: 56300 Epoch 28/50 Iteration: 56300 Avg. Training loss: 2.9635 0.0131 sec/batch\n",
      "Global Step: 56400 Epoch 28/50 Iteration: 56400 Avg. Training loss: 2.9521 0.0134 sec/batch\n",
      "Global Step: 56500 Epoch 28/50 Iteration: 56500 Avg. Training loss: 2.9379 0.0139 sec/batch\n",
      "Global Step: 56600 Epoch 28/50 Iteration: 56600 Avg. Training loss: 2.9352 0.0136 sec/batch\n",
      "Global Step: 56700 Epoch 28/50 Iteration: 56700 Avg. Training loss: 2.9613 0.0147 sec/batch\n",
      "Global Step: 56800 Epoch 28/50 Iteration: 56800 Avg. Training loss: 2.9453 0.0140 sec/batch\n",
      "Global Step: 56900 Epoch 28/50 Iteration: 56900 Avg. Training loss: 2.9439 0.0133 sec/batch\n",
      "Global Step: 57000 Epoch 28/50 Iteration: 57000 Avg. Training loss: 2.9523 0.0152 sec/batch\n",
      "Global Step: 57100 Epoch 28/50 Iteration: 57100 Avg. Training loss: 2.9499 0.0143 sec/batch\n",
      "Global Step: 57200 Epoch 28/50 Iteration: 57200 Avg. Training loss: 2.9527 0.0150 sec/batch\n",
      "Global Step: 57300 Epoch 28/50 Iteration: 57300 Avg. Training loss: 2.9685 0.0162 sec/batch\n",
      "Global Step: 57400 Epoch 28/50 Iteration: 57400 Avg. Training loss: 2.9358 0.0145 sec/batch\n",
      "Global Step: 57500 Epoch 28/50 Iteration: 57500 Avg. Training loss: 2.9294 0.0149 sec/batch\n",
      "Global Step: 57600 Epoch 28/50 Iteration: 57600 Avg. Training loss: 2.9422 0.0152 sec/batch\n",
      "Global Step: 57700 Epoch 29/50 Iteration: 57700 Avg. Training loss: 2.9523 0.0032 sec/batch\n",
      "Global Step: 57800 Epoch 29/50 Iteration: 57800 Avg. Training loss: 2.9610 0.0144 sec/batch\n",
      "Global Step: 57900 Epoch 29/50 Iteration: 57900 Avg. Training loss: 2.9723 0.0142 sec/batch\n",
      "Global Step: 58000 Epoch 29/50 Iteration: 58000 Avg. Training loss: 2.9493 0.0142 sec/batch\n",
      "Global Step: 58100 Epoch 29/50 Iteration: 58100 Avg. Training loss: 2.9462 0.0150 sec/batch\n",
      "Global Step: 58200 Epoch 29/50 Iteration: 58200 Avg. Training loss: 2.9386 0.0139 sec/batch\n",
      "Global Step: 58300 Epoch 29/50 Iteration: 58300 Avg. Training loss: 2.9465 0.0131 sec/batch\n",
      "Global Step: 58400 Epoch 29/50 Iteration: 58400 Avg. Training loss: 2.9524 0.0141 sec/batch\n",
      "Global Step: 58500 Epoch 29/50 Iteration: 58500 Avg. Training loss: 2.9616 0.0141 sec/batch\n",
      "Global Step: 58600 Epoch 29/50 Iteration: 58600 Avg. Training loss: 2.9284 0.0132 sec/batch\n",
      "Global Step: 58700 Epoch 29/50 Iteration: 58700 Avg. Training loss: 2.9446 0.0153 sec/batch\n",
      "Global Step: 58800 Epoch 29/50 Iteration: 58800 Avg. Training loss: 2.9545 0.0152 sec/batch\n",
      "Global Step: 58900 Epoch 29/50 Iteration: 58900 Avg. Training loss: 2.9425 0.0140 sec/batch\n",
      "Global Step: 59000 Epoch 29/50 Iteration: 59000 Avg. Training loss: 2.9569 0.0152 sec/batch\n",
      "Global Step: 59100 Epoch 29/50 Iteration: 59100 Avg. Training loss: 2.9377 0.0126 sec/batch\n",
      "Global Step: 59200 Epoch 29/50 Iteration: 59200 Avg. Training loss: 2.9666 0.0146 sec/batch\n",
      "Global Step: 59300 Epoch 29/50 Iteration: 59300 Avg. Training loss: 2.9548 0.0149 sec/batch\n",
      "Global Step: 59400 Epoch 29/50 Iteration: 59400 Avg. Training loss: 2.9627 0.0137 sec/batch\n",
      "Global Step: 59500 Epoch 29/50 Iteration: 59500 Avg. Training loss: 2.9277 0.0153 sec/batch\n",
      "Global Step: 59600 Epoch 29/50 Iteration: 59600 Avg. Training loss: 2.9377 0.0147 sec/batch\n",
      "Global Step: 59700 Epoch 29/50 Iteration: 59700 Avg. Training loss: 2.9486 0.0163 sec/batch\n",
      "Global Step: 59800 Epoch 30/50 Iteration: 59800 Avg. Training loss: 2.9525 0.0080 sec/batch\n",
      "Global Step: 59900 Epoch 30/50 Iteration: 59900 Avg. Training loss: 2.9689 0.0130 sec/batch\n",
      "Global Step: 60000 Epoch 30/50 Iteration: 60000 Avg. Training loss: 2.9631 0.0143 sec/batch\n",
      "Global Step: 60100 Epoch 30/50 Iteration: 60100 Avg. Training loss: 2.9357 0.0151 sec/batch\n",
      "Global Step: 60200 Epoch 30/50 Iteration: 60200 Avg. Training loss: 2.9435 0.0135 sec/batch\n",
      "Global Step: 60300 Epoch 30/50 Iteration: 60300 Avg. Training loss: 2.9515 0.0140 sec/batch\n",
      "Global Step: 60400 Epoch 30/50 Iteration: 60400 Avg. Training loss: 2.9489 0.0148 sec/batch\n",
      "Global Step: 60500 Epoch 30/50 Iteration: 60500 Avg. Training loss: 2.9554 0.0148 sec/batch\n",
      "Global Step: 60600 Epoch 30/50 Iteration: 60600 Avg. Training loss: 2.9443 0.0120 sec/batch\n",
      "Global Step: 60700 Epoch 30/50 Iteration: 60700 Avg. Training loss: 2.9361 0.0141 sec/batch\n",
      "Global Step: 60800 Epoch 30/50 Iteration: 60800 Avg. Training loss: 2.9553 0.0133 sec/batch\n",
      "Global Step: 60900 Epoch 30/50 Iteration: 60900 Avg. Training loss: 2.9409 0.0138 sec/batch\n",
      "Global Step: 61000 Epoch 30/50 Iteration: 61000 Avg. Training loss: 2.9589 0.0154 sec/batch\n",
      "Global Step: 61100 Epoch 30/50 Iteration: 61100 Avg. Training loss: 2.9498 0.0136 sec/batch\n",
      "Global Step: 61200 Epoch 30/50 Iteration: 61200 Avg. Training loss: 2.9471 0.0131 sec/batch\n",
      "Global Step: 61300 Epoch 30/50 Iteration: 61300 Avg. Training loss: 2.9599 0.0161 sec/batch\n",
      "Global Step: 61400 Epoch 30/50 Iteration: 61400 Avg. Training loss: 2.9588 0.0128 sec/batch\n",
      "Global Step: 61500 Epoch 30/50 Iteration: 61500 Avg. Training loss: 2.9417 0.0145 sec/batch\n",
      "Global Step: 61600 Epoch 30/50 Iteration: 61600 Avg. Training loss: 2.9260 0.0154 sec/batch\n",
      "Global Step: 61700 Epoch 30/50 Iteration: 61700 Avg. Training loss: 2.9487 0.0147 sec/batch\n",
      "Global Step: 61800 Epoch 30/50 Iteration: 61800 Avg. Training loss: 2.9402 0.0148 sec/batch\n",
      "Global Step: 61900 Epoch 31/50 Iteration: 61900 Avg. Training loss: 2.9633 0.0181 sec/batch\n",
      "Global Step: 62000 Epoch 31/50 Iteration: 62000 Avg. Training loss: 2.9712 0.0154 sec/batch\n",
      "Global Step: 62100 Epoch 31/50 Iteration: 62100 Avg. Training loss: 2.9532 0.0144 sec/batch\n",
      "Global Step: 62200 Epoch 31/50 Iteration: 62200 Avg. Training loss: 2.9421 0.0151 sec/batch\n",
      "Global Step: 62300 Epoch 31/50 Iteration: 62300 Avg. Training loss: 2.9413 0.0156 sec/batch\n",
      "Global Step: 62400 Epoch 31/50 Iteration: 62400 Avg. Training loss: 2.9430 0.0146 sec/batch\n",
      "Global Step: 62500 Epoch 31/50 Iteration: 62500 Avg. Training loss: 2.9584 0.0132 sec/batch\n",
      "Global Step: 62600 Epoch 31/50 Iteration: 62600 Avg. Training loss: 2.9593 0.0146 sec/batch\n",
      "Global Step: 62700 Epoch 31/50 Iteration: 62700 Avg. Training loss: 2.9371 0.0146 sec/batch\n",
      "Global Step: 62800 Epoch 31/50 Iteration: 62800 Avg. Training loss: 2.9422 0.0143 sec/batch\n",
      "Global Step: 62900 Epoch 31/50 Iteration: 62900 Avg. Training loss: 2.9430 0.0130 sec/batch\n",
      "Global Step: 63000 Epoch 31/50 Iteration: 63000 Avg. Training loss: 2.9544 0.0156 sec/batch\n",
      "Global Step: 63100 Epoch 31/50 Iteration: 63100 Avg. Training loss: 2.9486 0.0136 sec/batch\n",
      "Global Step: 63200 Epoch 31/50 Iteration: 63200 Avg. Training loss: 2.9371 0.0135 sec/batch\n",
      "Global Step: 63300 Epoch 31/50 Iteration: 63300 Avg. Training loss: 2.9681 0.0158 sec/batch\n",
      "Global Step: 63400 Epoch 31/50 Iteration: 63400 Avg. Training loss: 2.9511 0.0145 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 63500 Epoch 31/50 Iteration: 63500 Avg. Training loss: 2.9645 0.0136 sec/batch\n",
      "Global Step: 63600 Epoch 31/50 Iteration: 63600 Avg. Training loss: 2.9300 0.0139 sec/batch\n",
      "Global Step: 63700 Epoch 31/50 Iteration: 63700 Avg. Training loss: 2.9344 0.0144 sec/batch\n",
      "Global Step: 63800 Epoch 31/50 Iteration: 63800 Avg. Training loss: 2.9401 0.0144 sec/batch\n",
      "Global Step: 63900 Epoch 32/50 Iteration: 63900 Avg. Training loss: 2.9553 0.0061 sec/batch\n",
      "Global Step: 64000 Epoch 32/50 Iteration: 64000 Avg. Training loss: 2.9673 0.0144 sec/batch\n",
      "Global Step: 64100 Epoch 32/50 Iteration: 64100 Avg. Training loss: 2.9695 0.0147 sec/batch\n",
      "Global Step: 64200 Epoch 32/50 Iteration: 64200 Avg. Training loss: 2.9406 0.0155 sec/batch\n",
      "Global Step: 64300 Epoch 32/50 Iteration: 64300 Avg. Training loss: 2.9433 0.0147 sec/batch\n",
      "Global Step: 64400 Epoch 32/50 Iteration: 64400 Avg. Training loss: 2.9425 0.0140 sec/batch\n",
      "Global Step: 64500 Epoch 32/50 Iteration: 64500 Avg. Training loss: 2.9532 0.0149 sec/batch\n",
      "Global Step: 64600 Epoch 32/50 Iteration: 64600 Avg. Training loss: 2.9508 0.0146 sec/batch\n",
      "Global Step: 64700 Epoch 32/50 Iteration: 64700 Avg. Training loss: 2.9513 0.0139 sec/batch\n",
      "Global Step: 64800 Epoch 32/50 Iteration: 64800 Avg. Training loss: 2.9387 0.0158 sec/batch\n",
      "Global Step: 64900 Epoch 32/50 Iteration: 64900 Avg. Training loss: 2.9482 0.0142 sec/batch\n",
      "Global Step: 65000 Epoch 32/50 Iteration: 65000 Avg. Training loss: 2.9434 0.0131 sec/batch\n",
      "Global Step: 65100 Epoch 32/50 Iteration: 65100 Avg. Training loss: 2.9545 0.0143 sec/batch\n",
      "Global Step: 65200 Epoch 32/50 Iteration: 65200 Avg. Training loss: 2.9489 0.0148 sec/batch\n",
      "Global Step: 65300 Epoch 32/50 Iteration: 65300 Avg. Training loss: 2.9400 0.0134 sec/batch\n",
      "Global Step: 65400 Epoch 32/50 Iteration: 65400 Avg. Training loss: 2.9687 0.0138 sec/batch\n",
      "Global Step: 65500 Epoch 32/50 Iteration: 65500 Avg. Training loss: 2.9503 0.0151 sec/batch\n",
      "Global Step: 65600 Epoch 32/50 Iteration: 65600 Avg. Training loss: 2.9556 0.0149 sec/batch\n",
      "Global Step: 65700 Epoch 32/50 Iteration: 65700 Avg. Training loss: 2.9265 0.0144 sec/batch\n",
      "Global Step: 65800 Epoch 32/50 Iteration: 65800 Avg. Training loss: 2.9429 0.0167 sec/batch\n",
      "Global Step: 65900 Epoch 32/50 Iteration: 65900 Avg. Training loss: 2.9433 0.0137 sec/batch\n",
      "Global Step: 66000 Epoch 33/50 Iteration: 66000 Avg. Training loss: 2.9486 0.0124 sec/batch\n",
      "Global Step: 66100 Epoch 33/50 Iteration: 66100 Avg. Training loss: 2.9846 0.0141 sec/batch\n",
      "Global Step: 66200 Epoch 33/50 Iteration: 66200 Avg. Training loss: 2.9533 0.0152 sec/batch\n",
      "Global Step: 66300 Epoch 33/50 Iteration: 66300 Avg. Training loss: 2.9410 0.0157 sec/batch\n",
      "Global Step: 66400 Epoch 33/50 Iteration: 66400 Avg. Training loss: 2.9351 0.0150 sec/batch\n",
      "Global Step: 66500 Epoch 33/50 Iteration: 66500 Avg. Training loss: 2.9482 0.0150 sec/batch\n",
      "Global Step: 66600 Epoch 33/50 Iteration: 66600 Avg. Training loss: 2.9624 0.0092 sec/batch\n",
      "Global Step: 66700 Epoch 33/50 Iteration: 66700 Avg. Training loss: 2.9506 0.0126 sec/batch\n",
      "Global Step: 66800 Epoch 33/50 Iteration: 66800 Avg. Training loss: 2.9389 0.0128 sec/batch\n",
      "Global Step: 66900 Epoch 33/50 Iteration: 66900 Avg. Training loss: 2.9354 0.0153 sec/batch\n",
      "Global Step: 67000 Epoch 33/50 Iteration: 67000 Avg. Training loss: 2.9601 0.0133 sec/batch\n",
      "Global Step: 67100 Epoch 33/50 Iteration: 67100 Avg. Training loss: 2.9458 0.0147 sec/batch\n",
      "Global Step: 67200 Epoch 33/50 Iteration: 67200 Avg. Training loss: 2.9439 0.0134 sec/batch\n",
      "Global Step: 67300 Epoch 33/50 Iteration: 67300 Avg. Training loss: 2.9516 0.0148 sec/batch\n",
      "Global Step: 67400 Epoch 33/50 Iteration: 67400 Avg. Training loss: 2.9503 0.0122 sec/batch\n",
      "Global Step: 67500 Epoch 33/50 Iteration: 67500 Avg. Training loss: 2.9515 0.0070 sec/batch\n",
      "Global Step: 67600 Epoch 33/50 Iteration: 67600 Avg. Training loss: 2.9684 0.0058 sec/batch\n",
      "Global Step: 67700 Epoch 33/50 Iteration: 67700 Avg. Training loss: 2.9361 0.0074 sec/batch\n",
      "Global Step: 67800 Epoch 33/50 Iteration: 67800 Avg. Training loss: 2.9289 0.0050 sec/batch\n",
      "Global Step: 67900 Epoch 33/50 Iteration: 67900 Avg. Training loss: 2.9438 0.0062 sec/batch\n",
      "Global Step: 68000 Epoch 34/50 Iteration: 68000 Avg. Training loss: 2.9505 0.0013 sec/batch\n",
      "Global Step: 68100 Epoch 34/50 Iteration: 68100 Avg. Training loss: 2.9609 0.0070 sec/batch\n",
      "Global Step: 68200 Epoch 34/50 Iteration: 68200 Avg. Training loss: 2.9708 0.0077 sec/batch\n",
      "Global Step: 68300 Epoch 34/50 Iteration: 68300 Avg. Training loss: 2.9481 0.0070 sec/batch\n",
      "Global Step: 68400 Epoch 34/50 Iteration: 68400 Avg. Training loss: 2.9469 0.0067 sec/batch\n",
      "Global Step: 68500 Epoch 34/50 Iteration: 68500 Avg. Training loss: 2.9393 0.0065 sec/batch\n",
      "Global Step: 68600 Epoch 34/50 Iteration: 68600 Avg. Training loss: 2.9461 0.0064 sec/batch\n",
      "Global Step: 68700 Epoch 34/50 Iteration: 68700 Avg. Training loss: 2.9526 0.0075 sec/batch\n",
      "Global Step: 68800 Epoch 34/50 Iteration: 68800 Avg. Training loss: 2.9640 0.0079 sec/batch\n",
      "Global Step: 68900 Epoch 34/50 Iteration: 68900 Avg. Training loss: 2.9292 0.0071 sec/batch\n",
      "Global Step: 69000 Epoch 34/50 Iteration: 69000 Avg. Training loss: 2.9437 0.0085 sec/batch\n",
      "Global Step: 69100 Epoch 34/50 Iteration: 69100 Avg. Training loss: 2.9516 0.0130 sec/batch\n",
      "Global Step: 69200 Epoch 34/50 Iteration: 69200 Avg. Training loss: 2.9425 0.0134 sec/batch\n",
      "Global Step: 69300 Epoch 34/50 Iteration: 69300 Avg. Training loss: 2.9554 0.0138 sec/batch\n",
      "Global Step: 69400 Epoch 34/50 Iteration: 69400 Avg. Training loss: 2.9373 0.0129 sec/batch\n",
      "Global Step: 69500 Epoch 34/50 Iteration: 69500 Avg. Training loss: 2.9669 0.0138 sec/batch\n",
      "Global Step: 69600 Epoch 34/50 Iteration: 69600 Avg. Training loss: 2.9544 0.0145 sec/batch\n",
      "Global Step: 69700 Epoch 34/50 Iteration: 69700 Avg. Training loss: 2.9632 0.0130 sec/batch\n",
      "Global Step: 69800 Epoch 34/50 Iteration: 69800 Avg. Training loss: 2.9279 0.0121 sec/batch\n",
      "Global Step: 69900 Epoch 34/50 Iteration: 69900 Avg. Training loss: 2.9362 0.0134 sec/batch\n",
      "Global Step: 70000 Epoch 34/50 Iteration: 70000 Avg. Training loss: 2.9489 0.0126 sec/batch\n",
      "Global Step: 70100 Epoch 35/50 Iteration: 70100 Avg. Training loss: 2.9527 0.0073 sec/batch\n",
      "Global Step: 70200 Epoch 35/50 Iteration: 70200 Avg. Training loss: 2.9709 0.0140 sec/batch\n",
      "Global Step: 70300 Epoch 35/50 Iteration: 70300 Avg. Training loss: 2.9647 0.0133 sec/batch\n",
      "Global Step: 70400 Epoch 35/50 Iteration: 70400 Avg. Training loss: 2.9356 0.0128 sec/batch\n",
      "Global Step: 70500 Epoch 35/50 Iteration: 70500 Avg. Training loss: 2.9448 0.0131 sec/batch\n",
      "Global Step: 70600 Epoch 35/50 Iteration: 70600 Avg. Training loss: 2.9508 0.0121 sec/batch\n",
      "Global Step: 70700 Epoch 35/50 Iteration: 70700 Avg. Training loss: 2.9482 0.0136 sec/batch\n",
      "Global Step: 70800 Epoch 35/50 Iteration: 70800 Avg. Training loss: 2.9567 0.0153 sec/batch\n",
      "Global Step: 70900 Epoch 35/50 Iteration: 70900 Avg. Training loss: 2.9447 0.0144 sec/batch\n",
      "Global Step: 71000 Epoch 35/50 Iteration: 71000 Avg. Training loss: 2.9358 0.0148 sec/batch\n",
      "Global Step: 71100 Epoch 35/50 Iteration: 71100 Avg. Training loss: 2.9549 0.0158 sec/batch\n",
      "Global Step: 71200 Epoch 35/50 Iteration: 71200 Avg. Training loss: 2.9404 0.0145 sec/batch\n",
      "Global Step: 71300 Epoch 35/50 Iteration: 71300 Avg. Training loss: 2.9588 0.0124 sec/batch\n",
      "Global Step: 71400 Epoch 35/50 Iteration: 71400 Avg. Training loss: 2.9498 0.0156 sec/batch\n",
      "Global Step: 71500 Epoch 35/50 Iteration: 71500 Avg. Training loss: 2.9474 0.0141 sec/batch\n",
      "Global Step: 71600 Epoch 35/50 Iteration: 71600 Avg. Training loss: 2.9586 0.0155 sec/batch\n",
      "Global Step: 71700 Epoch 35/50 Iteration: 71700 Avg. Training loss: 2.9576 0.0153 sec/batch\n",
      "Global Step: 71800 Epoch 35/50 Iteration: 71800 Avg. Training loss: 2.9410 0.0143 sec/batch\n",
      "Global Step: 71900 Epoch 35/50 Iteration: 71900 Avg. Training loss: 2.9253 0.0155 sec/batch\n",
      "Global Step: 72000 Epoch 35/50 Iteration: 72000 Avg. Training loss: 2.9484 0.0147 sec/batch\n",
      "Global Step: 72100 Epoch 35/50 Iteration: 72100 Avg. Training loss: 2.9379 0.0116 sec/batch\n",
      "Global Step: 72200 Epoch 36/50 Iteration: 72200 Avg. Training loss: 2.9638 0.0131 sec/batch\n",
      "Global Step: 72300 Epoch 36/50 Iteration: 72300 Avg. Training loss: 2.9736 0.0142 sec/batch\n",
      "Global Step: 72400 Epoch 36/50 Iteration: 72400 Avg. Training loss: 2.9521 0.0145 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 72500 Epoch 36/50 Iteration: 72500 Avg. Training loss: 2.9423 0.0125 sec/batch\n",
      "Global Step: 72600 Epoch 36/50 Iteration: 72600 Avg. Training loss: 2.9416 0.0140 sec/batch\n",
      "Global Step: 72700 Epoch 36/50 Iteration: 72700 Avg. Training loss: 2.9425 0.0142 sec/batch\n",
      "Global Step: 72800 Epoch 36/50 Iteration: 72800 Avg. Training loss: 2.9609 0.0154 sec/batch\n",
      "Global Step: 72900 Epoch 36/50 Iteration: 72900 Avg. Training loss: 2.9589 0.0136 sec/batch\n",
      "Global Step: 73000 Epoch 36/50 Iteration: 73000 Avg. Training loss: 2.9371 0.0143 sec/batch\n",
      "Global Step: 73100 Epoch 36/50 Iteration: 73100 Avg. Training loss: 2.9431 0.0135 sec/batch\n",
      "Global Step: 73200 Epoch 36/50 Iteration: 73200 Avg. Training loss: 2.9419 0.0138 sec/batch\n",
      "Global Step: 73300 Epoch 36/50 Iteration: 73300 Avg. Training loss: 2.9541 0.0132 sec/batch\n",
      "Global Step: 73400 Epoch 36/50 Iteration: 73400 Avg. Training loss: 2.9496 0.0147 sec/batch\n",
      "Global Step: 73500 Epoch 36/50 Iteration: 73500 Avg. Training loss: 2.9383 0.0152 sec/batch\n",
      "Global Step: 73600 Epoch 36/50 Iteration: 73600 Avg. Training loss: 2.9677 0.0133 sec/batch\n",
      "Global Step: 73700 Epoch 36/50 Iteration: 73700 Avg. Training loss: 2.9525 0.0156 sec/batch\n",
      "Global Step: 73800 Epoch 36/50 Iteration: 73800 Avg. Training loss: 2.9638 0.0153 sec/batch\n",
      "Global Step: 73900 Epoch 36/50 Iteration: 73900 Avg. Training loss: 2.9303 0.0143 sec/batch\n",
      "Global Step: 74000 Epoch 36/50 Iteration: 74000 Avg. Training loss: 2.9340 0.0152 sec/batch\n",
      "Global Step: 74100 Epoch 36/50 Iteration: 74100 Avg. Training loss: 2.9396 0.0145 sec/batch\n",
      "Global Step: 74200 Epoch 37/50 Iteration: 74200 Avg. Training loss: 2.9549 0.0061 sec/batch\n",
      "Global Step: 74300 Epoch 37/50 Iteration: 74300 Avg. Training loss: 2.9663 0.0153 sec/batch\n",
      "Global Step: 74400 Epoch 37/50 Iteration: 74400 Avg. Training loss: 2.9686 0.0136 sec/batch\n",
      "Global Step: 74500 Epoch 37/50 Iteration: 74500 Avg. Training loss: 2.9376 0.0140 sec/batch\n",
      "Global Step: 74600 Epoch 37/50 Iteration: 74600 Avg. Training loss: 2.9429 0.0156 sec/batch\n",
      "Global Step: 74700 Epoch 37/50 Iteration: 74700 Avg. Training loss: 2.9422 0.0132 sec/batch\n",
      "Global Step: 74800 Epoch 37/50 Iteration: 74800 Avg. Training loss: 2.9543 0.0145 sec/batch\n",
      "Global Step: 74900 Epoch 37/50 Iteration: 74900 Avg. Training loss: 2.9505 0.0151 sec/batch\n",
      "Global Step: 75000 Epoch 37/50 Iteration: 75000 Avg. Training loss: 2.9513 0.0150 sec/batch\n",
      "Global Step: 75100 Epoch 37/50 Iteration: 75100 Avg. Training loss: 2.9384 0.0125 sec/batch\n",
      "Global Step: 75200 Epoch 37/50 Iteration: 75200 Avg. Training loss: 2.9488 0.0146 sec/batch\n",
      "Global Step: 75300 Epoch 37/50 Iteration: 75300 Avg. Training loss: 2.9444 0.0150 sec/batch\n",
      "Global Step: 75400 Epoch 37/50 Iteration: 75400 Avg. Training loss: 2.9560 0.0136 sec/batch\n",
      "Global Step: 75500 Epoch 37/50 Iteration: 75500 Avg. Training loss: 2.9486 0.0146 sec/batch\n",
      "Global Step: 75600 Epoch 37/50 Iteration: 75600 Avg. Training loss: 2.9405 0.0156 sec/batch\n",
      "Global Step: 75700 Epoch 37/50 Iteration: 75700 Avg. Training loss: 2.9686 0.0137 sec/batch\n",
      "Global Step: 75800 Epoch 37/50 Iteration: 75800 Avg. Training loss: 2.9511 0.0157 sec/batch\n",
      "Global Step: 75900 Epoch 37/50 Iteration: 75900 Avg. Training loss: 2.9563 0.0135 sec/batch\n",
      "Global Step: 76000 Epoch 37/50 Iteration: 76000 Avg. Training loss: 2.9276 0.0143 sec/batch\n",
      "Global Step: 76100 Epoch 37/50 Iteration: 76100 Avg. Training loss: 2.9431 0.0152 sec/batch\n",
      "Global Step: 76200 Epoch 37/50 Iteration: 76200 Avg. Training loss: 2.9410 0.0142 sec/batch\n",
      "Global Step: 76300 Epoch 38/50 Iteration: 76300 Avg. Training loss: 2.9480 0.0125 sec/batch\n",
      "Global Step: 76400 Epoch 38/50 Iteration: 76400 Avg. Training loss: 2.9841 0.0136 sec/batch\n",
      "Global Step: 76500 Epoch 38/50 Iteration: 76500 Avg. Training loss: 2.9546 0.0153 sec/batch\n",
      "Global Step: 76600 Epoch 38/50 Iteration: 76600 Avg. Training loss: 2.9420 0.0140 sec/batch\n",
      "Global Step: 76700 Epoch 38/50 Iteration: 76700 Avg. Training loss: 2.9356 0.0134 sec/batch\n",
      "Global Step: 76800 Epoch 38/50 Iteration: 76800 Avg. Training loss: 2.9485 0.0145 sec/batch\n",
      "Global Step: 76900 Epoch 38/50 Iteration: 76900 Avg. Training loss: 2.9642 0.0142 sec/batch\n",
      "Global Step: 77000 Epoch 38/50 Iteration: 77000 Avg. Training loss: 2.9518 0.0140 sec/batch\n",
      "Global Step: 77100 Epoch 38/50 Iteration: 77100 Avg. Training loss: 2.9373 0.0139 sec/batch\n",
      "Global Step: 77200 Epoch 38/50 Iteration: 77200 Avg. Training loss: 2.9345 0.0139 sec/batch\n",
      "Global Step: 77300 Epoch 38/50 Iteration: 77300 Avg. Training loss: 2.9612 0.0142 sec/batch\n",
      "Global Step: 77400 Epoch 38/50 Iteration: 77400 Avg. Training loss: 2.9466 0.0129 sec/batch\n",
      "Global Step: 77500 Epoch 38/50 Iteration: 77500 Avg. Training loss: 2.9440 0.0145 sec/batch\n",
      "Global Step: 77600 Epoch 38/50 Iteration: 77600 Avg. Training loss: 2.9516 0.0147 sec/batch\n",
      "Global Step: 77700 Epoch 38/50 Iteration: 77700 Avg. Training loss: 2.9509 0.0129 sec/batch\n",
      "Global Step: 77800 Epoch 38/50 Iteration: 77800 Avg. Training loss: 2.9522 0.0143 sec/batch\n",
      "Global Step: 77900 Epoch 38/50 Iteration: 77900 Avg. Training loss: 2.9678 0.0154 sec/batch\n",
      "Global Step: 78000 Epoch 38/50 Iteration: 78000 Avg. Training loss: 2.9358 0.0148 sec/batch\n",
      "Global Step: 78100 Epoch 38/50 Iteration: 78100 Avg. Training loss: 2.9288 0.0152 sec/batch\n",
      "Global Step: 78200 Epoch 38/50 Iteration: 78200 Avg. Training loss: 2.9429 0.0130 sec/batch\n",
      "Global Step: 78300 Epoch 39/50 Iteration: 78300 Avg. Training loss: 2.9519 0.0029 sec/batch\n",
      "Global Step: 78400 Epoch 39/50 Iteration: 78400 Avg. Training loss: 2.9608 0.0161 sec/batch\n",
      "Global Step: 78500 Epoch 39/50 Iteration: 78500 Avg. Training loss: 2.9713 0.0147 sec/batch\n",
      "Global Step: 78600 Epoch 39/50 Iteration: 78600 Avg. Training loss: 2.9488 0.0145 sec/batch\n",
      "Global Step: 78700 Epoch 39/50 Iteration: 78700 Avg. Training loss: 2.9464 0.0154 sec/batch\n",
      "Global Step: 78800 Epoch 39/50 Iteration: 78800 Avg. Training loss: 2.9387 0.0160 sec/batch\n",
      "Global Step: 78900 Epoch 39/50 Iteration: 78900 Avg. Training loss: 2.9459 0.0139 sec/batch\n",
      "Global Step: 79000 Epoch 39/50 Iteration: 79000 Avg. Training loss: 2.9526 0.0143 sec/batch\n",
      "Global Step: 79100 Epoch 39/50 Iteration: 79100 Avg. Training loss: 2.9636 0.0147 sec/batch\n",
      "Global Step: 79200 Epoch 39/50 Iteration: 79200 Avg. Training loss: 2.9294 0.0136 sec/batch\n",
      "Global Step: 79300 Epoch 39/50 Iteration: 79300 Avg. Training loss: 2.9439 0.0140 sec/batch\n",
      "Global Step: 79400 Epoch 39/50 Iteration: 79400 Avg. Training loss: 2.9539 0.0140 sec/batch\n",
      "Global Step: 79500 Epoch 39/50 Iteration: 79500 Avg. Training loss: 2.9432 0.0136 sec/batch\n",
      "Global Step: 79600 Epoch 39/50 Iteration: 79600 Avg. Training loss: 2.9555 0.0142 sec/batch\n",
      "Global Step: 79700 Epoch 39/50 Iteration: 79700 Avg. Training loss: 2.9367 0.0132 sec/batch\n",
      "Global Step: 79800 Epoch 39/50 Iteration: 79800 Avg. Training loss: 2.9676 0.0136 sec/batch\n",
      "Global Step: 79900 Epoch 39/50 Iteration: 79900 Avg. Training loss: 2.9533 0.0148 sec/batch\n",
      "Global Step: 80000 Epoch 39/50 Iteration: 80000 Avg. Training loss: 2.9631 0.0144 sec/batch\n",
      "Global Step: 80100 Epoch 39/50 Iteration: 80100 Avg. Training loss: 2.9283 0.0148 sec/batch\n",
      "Global Step: 80200 Epoch 39/50 Iteration: 80200 Avg. Training loss: 2.9365 0.0149 sec/batch\n",
      "Global Step: 80300 Epoch 39/50 Iteration: 80300 Avg. Training loss: 2.9491 0.0156 sec/batch\n",
      "Global Step: 80400 Epoch 40/50 Iteration: 80400 Avg. Training loss: 2.9526 0.0069 sec/batch\n",
      "Global Step: 80500 Epoch 40/50 Iteration: 80500 Avg. Training loss: 2.9683 0.0142 sec/batch\n",
      "Global Step: 80600 Epoch 40/50 Iteration: 80600 Avg. Training loss: 2.9636 0.0149 sec/batch\n",
      "Global Step: 80700 Epoch 40/50 Iteration: 80700 Avg. Training loss: 2.9363 0.0156 sec/batch\n",
      "Global Step: 80800 Epoch 40/50 Iteration: 80800 Avg. Training loss: 2.9432 0.0151 sec/batch\n",
      "Global Step: 80900 Epoch 40/50 Iteration: 80900 Avg. Training loss: 2.9505 0.0144 sec/batch\n",
      "Global Step: 81000 Epoch 40/50 Iteration: 81000 Avg. Training loss: 2.9485 0.0152 sec/batch\n",
      "Global Step: 81100 Epoch 40/50 Iteration: 81100 Avg. Training loss: 2.9540 0.0151 sec/batch\n",
      "Global Step: 81200 Epoch 40/50 Iteration: 81200 Avg. Training loss: 2.9443 0.0121 sec/batch\n",
      "Global Step: 81300 Epoch 40/50 Iteration: 81300 Avg. Training loss: 2.9363 0.0128 sec/batch\n",
      "Global Step: 81400 Epoch 40/50 Iteration: 81400 Avg. Training loss: 2.9560 0.0146 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 81500 Epoch 40/50 Iteration: 81500 Avg. Training loss: 2.9416 0.0149 sec/batch\n",
      "Global Step: 81600 Epoch 40/50 Iteration: 81600 Avg. Training loss: 2.9573 0.0135 sec/batch\n",
      "Global Step: 81700 Epoch 40/50 Iteration: 81700 Avg. Training loss: 2.9498 0.0146 sec/batch\n",
      "Global Step: 81800 Epoch 40/50 Iteration: 81800 Avg. Training loss: 2.9447 0.0140 sec/batch\n",
      "Global Step: 81900 Epoch 40/50 Iteration: 81900 Avg. Training loss: 2.9598 0.0137 sec/batch\n",
      "Global Step: 82000 Epoch 40/50 Iteration: 82000 Avg. Training loss: 2.9589 0.0120 sec/batch\n",
      "Global Step: 82100 Epoch 40/50 Iteration: 82100 Avg. Training loss: 2.9421 0.0155 sec/batch\n",
      "Global Step: 82200 Epoch 40/50 Iteration: 82200 Avg. Training loss: 2.9257 0.0139 sec/batch\n",
      "Global Step: 82300 Epoch 40/50 Iteration: 82300 Avg. Training loss: 2.9496 0.0133 sec/batch\n",
      "Global Step: 82400 Epoch 40/50 Iteration: 82400 Avg. Training loss: 2.9406 0.0137 sec/batch\n",
      "Global Step: 82500 Epoch 41/50 Iteration: 82500 Avg. Training loss: 2.9634 0.0145 sec/batch\n",
      "Global Step: 82600 Epoch 41/50 Iteration: 82600 Avg. Training loss: 2.9722 0.0149 sec/batch\n",
      "Global Step: 82700 Epoch 41/50 Iteration: 82700 Avg. Training loss: 2.9509 0.0144 sec/batch\n",
      "Global Step: 82800 Epoch 41/50 Iteration: 82800 Avg. Training loss: 2.9414 0.0157 sec/batch\n",
      "Global Step: 82900 Epoch 41/50 Iteration: 82900 Avg. Training loss: 2.9404 0.0145 sec/batch\n",
      "Global Step: 83000 Epoch 41/50 Iteration: 83000 Avg. Training loss: 2.9437 0.0162 sec/batch\n",
      "Global Step: 83100 Epoch 41/50 Iteration: 83100 Avg. Training loss: 2.9580 0.0149 sec/batch\n",
      "Global Step: 83200 Epoch 41/50 Iteration: 83200 Avg. Training loss: 2.9576 0.0145 sec/batch\n",
      "Global Step: 83300 Epoch 41/50 Iteration: 83300 Avg. Training loss: 2.9361 0.0164 sec/batch\n",
      "Global Step: 83400 Epoch 41/50 Iteration: 83400 Avg. Training loss: 2.9415 0.0141 sec/batch\n",
      "Global Step: 83500 Epoch 41/50 Iteration: 83500 Avg. Training loss: 2.9412 0.0136 sec/batch\n",
      "Global Step: 83600 Epoch 41/50 Iteration: 83600 Avg. Training loss: 2.9536 0.0141 sec/batch\n",
      "Global Step: 83700 Epoch 41/50 Iteration: 83700 Avg. Training loss: 2.9480 0.0161 sec/batch\n",
      "Global Step: 83800 Epoch 41/50 Iteration: 83800 Avg. Training loss: 2.9388 0.0134 sec/batch\n",
      "Global Step: 83900 Epoch 41/50 Iteration: 83900 Avg. Training loss: 2.9677 0.0140 sec/batch\n",
      "Global Step: 84000 Epoch 41/50 Iteration: 84000 Avg. Training loss: 2.9506 0.0142 sec/batch\n",
      "Global Step: 84100 Epoch 41/50 Iteration: 84100 Avg. Training loss: 2.9641 0.0132 sec/batch\n",
      "Global Step: 84200 Epoch 41/50 Iteration: 84200 Avg. Training loss: 2.9304 0.0136 sec/batch\n",
      "Global Step: 84300 Epoch 41/50 Iteration: 84300 Avg. Training loss: 2.9331 0.0068 sec/batch\n",
      "Global Step: 84400 Epoch 41/50 Iteration: 84400 Avg. Training loss: 2.9402 0.0059 sec/batch\n",
      "Global Step: 84500 Epoch 42/50 Iteration: 84500 Avg. Training loss: 2.9551 0.0035 sec/batch\n",
      "Global Step: 84600 Epoch 42/50 Iteration: 84600 Avg. Training loss: 2.9662 0.0071 sec/batch\n",
      "Global Step: 84700 Epoch 42/50 Iteration: 84700 Avg. Training loss: 2.9687 0.0069 sec/batch\n",
      "Global Step: 84800 Epoch 42/50 Iteration: 84800 Avg. Training loss: 2.9396 0.0049 sec/batch\n",
      "Global Step: 84900 Epoch 42/50 Iteration: 84900 Avg. Training loss: 2.9427 0.0056 sec/batch\n",
      "Global Step: 85000 Epoch 42/50 Iteration: 85000 Avg. Training loss: 2.9436 0.0057 sec/batch\n",
      "Global Step: 85100 Epoch 42/50 Iteration: 85100 Avg. Training loss: 2.9539 0.0070 sec/batch\n",
      "Global Step: 85200 Epoch 42/50 Iteration: 85200 Avg. Training loss: 2.9512 0.0079 sec/batch\n",
      "Global Step: 85300 Epoch 42/50 Iteration: 85300 Avg. Training loss: 2.9513 0.0062 sec/batch\n",
      "Global Step: 85400 Epoch 42/50 Iteration: 85400 Avg. Training loss: 2.9388 0.0061 sec/batch\n",
      "Global Step: 85500 Epoch 42/50 Iteration: 85500 Avg. Training loss: 2.9475 0.0074 sec/batch\n",
      "Global Step: 85600 Epoch 42/50 Iteration: 85600 Avg. Training loss: 2.9435 0.0128 sec/batch\n",
      "Global Step: 85700 Epoch 42/50 Iteration: 85700 Avg. Training loss: 2.9546 0.0138 sec/batch\n",
      "Global Step: 85800 Epoch 42/50 Iteration: 85800 Avg. Training loss: 2.9491 0.0140 sec/batch\n",
      "Global Step: 85900 Epoch 42/50 Iteration: 85900 Avg. Training loss: 2.9394 0.0131 sec/batch\n",
      "Global Step: 86000 Epoch 42/50 Iteration: 86000 Avg. Training loss: 2.9688 0.0137 sec/batch\n",
      "Global Step: 86100 Epoch 42/50 Iteration: 86100 Avg. Training loss: 2.9511 0.0147 sec/batch\n",
      "Global Step: 86200 Epoch 42/50 Iteration: 86200 Avg. Training loss: 2.9561 0.0134 sec/batch\n",
      "Global Step: 86300 Epoch 42/50 Iteration: 86300 Avg. Training loss: 2.9270 0.0120 sec/batch\n",
      "Global Step: 86400 Epoch 42/50 Iteration: 86400 Avg. Training loss: 2.9428 0.0130 sec/batch\n",
      "Global Step: 86500 Epoch 42/50 Iteration: 86500 Avg. Training loss: 2.9438 0.0129 sec/batch\n",
      "Global Step: 86600 Epoch 43/50 Iteration: 86600 Avg. Training loss: 2.9490 0.0107 sec/batch\n",
      "Global Step: 86700 Epoch 43/50 Iteration: 86700 Avg. Training loss: 2.9841 0.0115 sec/batch\n",
      "Global Step: 86800 Epoch 43/50 Iteration: 86800 Avg. Training loss: 2.9536 0.0124 sec/batch\n",
      "Global Step: 86900 Epoch 43/50 Iteration: 86900 Avg. Training loss: 2.9415 0.0136 sec/batch\n",
      "Global Step: 87000 Epoch 43/50 Iteration: 87000 Avg. Training loss: 2.9347 0.0144 sec/batch\n",
      "Global Step: 87100 Epoch 43/50 Iteration: 87100 Avg. Training loss: 2.9492 0.0122 sec/batch\n",
      "Global Step: 87200 Epoch 43/50 Iteration: 87200 Avg. Training loss: 2.9623 0.0141 sec/batch\n",
      "Global Step: 87300 Epoch 43/50 Iteration: 87300 Avg. Training loss: 2.9509 0.0139 sec/batch\n",
      "Global Step: 87400 Epoch 43/50 Iteration: 87400 Avg. Training loss: 2.9383 0.0142 sec/batch\n",
      "Global Step: 87500 Epoch 43/50 Iteration: 87500 Avg. Training loss: 2.9348 0.0143 sec/batch\n",
      "Global Step: 87600 Epoch 43/50 Iteration: 87600 Avg. Training loss: 2.9610 0.0143 sec/batch\n",
      "Global Step: 87700 Epoch 43/50 Iteration: 87700 Avg. Training loss: 2.9474 0.0135 sec/batch\n",
      "Global Step: 87800 Epoch 43/50 Iteration: 87800 Avg. Training loss: 2.9432 0.0137 sec/batch\n",
      "Global Step: 87900 Epoch 43/50 Iteration: 87900 Avg. Training loss: 2.9508 0.0136 sec/batch\n",
      "Global Step: 88000 Epoch 43/50 Iteration: 88000 Avg. Training loss: 2.9503 0.0147 sec/batch\n",
      "Global Step: 88100 Epoch 43/50 Iteration: 88100 Avg. Training loss: 2.9523 0.0170 sec/batch\n",
      "Global Step: 88200 Epoch 43/50 Iteration: 88200 Avg. Training loss: 2.9688 0.0144 sec/batch\n",
      "Global Step: 88300 Epoch 43/50 Iteration: 88300 Avg. Training loss: 2.9371 0.0144 sec/batch\n",
      "Global Step: 88400 Epoch 43/50 Iteration: 88400 Avg. Training loss: 2.9288 0.0165 sec/batch\n",
      "Global Step: 88500 Epoch 43/50 Iteration: 88500 Avg. Training loss: 2.9424 0.0147 sec/batch\n",
      "Global Step: 88600 Epoch 44/50 Iteration: 88600 Avg. Training loss: 2.9498 0.0025 sec/batch\n",
      "Global Step: 88700 Epoch 44/50 Iteration: 88700 Avg. Training loss: 2.9610 0.0150 sec/batch\n",
      "Global Step: 88800 Epoch 44/50 Iteration: 88800 Avg. Training loss: 2.9711 0.0150 sec/batch\n",
      "Global Step: 88900 Epoch 44/50 Iteration: 88900 Avg. Training loss: 2.9504 0.0173 sec/batch\n",
      "Global Step: 89000 Epoch 44/50 Iteration: 89000 Avg. Training loss: 2.9462 0.0137 sec/batch\n",
      "Global Step: 89100 Epoch 44/50 Iteration: 89100 Avg. Training loss: 2.9382 0.0147 sec/batch\n",
      "Global Step: 89200 Epoch 44/50 Iteration: 89200 Avg. Training loss: 2.9471 0.0145 sec/batch\n",
      "Global Step: 89300 Epoch 44/50 Iteration: 89300 Avg. Training loss: 2.9522 0.0155 sec/batch\n",
      "Global Step: 89400 Epoch 44/50 Iteration: 89400 Avg. Training loss: 2.9632 0.0124 sec/batch\n",
      "Global Step: 89500 Epoch 44/50 Iteration: 89500 Avg. Training loss: 2.9287 0.0137 sec/batch\n",
      "Global Step: 89600 Epoch 44/50 Iteration: 89600 Avg. Training loss: 2.9439 0.0149 sec/batch\n",
      "Global Step: 89700 Epoch 44/50 Iteration: 89700 Avg. Training loss: 2.9538 0.0142 sec/batch\n",
      "Global Step: 89800 Epoch 44/50 Iteration: 89800 Avg. Training loss: 2.9429 0.0136 sec/batch\n",
      "Global Step: 89900 Epoch 44/50 Iteration: 89900 Avg. Training loss: 2.9557 0.0144 sec/batch\n",
      "Global Step: 90000 Epoch 44/50 Iteration: 90000 Avg. Training loss: 2.9365 0.0146 sec/batch\n",
      "Global Step: 90100 Epoch 44/50 Iteration: 90100 Avg. Training loss: 2.9676 0.0135 sec/batch\n",
      "Global Step: 90200 Epoch 44/50 Iteration: 90200 Avg. Training loss: 2.9532 0.0153 sec/batch\n",
      "Global Step: 90300 Epoch 44/50 Iteration: 90300 Avg. Training loss: 2.9648 0.0155 sec/batch\n",
      "Global Step: 90400 Epoch 44/50 Iteration: 90400 Avg. Training loss: 2.9282 0.0144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 90500 Epoch 44/50 Iteration: 90500 Avg. Training loss: 2.9355 0.0152 sec/batch\n",
      "Global Step: 90600 Epoch 44/50 Iteration: 90600 Avg. Training loss: 2.9490 0.0161 sec/batch\n",
      "Global Step: 90700 Epoch 45/50 Iteration: 90700 Avg. Training loss: 2.9522 0.0087 sec/batch\n",
      "Global Step: 90800 Epoch 45/50 Iteration: 90800 Avg. Training loss: 2.9700 0.0148 sec/batch\n",
      "Global Step: 90900 Epoch 45/50 Iteration: 90900 Avg. Training loss: 2.9631 0.0143 sec/batch\n",
      "Global Step: 91000 Epoch 45/50 Iteration: 91000 Avg. Training loss: 2.9349 0.0148 sec/batch\n",
      "Global Step: 91100 Epoch 45/50 Iteration: 91100 Avg. Training loss: 2.9423 0.0139 sec/batch\n",
      "Global Step: 91200 Epoch 45/50 Iteration: 91200 Avg. Training loss: 2.9494 0.0130 sec/batch\n",
      "Global Step: 91300 Epoch 45/50 Iteration: 91300 Avg. Training loss: 2.9468 0.0125 sec/batch\n",
      "Global Step: 91400 Epoch 45/50 Iteration: 91400 Avg. Training loss: 2.9535 0.0148 sec/batch\n",
      "Global Step: 91500 Epoch 45/50 Iteration: 91500 Avg. Training loss: 2.9435 0.0147 sec/batch\n",
      "Global Step: 91600 Epoch 45/50 Iteration: 91600 Avg. Training loss: 2.9362 0.0115 sec/batch\n",
      "Global Step: 91700 Epoch 45/50 Iteration: 91700 Avg. Training loss: 2.9554 0.0113 sec/batch\n",
      "Global Step: 91800 Epoch 45/50 Iteration: 91800 Avg. Training loss: 2.9407 0.0145 sec/batch\n",
      "Global Step: 91900 Epoch 45/50 Iteration: 91900 Avg. Training loss: 2.9588 0.0134 sec/batch\n",
      "Global Step: 92000 Epoch 45/50 Iteration: 92000 Avg. Training loss: 2.9485 0.0130 sec/batch\n",
      "Global Step: 92100 Epoch 45/50 Iteration: 92100 Avg. Training loss: 2.9457 0.0152 sec/batch\n",
      "Global Step: 92200 Epoch 45/50 Iteration: 92200 Avg. Training loss: 2.9606 0.0155 sec/batch\n",
      "Global Step: 92300 Epoch 45/50 Iteration: 92300 Avg. Training loss: 2.9571 0.0136 sec/batch\n",
      "Global Step: 92400 Epoch 45/50 Iteration: 92400 Avg. Training loss: 2.9406 0.0135 sec/batch\n",
      "Global Step: 92500 Epoch 45/50 Iteration: 92500 Avg. Training loss: 2.9248 0.0135 sec/batch\n",
      "Global Step: 92600 Epoch 45/50 Iteration: 92600 Avg. Training loss: 2.9501 0.0147 sec/batch\n",
      "Global Step: 92700 Epoch 45/50 Iteration: 92700 Avg. Training loss: 2.9402 0.0141 sec/batch\n",
      "Global Step: 92800 Epoch 46/50 Iteration: 92800 Avg. Training loss: 2.9630 0.0148 sec/batch\n",
      "Global Step: 92900 Epoch 46/50 Iteration: 92900 Avg. Training loss: 2.9728 0.0139 sec/batch\n",
      "Global Step: 93000 Epoch 46/50 Iteration: 93000 Avg. Training loss: 2.9528 0.0154 sec/batch\n",
      "Global Step: 93100 Epoch 46/50 Iteration: 93100 Avg. Training loss: 2.9425 0.0159 sec/batch\n",
      "Global Step: 93200 Epoch 46/50 Iteration: 93200 Avg. Training loss: 2.9411 0.0128 sec/batch\n",
      "Global Step: 93300 Epoch 46/50 Iteration: 93300 Avg. Training loss: 2.9422 0.0144 sec/batch\n",
      "Global Step: 93400 Epoch 46/50 Iteration: 93400 Avg. Training loss: 2.9584 0.0138 sec/batch\n",
      "Global Step: 93500 Epoch 46/50 Iteration: 93500 Avg. Training loss: 2.9603 0.0139 sec/batch\n",
      "Global Step: 93600 Epoch 46/50 Iteration: 93600 Avg. Training loss: 2.9361 0.0156 sec/batch\n",
      "Global Step: 93700 Epoch 46/50 Iteration: 93700 Avg. Training loss: 2.9430 0.0149 sec/batch\n",
      "Global Step: 93800 Epoch 46/50 Iteration: 93800 Avg. Training loss: 2.9427 0.0133 sec/batch\n",
      "Global Step: 93900 Epoch 46/50 Iteration: 93900 Avg. Training loss: 2.9535 0.0147 sec/batch\n",
      "Global Step: 94000 Epoch 46/50 Iteration: 94000 Avg. Training loss: 2.9487 0.0137 sec/batch\n",
      "Global Step: 94100 Epoch 46/50 Iteration: 94100 Avg. Training loss: 2.9390 0.0149 sec/batch\n",
      "Global Step: 94200 Epoch 46/50 Iteration: 94200 Avg. Training loss: 2.9676 0.0161 sec/batch\n",
      "Global Step: 94300 Epoch 46/50 Iteration: 94300 Avg. Training loss: 2.9507 0.0148 sec/batch\n",
      "Global Step: 94400 Epoch 46/50 Iteration: 94400 Avg. Training loss: 2.9630 0.0138 sec/batch\n",
      "Global Step: 94500 Epoch 46/50 Iteration: 94500 Avg. Training loss: 2.9306 0.0154 sec/batch\n",
      "Global Step: 94600 Epoch 46/50 Iteration: 94600 Avg. Training loss: 2.9338 0.0147 sec/batch\n",
      "Global Step: 94700 Epoch 46/50 Iteration: 94700 Avg. Training loss: 2.9398 0.0137 sec/batch\n",
      "Global Step: 94800 Epoch 47/50 Iteration: 94800 Avg. Training loss: 2.9533 0.0059 sec/batch\n",
      "Global Step: 94900 Epoch 47/50 Iteration: 94900 Avg. Training loss: 2.9669 0.0137 sec/batch\n",
      "Global Step: 95000 Epoch 47/50 Iteration: 95000 Avg. Training loss: 2.9684 0.0156 sec/batch\n",
      "Global Step: 95100 Epoch 47/50 Iteration: 95100 Avg. Training loss: 2.9388 0.0152 sec/batch\n",
      "Global Step: 95200 Epoch 47/50 Iteration: 95200 Avg. Training loss: 2.9419 0.0146 sec/batch\n",
      "Global Step: 95300 Epoch 47/50 Iteration: 95300 Avg. Training loss: 2.9439 0.0151 sec/batch\n",
      "Global Step: 95400 Epoch 47/50 Iteration: 95400 Avg. Training loss: 2.9541 0.0148 sec/batch\n",
      "Global Step: 95500 Epoch 47/50 Iteration: 95500 Avg. Training loss: 2.9510 0.0133 sec/batch\n",
      "Global Step: 95600 Epoch 47/50 Iteration: 95600 Avg. Training loss: 2.9521 0.0151 sec/batch\n",
      "Global Step: 95700 Epoch 47/50 Iteration: 95700 Avg. Training loss: 2.9388 0.0147 sec/batch\n",
      "Global Step: 95800 Epoch 47/50 Iteration: 95800 Avg. Training loss: 2.9481 0.0139 sec/batch\n",
      "Global Step: 95900 Epoch 47/50 Iteration: 95900 Avg. Training loss: 2.9429 0.0145 sec/batch\n",
      "Global Step: 96000 Epoch 47/50 Iteration: 96000 Avg. Training loss: 2.9548 0.0131 sec/batch\n",
      "Global Step: 96100 Epoch 47/50 Iteration: 96100 Avg. Training loss: 2.9492 0.0144 sec/batch\n",
      "Global Step: 96200 Epoch 47/50 Iteration: 96200 Avg. Training loss: 2.9400 0.0132 sec/batch\n",
      "Global Step: 96300 Epoch 47/50 Iteration: 96300 Avg. Training loss: 2.9690 0.0137 sec/batch\n",
      "Global Step: 96400 Epoch 47/50 Iteration: 96400 Avg. Training loss: 2.9513 0.0144 sec/batch\n",
      "Global Step: 96500 Epoch 47/50 Iteration: 96500 Avg. Training loss: 2.9559 0.0142 sec/batch\n",
      "Global Step: 96600 Epoch 47/50 Iteration: 96600 Avg. Training loss: 2.9282 0.0131 sec/batch\n",
      "Global Step: 96700 Epoch 47/50 Iteration: 96700 Avg. Training loss: 2.9424 0.0140 sec/batch\n",
      "Global Step: 96800 Epoch 47/50 Iteration: 96800 Avg. Training loss: 2.9420 0.0149 sec/batch\n",
      "Global Step: 96900 Epoch 48/50 Iteration: 96900 Avg. Training loss: 2.9483 0.0106 sec/batch\n",
      "Global Step: 97000 Epoch 48/50 Iteration: 97000 Avg. Training loss: 2.9812 0.0119 sec/batch\n",
      "Global Step: 97100 Epoch 48/50 Iteration: 97100 Avg. Training loss: 2.9535 0.0149 sec/batch\n",
      "Global Step: 97200 Epoch 48/50 Iteration: 97200 Avg. Training loss: 2.9410 0.0139 sec/batch\n",
      "Global Step: 97300 Epoch 48/50 Iteration: 97300 Avg. Training loss: 2.9359 0.0136 sec/batch\n",
      "Global Step: 97400 Epoch 48/50 Iteration: 97400 Avg. Training loss: 2.9497 0.0155 sec/batch\n",
      "Global Step: 97500 Epoch 48/50 Iteration: 97500 Avg. Training loss: 2.9630 0.0145 sec/batch\n",
      "Global Step: 97600 Epoch 48/50 Iteration: 97600 Avg. Training loss: 2.9529 0.0152 sec/batch\n",
      "Global Step: 97700 Epoch 48/50 Iteration: 97700 Avg. Training loss: 2.9379 0.0164 sec/batch\n",
      "Global Step: 97800 Epoch 48/50 Iteration: 97800 Avg. Training loss: 2.9344 0.0123 sec/batch\n",
      "Global Step: 97900 Epoch 48/50 Iteration: 97900 Avg. Training loss: 2.9594 0.0135 sec/batch\n",
      "Global Step: 98000 Epoch 48/50 Iteration: 98000 Avg. Training loss: 2.9458 0.0148 sec/batch\n",
      "Global Step: 98100 Epoch 48/50 Iteration: 98100 Avg. Training loss: 2.9431 0.0149 sec/batch\n",
      "Global Step: 98200 Epoch 48/50 Iteration: 98200 Avg. Training loss: 2.9521 0.0151 sec/batch\n",
      "Global Step: 98300 Epoch 48/50 Iteration: 98300 Avg. Training loss: 2.9487 0.0147 sec/batch\n",
      "Global Step: 98400 Epoch 48/50 Iteration: 98400 Avg. Training loss: 2.9534 0.0144 sec/batch\n",
      "Global Step: 98500 Epoch 48/50 Iteration: 98500 Avg. Training loss: 2.9680 0.0131 sec/batch\n",
      "Global Step: 98600 Epoch 48/50 Iteration: 98600 Avg. Training loss: 2.9352 0.0136 sec/batch\n",
      "Global Step: 98700 Epoch 48/50 Iteration: 98700 Avg. Training loss: 2.9278 0.0135 sec/batch\n",
      "Global Step: 98800 Epoch 48/50 Iteration: 98800 Avg. Training loss: 2.9429 0.0149 sec/batch\n",
      "Global Step: 98900 Epoch 49/50 Iteration: 98900 Avg. Training loss: 2.9517 0.0028 sec/batch\n",
      "Global Step: 99000 Epoch 49/50 Iteration: 99000 Avg. Training loss: 2.9607 0.0136 sec/batch\n",
      "Global Step: 99100 Epoch 49/50 Iteration: 99100 Avg. Training loss: 2.9719 0.0140 sec/batch\n",
      "Global Step: 99200 Epoch 49/50 Iteration: 99200 Avg. Training loss: 2.9493 0.0151 sec/batch\n",
      "Global Step: 99300 Epoch 49/50 Iteration: 99300 Avg. Training loss: 2.9459 0.0125 sec/batch\n",
      "Global Step: 99400 Epoch 49/50 Iteration: 99400 Avg. Training loss: 2.9388 0.0146 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 99500 Epoch 49/50 Iteration: 99500 Avg. Training loss: 2.9469 0.0152 sec/batch\n",
      "Global Step: 99600 Epoch 49/50 Iteration: 99600 Avg. Training loss: 2.9533 0.0142 sec/batch\n",
      "Global Step: 99700 Epoch 49/50 Iteration: 99700 Avg. Training loss: 2.9648 0.0150 sec/batch\n",
      "Global Step: 99800 Epoch 49/50 Iteration: 99800 Avg. Training loss: 2.9302 0.0143 sec/batch\n",
      "Global Step: 99900 Epoch 49/50 Iteration: 99900 Avg. Training loss: 2.9436 0.0155 sec/batch\n",
      "Global Step: 100000 Epoch 49/50 Iteration: 100000 Avg. Training loss: 2.9532 0.0145 sec/batch\n",
      "Global Step: 100100 Epoch 49/50 Iteration: 100100 Avg. Training loss: 2.9426 0.0138 sec/batch\n",
      "Global Step: 100200 Epoch 49/50 Iteration: 100200 Avg. Training loss: 2.9550 0.0164 sec/batch\n",
      "Global Step: 100300 Epoch 49/50 Iteration: 100300 Avg. Training loss: 2.9362 0.0141 sec/batch\n",
      "Global Step: 100400 Epoch 49/50 Iteration: 100400 Avg. Training loss: 2.9667 0.0135 sec/batch\n",
      "Global Step: 100500 Epoch 49/50 Iteration: 100500 Avg. Training loss: 2.9536 0.0151 sec/batch\n",
      "Global Step: 100600 Epoch 49/50 Iteration: 100600 Avg. Training loss: 2.9624 0.0144 sec/batch\n",
      "Global Step: 100700 Epoch 49/50 Iteration: 100700 Avg. Training loss: 2.9270 0.0140 sec/batch\n",
      "Global Step: 100800 Epoch 49/50 Iteration: 100800 Avg. Training loss: 2.9361 0.0129 sec/batch\n",
      "Global Step: 100900 Epoch 49/50 Iteration: 100900 Avg. Training loss: 2.9479 0.0064 sec/batch\n",
      "Global Step: 101000 Epoch 50/50 Iteration: 101000 Avg. Training loss: 2.9530 0.0046 sec/batch\n",
      "Global Step: 101100 Epoch 50/50 Iteration: 101100 Avg. Training loss: 2.9684 0.0077 sec/batch\n",
      "Global Step: 101200 Epoch 50/50 Iteration: 101200 Avg. Training loss: 2.9644 0.0062 sec/batch\n",
      "Global Step: 101300 Epoch 50/50 Iteration: 101300 Avg. Training loss: 2.9352 0.0054 sec/batch\n",
      "Global Step: 101400 Epoch 50/50 Iteration: 101400 Avg. Training loss: 2.9433 0.0053 sec/batch\n",
      "Global Step: 101500 Epoch 50/50 Iteration: 101500 Avg. Training loss: 2.9517 0.0052 sec/batch\n",
      "Global Step: 101600 Epoch 50/50 Iteration: 101600 Avg. Training loss: 2.9491 0.0059 sec/batch\n",
      "Global Step: 101700 Epoch 50/50 Iteration: 101700 Avg. Training loss: 2.9545 0.0069 sec/batch\n",
      "Global Step: 101800 Epoch 50/50 Iteration: 101800 Avg. Training loss: 2.9429 0.0061 sec/batch\n",
      "Global Step: 101900 Epoch 50/50 Iteration: 101900 Avg. Training loss: 2.9355 0.0073 sec/batch\n",
      "Global Step: 102000 Epoch 50/50 Iteration: 102000 Avg. Training loss: 2.9556 0.0068 sec/batch\n",
      "Global Step: 102100 Epoch 50/50 Iteration: 102100 Avg. Training loss: 2.9415 0.0061 sec/batch\n",
      "Global Step: 102200 Epoch 50/50 Iteration: 102200 Avg. Training loss: 2.9569 0.0100 sec/batch\n",
      "Global Step: 102300 Epoch 50/50 Iteration: 102300 Avg. Training loss: 2.9473 0.0144 sec/batch\n",
      "Global Step: 102400 Epoch 50/50 Iteration: 102400 Avg. Training loss: 2.9443 0.0133 sec/batch\n",
      "Global Step: 102500 Epoch 50/50 Iteration: 102500 Avg. Training loss: 2.9603 0.0152 sec/batch\n",
      "Global Step: 102600 Epoch 50/50 Iteration: 102600 Avg. Training loss: 2.9578 0.0141 sec/batch\n",
      "Global Step: 102700 Epoch 50/50 Iteration: 102700 Avg. Training loss: 2.9408 0.0135 sec/batch\n",
      "Global Step: 102800 Epoch 50/50 Iteration: 102800 Avg. Training loss: 2.9249 0.0144 sec/batch\n",
      "Global Step: 102900 Epoch 50/50 Iteration: 102900 Avg. Training loss: 2.9498 0.0124 sec/batch\n",
      "Global Step: 103000 Epoch 50/50 Iteration: 103000 Avg. Training loss: 2.9388 0.0138 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "#             if iteration % 1000 == 0:\n",
    "#                 ## From Thushan Ganegedara's implementation\n",
    "#                 # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#                 sim = similarity.eval()\n",
    "#                 for i in range(valid_size):\n",
    "#                     valid_word = int_to_vocab[valid_examples[i]]\n",
    "#                     top_k = 8 # number of nearest neighbors\n",
    "#                     nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "#                     log = 'Nearest to %s:' % valid_word\n",
    "#                     for k in range(top_k):\n",
    "#                         close_word = int_to_vocab[nearest[k]]\n",
    "#                         log = '%s %s,' % (log, close_word)\n",
    "#                     print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/pos/pos.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/pos/pos.ckpt\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/pos'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB14AAAcMCAYAAAAHCCGfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3X+YV3WB9//XGX4IQoAKgbdZKv6iWZVEckVgTJJq27a0\nW8uiQEnXMq32Nq/6pojY3bVbXdv13dX6KuGiua6ZGl7uXio6Gyiaq4LrGlFqBWY6CCKOCAoyn+8f\nw0z8mIFhzgzDjI/HdX2u+XDO+33O+/iPwJNzTlGpVAIAAAAAAABA+1V19QIAAAAAAAAAujvhFQAA\nAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAAoCThFQAAAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAAoCTh\nFQAAAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAAoCThFQAAAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAA\noCThFQAAAAAAAKAk4RUAAAAAAACgpN5dvYCeoiiKPyQZlGR5Fy8FAAAAAAAAaLtDktRXKpVDyxxE\neO04g/r377//qFGj9u/qhQAAAAAAAABts2zZsmzYsKH0cYTXjrN81KhR+y9evLir1wEAAAAAAAC0\n0ZgxY7JkyZLlZY/jHa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAA\nAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmv\nAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAA\nJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAA\nAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8A\nAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAl\nCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAA\nAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAA\nAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJ\nrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAAACUJrwAAAAAAAAAlCa8AAAAAAAAAJQmvAAAAAAAA\nACUJrwAAAAAAAAAlCa8AAAAAW0ybNi1FUeS9731vm+dcc801KYoi/fr1y9q1a7NgwYIURdHiZ8CA\nARk1alQuuOCCLFu2rNVjnnLKKc1zzjjjjJ2ef/z48SmKIjNnzmzzmgEAgI4nvAIAAABsMXXq1CTJ\nsmXL8vjjj7dpzo033pgk+fjHP54hQ4Zss2/o0KEZPnx4hg8fnmHDhuWNN97Ib37zm1x77bU57rjj\ncvvtt+/y+D//+c+zePHi3bwSAABgTxNeAQAAALY45ZRT8p73vCfJn4Pqzvz2t7/No48+muTP0XZr\njz32WOrq6lJXV5eXXnopb775Zmpra3PkkUdm06ZNmT59el577bVdnueyyy7bzSsBAAD2NOEVAAAA\nYIuiKPK5z30uSXLLLbfkrbfe2un4pjg7YsSIfOhDH9rl8Xv37p1TTz01//Iv/5IkefXVV/Pggw+2\nOv7DH/5wiqLIPffck0WLFrX1MgAAgC4gvAIAAABs5fOf/3ySZNWqVbn77rtbHVepVHLTTTclST77\n2c+mV69ebT7Hscce2/z99ddfb3XccccdlzPPPDOJu14BAGBvJ7wCAAAAbOWII47IuHHjkuz8ccML\nFizIc889l6TlxwzvzFNPPdX8/fDDD9/p2CuvvDK9evXKwoULc9999+3WeQAAgD1HeAUAAADYTlNI\nveuuu7J27doWxzRF2fe973055phj2nTczZs3Z+HChTnnnHOSJDU1NXnf+9630zlHH310pkyZksRd\nrwAAsDcTXgEAAAC2c9ZZZ6Vfv3558803c+utt+6wf/369bn99tuT7Pxu17Fjx2bEiBEZMWJE3vnO\nd2afffbJKaeckpdffjlf/vKXc9ddd7VpPVdccUX69OmTRx99NHfeeWf7LgoAAOhUwisAAADAdoYM\nGZKPf/zjSVp+3PDPf/7zvPbaa+ndu3c+85nPtHqc1atXZ+XKlVm5cmVWrVqVzZs3J0nWrVuXtWvX\n5rXXXmvTeg499NBMnz49SXL55ZenUqns7iUBAACdTHgFAAAAaMG0adOSJA899FB+//vfb7OvKcZ+\n5CMfybBhw1o9xh/+8IdUKpXmz0svvZT//M//zJgxY3LTTTdl3Lhxef7559u0nssuuyz9+vXLU089\nlVtuuaV9FwUAAHQa4RUAAACgBaeddloOPPDAJMlPfvKT5u0vvvhiamtrk+z8McMtGTZsWD7wgQ/k\nvvvuy2GHHZYVK1Zk5syZbZp70EEH5Ytf/GKSZObMmc13zwIAAHsH4RUAAACgBb169cqUKVOSbBte\nb7rppmzevDn7779/Pvaxj7Xr2P37989ZZ52VJC2+Q7Y13/zmNzNgwIA8/fTTueGGG9p1bgAAoHMI\nrwAAAACtaLqj9Xe/+10efvjhJH+OsJ/+9KfTt2/fdh/73e9+d5Lktddey+rVq9s0Z9iwYfnKV76S\nJJk1a1Y2btzY7vMDAAAdS3gFAAAAaEV1dXXGjBmTpPG9rk888USeeuqpJLv/mOHt/elPf2r+3qdP\nnzbPu+SSSzJ48OCsWLEi1113Xak1AAAAHUd4BQAAANiJpsB66623Zvbs2UmSo48+Ou9///vbfcxN\nmzZl3rx5SZLDDjssgwcPbvPc/fbbL5dcckmS5Dvf+U42bNjQ7nUAAAAdR3gFAAAA2Imzzz47ffr0\nySuvvJJrr702Sfvvdm1oaMiyZcty5plnZunSpUmSiy66aLeP89WvfjVDhw7Niy++mCVLlrRrLQAA\nQMcSXgEAAAB2YujQofnoRz+apDGcVlVVZcqUKW2aO3bs2IwYMaL5079//7z3ve/NnXfemSQ555xz\ncvHFF+/2mgYOHJhvfOMbuz0PAADoPL27egEAAAAAnW3p0qS2NqmvTwYNSiZNSqqr2z5/6tSpzY8G\nPvXUU/Oud72rTfNWr169za/79u2bgw8+OCeeeGLOPffcfOQjH2n7IrbzpS99Kf/4j/+YF154od3H\nAAAAOo7wCgAAAPRYtbXJrFnJAw/suG/ixGTGjMYIuyuf+MQnUqlU2nTOU045pc1jW7NgwYJdjunf\nv3/+9Kc/lToPAADQcTxqGAAAAOiR5sxJJk9uObomjdsnT06uv37PrgsAAOiZhFcAAACgx6mtTc4/\nP2lo2Pm4hobkvPMaxwMAAJQhvAIAAAA9zqxZu46uTRoakquu6tz1AAAAPZ/wCgAAAPQoS5e2/njh\n1ixc2DgPAACgvYRXAAAAoEdp72ODPW4YAAAoQ3gFAAAAepT6+j07DwAAIBFeAQAAgB5m0KA9Ow8A\nACARXgEAAIAeZtKkPTsPAAAgEV4BAACAHqa6Opk4cffm1NQ0zgMAAGgv4RUAAADocWbMSKra+Lce\nVVXJ5Zd37noAAICeT3gFAAAAepxJk5Lrrtt1fK2qSmbP9phhAACgPOEVAAAA6JGmT0/mz298jHBL\namoa95977p5dFwAA0DP17uoFAAAAAHSWSZMaP0uXJrW1SX19MmhQ4zbvdAUAADqS8AoAAAD0eNXV\nQisAANC5PGoYAAAAAAAAoCThFQAAAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAAoCThFQAAAAAAAKAk\n4RUAAAAAAACgJOEVAAAAAAAAoCThFQAAAAAAAKAk4RUAAAAAAACgJOEVAAAAAAAAoCThFQAAAAAA\nAKAk4RUAAAAAAACgpL0ivBZF8b+LovjnoigeLIqiviiKSlEUN7Uy9pAt+1v73LKT80wtiuLRoijW\nFUXxalEUC4qi+OvOuzIAAAAAAADg7aB3Vy9gi8uSHJdkXZLnkxzdhjlPJpnXwvZftTS4KIrvJ/k/\nW44/O0nfJJ9OcldRFBdVKpWr27FuAAAAAAAAgL0mvH4tjUH02SQ1SX7Rhjn/XalUZrbl4EVRjEtj\ndP1dkrGVSuWVLdu/l2Rxku8XRfHvlUpl+e4vHQAAAAAAAHi72yseNVypVH5RqVSeqVQqlU46xQVb\nfv7fpui65bzLk1yTZJ8k53TSuQEAAAAAAIAebq8Ir+30v4qi+NuiKP6fLT+P3cnYU7f8vKeFfXdv\nNwYAAAAAAABgt+wtjxpuj9O2fJoVRbEgydRKpfLcVtsGJDkoybpKpfJiC8d5ZsvPI9ty0qIoFrey\nqy3vpQUAAAAAAAB6oO54x+v6JFclGZNkvy2fpvfCnpKkdktsbTJ4y89XWzle0/YhHb5SAAAAAAAA\n4G2h293xWqlUXkoyY7vNDxRFMTnJoiQnJvlCkv+3k84/pqXtW+6EPb4zzgkAAAAAAADs3brjHa8t\nqlQqbyX58ZZfTtxqV9MdrYPTsqbtaztjXQAAAAAAAEDP12PC6xartvxsftRwpVJ5PcmfkgwsiuLA\nFuYcseXn0528NgAAAAAAAKCH6mnh9S+3/Pz9dtv/c8vPD7cw5yPbjQEAAAAAAADYLd0uvBZFcXxR\nFDusuyiKSUm+tuWXN223+//b8vNbRVHst9WcQ5JcmOTNJP/S4YsFAAAAAAAA3hZ6d/UCkqQoik8k\n+cSWX47Y8vOkoijmbvm+ulKpXLLl+z8mOaIoioeTPL9l27FJTt3y/fJKpfLw1sevVCoPF0Xxj0n+\nLsn/FEVxW5K+ST6VZP8kF1UqleUde1UAAAAAAADA28VeEV6TjE4ydbtth235JMmKJE3h9SdJTk8y\nNo2PCe6TZGWSW5NcXalUHmzpBJVK5f8URfFUGu9wPT9JQ5IlSb5XqVT+veMuBQAAAAAAAHi72SvC\na6VSmZlkZhvHzkkyp53nmZtkbnvmAgAAAAAAALSm273jFQAAAAAAAGBvI7wCAAAAAAAAlCS8AgAA\nAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8\nAgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAA\nlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAA\nAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwC\nAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACU\nJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAA\nAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIA\nAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQk\nvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAA\nAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAA\nAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8\nAgAAAAAAAJQkvAIAAAAAAACUJLwCAMBeYNq0aSmKYofPoEGDMnr06Hz961/P888/v9Nj3HfffTnn\nnHNyxBFH5B3veEcGDhyYww8/PNOmTcv8+fPbtI7f/va3ufjii3PMMcfkHe94R/bZZ58cfPDBef/7\n358LLrggt9xyS9asWdMRlwwAAADQoxSVSqWr19AjFEWx+Pjjjz9+8eLFXb0UAAC6oWnTpuWGG25I\nnz59sv/++ydJKpVKVq1alabfsw8ZMiR33XVXxo8fv83cNWvW5LOf/Wzuueee5m377rtviqLI66+/\n3rztQx/6UG6++ebm42/vuuuuy0UXXZSNGzcmSYqiyJAhQ7J+/fq8+eabzeN+8IMf5Ktf/WrHXDgA\nAABAFxszZkyWLFmypFKpjClzHHe8AgDAXmTcuHGpq6tLXV1dVq5cmXXr1uXGG2/MkCFDsnbt2px5\n5pnZsGFD8/i1a9dm/Pjxueeee7LPPvvksssuyx/+8Ie8/vrrWbduXVasWJErrrgi/fr1y7333pvx\n48dn7dq1O5z3oYceygUXXJCNGzfmgx/8YBYuXJg33ngja9asyYYNG/L000/n6quvzkknnZSiKPbk\nfxIAAACAbqF3Vy8AAABo3b777pvPfe5zSZLPf/7zqaury7x583L22WcnSc4777wsW7Ys/fv3z913\n352amppt5r/73e/OzJkzc+qpp+bDH/5wli1blvPPPz+33nrrNuP++Z//OZVKJccee2zuueee9OrV\nq3lfURQ54ogjcsQRR+TCCy/MG2+80clXDQAAAND9uOMVAAC6gbPOOitVVY2/fW96vcXjjz+e2267\nLUkya9asHaLr1iZOnJgrr7wySfKzn/0s278i46mnnkqSfOQjH9kmurakX79+7bsIAAAAgB5MeAUA\ngG5gn332ydChQ5Mk9fX1SZJrr702SeO7Xy+88MJdHuPCCy/M4MGDt5m7vT/96U8dsVwAAACAtx3h\nFQAAuoENGzZk1apVSRpDa5IsWLAgSTJ58uT0799/l8fYd999M3ny5G3mNjnhhBOSJD/96U9zxx13\ndNCqAQAAAN4+hFcAAOgG5syZk0qlkiQ58cQTs2nTpjz77LNJkuOOO67Nxzn22GOTJM8880zeeuut\n5u2XXnpp9t1332zatCmf/OQnc8ghh+Scc87Jj370oyxevDibN2/uwKsBAAAA6HmEVwAA2EtVKpUs\nX7483//+93PppZcmSd7znvfkYx/7WNasWdM87oADDmjzMZseV5xkm2NUV1fn/vvvT3V1dZJkxYoV\nmTt3br70pS/lhBNOyAEHHJALLrggf/zjH8teFgAAAECP1LurFwAAAPzZwoULUxRFi/sOPPDAzJs3\nL3379u2Uc5900kl56qmn8sADD+Tuu+/OL3/5y/z3f/936uvr8+qrr+baa6/NLbfckrvuuisTJkzo\nlDUAAAAAdFfCKwAA7EX69OmT/fffP0lSFEUGDBiQww47LKeddlq+8IUvZL/99kuS5jFJ8vLLL7f5\n+KtXr27+vvUxmhRFkZqamtTU1CRJNm/enEceeSSzZ8/OjTfemFdffTWf+tSn8rvf/a5N75UFAAAA\neLsQXgEAYC8ybty4LFiwYJfj+vTpk5EjR+Z3v/tdnnzyyTYf/3/+53+SJEcccUR69971Hwd69eqV\nk08+OSeffHIOP/zwXH755XnxxRdzzz335PTTT2/zeQEAAAB6Ou94BQCAbuoDH/hAkmT+/PnZsGHD\nLsevX78+8+fPT5LmO1p3x/Tp05u/P/3007s9HwAAAKAnE14BAKCbOv/885Mka9euzTXXXLPL8ddc\nc01effXVJMnf/u3f7vb5BgwY0Py9s94zCwAAANBdCa8AANBNjR07NmeccUaSZMaMGXnggQdaHfvg\ngw/miiuuSJJ88pOfzAknnLDN/gULFmTz5s07Pd/NN9/c/H306NHtXTYAAABAjyS8AgBAN/bjH/84\nRx11VDZs2JDJkydnxowZee6555r3//GPf8yVV16ZyZMnZ8OGDTnqqKMye/bsHY5zySWX5PDDD8/M\nmTPz2GOPZdOmTUmShoaG/OEPf8g3v/nNXHzxxUkao+vEiRP3zAUCAAAAdBO9u3oBAABA++233355\n6KGHcvbZZ+e+++7LVVddlauuuioDBgxIURRZt25d89gPfvCDueWWW7LffvvtcJw+ffpk+fLlufLK\nK3PllVemqqoqgwcPzrp165ojbJKMGjUq8+bNS69evfbI9QEAAAB0F8IrAAB0kqVLk9rapL4+GTQo\nmTQpqa7u+PMccMABmT9/fu69997cfPPNWbRoUerq6lKpVHLYYYfl5JNPzmc+85l8+MMfbvUYv/jF\nL3LvvfemtrY2jz32WJ599tmsXbs2vXv3zogRI3Lcccfl9NNPz5QpU7zfFQAAAKAFRaVS6eo19AhF\nUSw+/vjjj1+8eHFXLwUAgC5WW5vMmpW09MrViROTGTMaIywAAAAAXW/MmDFZsmTJkkqlMqbMcbzj\nFQAAOtCcOcnkyS1H16Rx++TJyfXX79l1AQAAANC5hFcAAOggtbXJ+ecnDQ07H9fQkJx3XuN4AAAA\nAHoG4RUAADrIrFm7jq5NGhqSq67q3PUAAAAAsOcIrwAA0AGWLm398cKtWbiwcR4AAAAA3Z/wCgAA\nHaC9jw32uGEAAACAnkF4BQCADlBfv2fnAQAAALB3EV4BAKADDBq0Z+cBAAAAsHcRXgEAoANMmrRn\n5wEAAACwdxFeAQCgA1RXJxMn7t6cmprGeQAAAAB0f8IrAAB0kBkzkqo2/g67qiq5/PLOXQ8AAAAA\ne47wCgAAHWTSpOS663YdX6uqktmzPWYYAAAAoCcRXgEAoANNn57Mn9/4GOGW1NQ07j/33D27LgAA\nAAA6V++uXgAAAPQ0kyY1fpYuTWprk/r6ZNCgxm3e6QoAAADQMwmvAADQSaqrhVYAAACAtwuPGgaA\nLjBt2rQURZFTTjllm+0zZ85MURQ55JBDumRdAAAAAAC0j/AKAAAAAAAAUJLwCgAAAAAAAFCS8AoA\nAAAAAABQkvAKAAAAAAAAUJLwCgAAAAAAAFCS8AoAAAAAdDurVq1KURQpiiJ33nlnq+O++MUvNo+7\n4447Wh130UUXpSiK/MVf/EXztkMOOaR5btOnV69eOeCAAzJhwoT84Ac/yPr16zv0ugCA7kt4BQAA\nAAC6nWHDhuXoo49OkjzwwAOtjtt6X1vG1dTU7LBvwIABGT58eIYPH57BgwdnzZo1WbRoUf7u7/4u\nY8eOzUsvvdTeywAAehDhFQAAAADolpoiaWtB9eWXX86yZcsyfPjwnY5bu3ZtfvWrXyVJJk6cuMP+\nSy65JHV1damrq8uaNWuyevXqfOtb30pRFPn1r3+d888/vyMuBwDo5oRXAAAAAKBbaoqkTzzxRNat\nW7fD/gcffDCVSiV/9Vd/laOOOipPPvlk6uvrWxzX0NCQpOU7Xrd3wAEH5Nvf/nbOOeecJMmdd96Z\nF154ocylAAA9gPAKAAAAAHRLTZF08+bNeeihh3bY/+CDDyZJJkyYkPHjx6ehoWGn44488siMGDGi\nzec/++yzm78vWbJkt9YOAPQ8wisAAAAA0C0ddNBBOeyww5K0/Bjhpm0TJkzIhAkTdjmupccM7+r8\nTVq6kxYAeHvp3dULAAAAAABor5qamvz+97/fIaiuW7cuTzzxREaMGJHDDz88RVEk2TG8rl+/vvlu\n1bY8Znhrzz33XPP3IUOGtGf5AEAP4o5XAAAAAKDbarpL9bHHHssbb7zRvP3hhx/O5s2bm+90HTly\nZA488MA8/vjj2bBhwzbjNm3alGT3w+vs2bOTJFVVVRk7dmyp6wAAuj/hFQAAAADotppi6Ztvvpn/\n+q//at7e9N7WrR8fPH78+GzcuLHFcYccckgOPvjgXZ5v48aN+fWvf50vfOELuf3225Mkn/rUpzJs\n2LDyFwMAdGvCKwAAAADQbR166KF517velWTbxwhv/X7XJuPHj2913M7udr3yyitTFEWKosg+++yT\n6urqzJkzJ0nyl3/5l/nhD3/YQVcDAHRn3vEKAF2o6R1Dbd0OAADAjiZOnJibb765OaJu3Lgxjz76\naAYPHpxjjjmmeVxThN16XNPdrzsLrwMGDMjAgQOTJL169crgwYMzatSonH766fn0pz+d3r39NSsA\nILwCQJfYuHFjkqR///5t2g4AAEDrampqcvPNN+eXv/xl3nrrrTz66KN54403cuqpp6aq6s8P/Tv2\n2GPzjne8I4888kg2bdqUxx57rPl9r1s/knh7l1xySWbOnNnZlwEAdHPCKwB0gKVLk9rapL4+GTQo\nmTQpqa5uffzKlSuTJEOHDm3TdgAAAFrXFE1ff/31LF68uPm9rVs/ZjhpvFv1pJNOyvz587NkyZLm\ncQcddFBGjhy5ZxcNAPQ4wisAlFBbm8yalWz1eqBmEycmM2Y0RtitbdiwIY8//niS5LjjjmveXqlU\nsmjRoh22AwAAsHNHH310hg8fnpUrV+aBBx5ofpRwS3exTpgwIfPnz99m3M4eMwwA0FZVux4CALRk\nzpxk8uSWo2vSuH3y5OT66/+8bdWqVZk6dWrq6+vTq1evnHHGGUmSV199NV/72tfy9NNPJ0nOOuus\nzl4+AABAj9J0d+uCBQvy8MMPp1+/fjnhhBN2GDd+/PjmcQ899FCSnT9mGACgrdzxCgDtUFubnH9+\n0tCw83ENDcl55yXr1j2cK6/8WNasWdO877LLLkvv3r0zbNiwrF69unn71KlTd3gcFgAAADtXU1OT\n2267Lffcc08aGhpSU1OTvn377jDuxBNPTJ8+fZrHNc0FACjLHa8A0A6zZu06ujZpaEiuv35jXnnl\nlQwePDgTJ07Mv/3bv2XmzJnZvHlzVq9enYEDB+b9739/rrnmmly/9S2yAAAAtEnTXatNMbW1f9Da\nv3//jBkzpnnc8OHDc/TRR++ZRQIAPZo7XgFgNy1d2vrjhVvz5JOn5Fe/akh19bbbDznkkFQqlY5b\nHAAAwNsO1phXAAAgAElEQVTUMccck/3337/5SUM7e5LQhAkT8sgjj+xyHADA7hBeAWA31da2f972\n4RUAAIBtLV3a+Oen+vpk0KBk0qS2/VmqKIq8/PLLbTrHd7/73Xz3u9/d5bjly5e36XgAAInwCgC7\nrb5+z84DAAB4O6itbXytS0tPGJo4MZkxozHCAgDsrbzjFQB206BBe3YeAABATzdnTjJ5cuuvdXng\ngcb911+/Z9cFALA7hFcA2E3t/RfW/mU2AADAjmprk/PPTxoadj6uoSE577z2v/4FAKCzCa8AsJuq\nqxsfc7U7amq83xUAAKAls2btOro2aWhIrrqqc9cDANBewisAtMOMGUlVG/8vWlWVXH55564HAACg\nO1q6tPXHC7dm4cLGeQAAexvhFQDaYdKk5Lrrdh1fq6qS2bM9ZhgAAKAl7X1ssMcNAwB7I+EVANpp\n+vRk/vzGxwi3pKamcf+55+7ZdQEAAHQX9fV7dh4AQGfq3dULAIDubNKkxs/SpY3/4rq+Phk0qHGb\nd7oCAADs3KBBe3YeAEBnEl4BoANUVwutAAAAu6u9r2XxOhcAYG/kUcMAAAAAQJeork4mTty9OTU1\n/uErALB3El4BAAAAgC4zY0ZS1ca/payqSi6/vHPXAwDQXsIrAAAAANBlJk1Krrtu1/G1qiqZPdtj\nhgGAvZfwCgAAAAB0qenTk/nzGx8j3JKamsb95567Z9cFALA7enf1AgAAAAAAJk1q/CxdmtTWJvX1\nyaBBjdu80xUA6A6EVwAAAABgr1FdLbQCAN2TRw0DAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACU\nJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAA\nAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIA\nAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQk\nvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAAAAAAJQkvAIAAAAAAACUJLwCAAAAAAAAlCS8AgAAQA8y\nd+7cnHLKKV29DAAAgLcd4RUAAAC2mDZtWoqi2OEzaNCgjB49Ol//+tfz/PPPbzNn+fLlLc7p06dP\nhg8fntNOOy0//vGP89Zbb7V63pkzZ7Z4jIEDB2bUqFH50pe+lN/+9redffkAAACUILwCAADAdpqi\n6fDhw/POd74z69aty5NPPpnvf//7OeaYY7Jo0aIW5+23337N8/r375+XXnop999/f84777x84AMf\nyPr163d63qqqqub5w4cPzxtvvJHf/OY3+dGPfpTjjjsut912W2dcLgAAAB1AeAUAAIDtjBs3LnV1\ndamrq8vKlSuzbt263HjjjRkyZEjWrl2bM888Mxs2bNhh3h133NE8r76+Pi+88EIuvPDCJMmiRYsy\nc+bMnZ734IMPbp5fV1eX9evX59///d/zrne9K2+++WY+//nP54UXXthh3oMPPpjTTz89I0aMyBe+\n8IUsXLgwQ4cOzTHHHJNp06bljjvu6JD/LgAAALROeAUAAIBd2HffffO5z30u//RP/5Qkqaury7x5\n83Y578ADD8zVV1+d0047LUnyk5/8ZLfO27dv33z0ox/Nv/7rvyZJNmzYkBtuuGGbMXPnzk1NTU3m\nzZuXlStXpl+/funTp082bNiQX/3qV7nhhhsyY8aM3TovAAAAu094BQAAgDY666yzUlXV+EfpxYsX\nt3ne5MmTkzQG2zVr1uz2eSdOnJiDDjpoh/PW19fnK1/5SiqVSv76r/86zz77bK6++uqMGzcur7/+\nelasWJHvfe97Oeqoo3b7nAAAAOwe4RUAAADaaJ999snQoUOTNEbPtqpUKs3fN2/e3K5zN4XXrc/7\n0EMPpb6+PkOHDs3PfvazjBw5cps57373u3PJJZfk9ttvb9c5AQAAaDvhFQAAANpow4YNWbVqVZJk\nyJAhbZ43f/78JMnAgQMzbNiwdp37ueee2+G8r7zySpLkPe95T/r169eu4wIAANAxhFcAAABoozlz\n5jTfvXriiSfucvyLL76Yiy66KPfff3+SZMqUKe0673/8x3+krq5uh/MeeuihSZKlS5fm2Wefbdex\nAQAA6Bi9u3oBAAAAsDerVCpZsWJFbrvttsyYMSNJ4x2mH/vYx3YYe8YZZ6Rv375JkvXr1+e1115r\n3nf88cfnO9/5zm6d+4UXXsjdd9+dSy+9NEkyaNCgTJ06tXn/iSeemPe973154oknMnbs2FxwwQV5\n/fXXd/saAQAAKE94BQAAgO0sXLgwRVG0uO/AAw/MvHnzmgPr1poe/bu96dOn54c//GGLc7a2YsWK\nVs87ePDg3Hrrrc3vmE2Sqqqq3HnnnTnrrLPyyCOP5O///u+b9x155JE57bTTct5552X06NE7PS8A\nAADledQwAAAAbKdPnz4ZPnx4hg8fnhEjRmTkyJE57bTT8t3vfjdLly5tNWT+4he/SKVSSaVSSV1d\nXebOnZsDDjgg119/fW644YZdnreqqmqb8x566KGpqanJFVdckV//+teZPHnyDnMOPvjg/PKXv0xt\nbW0uvPDCHHnkkUmSZ555Jj/84Q9z/PHH5xvf+Ea5/yAAAADskjteAQAAYDvjxo3LggULSh1j+PDh\nmTp1akaOHJmJEyfmy1/+csaOHbvTu08PPvjgLF++vF3nO/XUU3Pqqadm7ty5ue666zJz5sxce+21\nueOOO/IP//APGTVq1DaPKQYAAKBjueMVAAAAOtH48eMzZcqUbNy4MV/72tf2yDn79u2byZMn5/bb\nb8/06dOTpE133AIAANB+wisAAAB0sm9961spiiILFizI/fffv0fP/fGPfzxJ8vzzz+/R8wIAALzd\nCK8AAADQyY466qj8zd/8TZLk29/+docd99VXX93lmN/85jdJkne+850ddl4AAAB2JLwCAADAHvD1\nr389SbJw4cIsWrSoQ47585//PCeddFJuu+22vPHGG9vsa2hoyE9/+tPMmjUryZ/vfAUAAKBz9O7q\nBQAAAMDbwcknn5xx48bl4YcfzlVXXZV777239DF79+6dRx55JGeeeWb69u2b9773vXnzzTfz/PPP\nZ+jQoXnllVeSNL5n9qKLLip9PgAAAFonvAIAANBjLV2a1NYm9fXJoEHJpElJdXXXrefSSy/NJz7x\nicyfPz+PPfZYxo4dW+p4U6ZMyciRI3P77bdn0aJFeeaZZ7J27do0NDRk4MCBGTNmTD796U/n4osv\nTt++fTvoKgAAAGhJUalUunoNPUJRFIuPP/744xcvXtzVSwEAAHjbq61NZs1KHnhgx30TJyYzZjRG\n2J5o7ty5mTt3bhYsWNDVSwEAAOgWxowZkyVLliypVCpjyhzHO14BAADoUebMSSZPbjm6Jo3bJ09O\nrr9+z64LAACAnk14BQAAoMeorU3OPz9paNj5uIaG5LzzGscDAABARxBeAQAA6DFmzdp1dG3S0JBc\ndVXnrqcrjB49OtOmTevqZQAAALzt9O7qBQAAAEBHWLq09ccLt2bhwsZ51dWds6auMHr06IwePbqr\nlwEAAPC2445XAAAAeoT2PjbY44YBAADoCMIrAAAAPUJ9/Z6dBwAAAFsTXgEAAOgRBg3as/MAAABg\na8IrAAAAPcKkSXt2HgAAAGxNeAUAAKBHqK5OJk7cvTk1NY3zAAAAoCzhFf5/9u49Sqv6vvf4Zw8X\nEeLIISIYtSGaoC1ejmL0aJQhTqMxNhd7jDEaq0B06crF5MSu1S4BCcQ0TdOTqybKUmlDjdHExJpb\nbccDGClewEQzseZmbGtEUS6jBOUy+/zxMARkgGE2wzOX12utWc/M3vu35/u4VjDyfvbeAABAvzFz\nZtLQxf/SbWhIZszo2XkAAAAYOIRXAAAA+o3m5uTGG3cdXxsakrlz3WYYAACAPUd4BQAAoF+ZNi25\n557abYQ709RU2z916t6dCwAAgP5tcL0HAAAAgD2tubn21dqatLQkbW1JY2Ntm2e6AgAA0BOEVwAA\nAPqtCROEVgAAAPYOtxoGAAAAAAAAqEh4BQAAAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAAqEh4BQAA\nAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAAqEh4BQAAAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAAqEh4\nBQAAAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAAqEh4BQAAAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAA\nqEh4BQAAAAAAAKhIeAUAAAAAAACoSHgFAAAAAAAAqEh4BQAAAAAAAKhIeAUAAAAAAACoSHgFAAAA\nAADoonnz5qUoihRFkd/+9rf1HgfoRYRXAAAAAAAAgIoG13sAAAAAAACAemltTVpakra2pLExaW5O\nJkyo91RAXyS8AgAAAAAAA05LSzJ7drJo0fb7Jk1KZs6sRdhXu+SSS3LJJZf0+HxA3+NWwwAAAAAA\nwIBy003JGWd0Hl2T2vYzzkhuvnnvzgX0bcIrAAAAAAAwYLS0JJddlrS37/y49vbk0ktrxwN0hfAK\nAAAAAAAMGLNn7zq6dmhvT+bM6dl5gP5DeAUAAAAAAAaE1tYd3154RxYurK0D2BXhFQAAAAAAGBC6\ne9tgtxsGukJ4BQAAAAAABoS2tr27DhhYekV4LYri3KIovlwUxX1FUbQVRVEWRTF/F2tOKYriB0VR\nrCyKYl1RFI8WRfGxoigG7WTNxUVRPFgUxUtFUawpimJBURR/tuffEQAAAAAA0Ns0Nu7ddcDA0ivC\na5LpST6c5H8meXpXBxdF8e4ki5JMSvKdJF9JMjTJ55PctoM1n0syL8lBSeYmmZ/k6CR3F0Xx4crv\nAAAAAAAA6NWam/fuOmBg6S3h9eNJxidpTHLFzg4siqIxtXC6KcnksiynlWX5l6lF239Pcm5RFOe/\nas0pST6R5NdJjinL8uNlWX4oycQkK5N8riiKcXv0HQEAAAAAAL3KhAnJpEm7t6apqbYOYFd6RXgt\ny/L/lWX5y7Isyy4cfm6S0UluK8vy4a3O8XJqV84m28fbyze/XluW5aqt1vw2yXVJ9kkypZvjAwAA\nAAAAfcTMmUlDF+tIQ0MyY0bPzgP0H70ivO6m0ze//qiTfYuS/D7JKUVR7NPFNT981TEAAAAAAEA/\n1dyc3HjjruNrQ0Myd67bDANd1xfD6xGbX3/x6h1lWW5M8mSSwUkOS5KiKEYkOTjJS2VZPtPJ+X65\n+XV8V355URRLO/tKcuRuvg8AAAAAAKAOpk1L7rmndhvhzjQ11fZPnbp35wL6tsH1HqAb9t/8umYH\n+zu2j+zm8QAAAAAAQD/X3Fz7am1NWlqStraksbG2zTNdge7oi+G1rsqynNjZ9s1XvR6/l8cBAAAA\nAAAqmDBBaAX2jL54q+GOK1T338H+ju2ru3k8AAAAAAAAwG7pi+H1ic2v2z2TtSiKwUnekGRjkt8k\nSVmWa5M8neQ1RVEc1Mn53rT5dbtnxgIAAAAAAAB0RV8Mr/dufn17J/smJRmeZHFZlq90cc1ZrzoG\nAAAAAAAAYLf0xfD6rSTPJzm/KIoTOjYWRTEsyac2//jVV6352ubXq4ui+B9brRmX5ENJXklySw/N\nCwAAAAAAAPRzg+s9QJIURfGeJO/Z/OPYza8nF0Uxb/P3z5dleVWSlGXZVhTFpakF2AVFUdyWZGWS\ndyU5YvP2b259/rIsFxdF8X+T/J8kjxZF8a0kQ5O8L8moJB8py/K3PfT2AAAAAAAAgH6uV4TXJP8z\nycWv2nbY5q8keSrJVR07yrL8blEUTUmuTvK/kwxL8qvUwuqXyrIsX/0LyrL8RFEUj6V2hetlSdqT\nLEvyd2VZfm/Pvh0AAAAAAABgIOkV4bUsy1lJZu3mmvuTvGM318xLMm931gAAAAAAAADsSl98xisA\nAAAAAABAryK8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABU\nJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAA\nAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIA\nAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQk\nvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAA\nAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAA\nAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8\nAgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAA\nVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAA\nAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwC\nAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABU\nJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCfd4ll1ySoijyJ3/yJ11ec91116Uoigwb\nNiyrV6/OggULUhRFp18jRozIH//xH+fyyy/P448/vsNzTp48ebu1DQ0NGTlyZE488cTMnj07K1eu\n3BNvGQAAAAAA6GWEV6DPu/jii5Mkjz/+eB5++OEurfnHf/zHJMm73/3ujBw5cpt9BxxwQMaMGZMx\nY8Zk9OjRefnll/Mf//EfueGGG3Lsscfm29/+9k7PPWzYsC3rX/va12bNmjV56KGHcs011+SYY47J\nE0880Y13CQAAAAAA9GbCK9DnTZ48Oa9//euT/CGo7swTTzyRBx98MMkfou3WHnrooSxfvjzLly/P\nc889l1deeSUtLS0ZP358NmzYkGnTpuXFF1/c4fnf9773bVm/YsWKrFmzJp///Oezzz775Omnn875\n55+fsiy7+W4BAAAAAIDeSHgF+ryiKHLRRRclSW677bZs3Lhxp8d3xNmxY8fmzDPP3OX5Bw8enNNP\nPz233HJLkmTNmjW57777ujxfY2NjPvaxj+Xqq69OkvzkJz/JkiVLurweAAAAAADo/YRXoF/4i7/4\niyTJihUr8sMf/nCHx5Vlmfnz5ydJLrzwwgwaNKjLv+OYY47Z8v3atWt3e8b3v//9W75funTpbq8H\nAAAAAAB6L+EV6Bfe9KY35ZRTTkmy89sNL1iwIP/5n/+ZpPPbDO/MY489tuX7N77xjbs948EHH7zl\n+7a2tt1eDwAAAAAA9F7CK9BvdITUu+++O6tXr+70mI4oe9xxx+Xoo4/u0nk3bdqUhQsXZsqUKUmS\npqamHHfccbs9X0fwTZKRI0fu9noAAAAAAKD3El6BfuO8887LsGHD8sorr+T222/fbv/vf//7fPvb\n306y86td3/zmN2fs2LEZO3ZsDjzwwOyzzz6ZPHlyXnjhhXz4wx/O3Xff3a355s6du+X7k046qVvn\nAAAAAAAAeifhFeg3Ro4cmXe/+91JOr/d8He+8528+OKLGTx4cC644IIdnuf555/Ps88+m2effTYr\nVqzIpk2bkiQvvfRSVq9enRdffLHLM23atCm//OUv89d//df5whe+kCQ5+eSTM3HixN15awAAAAAA\nQC8nvAL9yiWXXJIkuf/++/Ob3/xmm30dMfass87K6NGjd3iOJ598MmVZbvl67rnncu+992bixImZ\nP39+TjnllPz3f//3Dtf/wz/8Q4qiSFEUGTx4cMaPH5/PfOYz2bRpU4444ojcdttt1d8oAAAAAADQ\nqwivQL/ytre9LQcddFCS5Otf//qW7c8880xaWlqS7Pw2w50ZPXp03vrWt+Zf//Vfc9hhh+Wpp57K\nrFmzdnj8sGHDMmbMmIwZMyYHHXRQxo8fn7PPPjvXX399li1blj/6oz/a/TcGAAAAAAD0asIr0K8M\nGjQoH/jAB5JsG17nz5+fTZs2ZdSoUXnnO9/ZrXPvu+++Oe+885Kk02fIdnjf+96X5cuXZ/ny5fnd\n736XJ554It/73vdyxRVXZPjw4d363QAAAAAAQO8mvAL9TscVrb/+9a+zePHiJH+IsOeff36GDh3a\n7XN3XK364osv5vnnn684KQAAAAAA0F8Ir0C/M2HChEycODFJ7bmujzzySB577LEku3+b4Vd7+umn\nt3w/ZMiQSucCAAAAAAD6j8H1HgCgJ1x88cVZunRpbr/99jQ01D5jcuSRR+bEE0/s9jk3bNiQ7373\nu0mSww47LPvvv/8emRUAAAAAAOj7XPEK9Evvf//7M2TIkKxatSo33HBDku5f7dre3p7HH388733v\ne9Pa2pok+chHPrLHZgUAAAAAAPo+V7wC/dIBBxyQs88+O9/97nfT3t6ehoaGfOADH+jS2je/+c0Z\nNGjQlp9XrVqV9evXb/l5ypQp+ehHP7rHZwYAAAAAAPou4RXotVpbk5aWpK0taWxMmpuTCRO6vv7i\niy/ecmvg008/PYccckiX1j3//PPb/Dx06NAceuihOemkkzJ16tScddZZXR8CAAAAAAAYEIRXoNdp\naUlmz04WLdp+36RJycyZtQi7K+95z3tSlmWXfufkyZO7fOyOLFiwoNJ6AAAAAACg7/KMV6BXuemm\n5IwzOo+uSW37GWckN9+8d+cCAAAAAADYGeEV6DVaWpLLLkva23d+XHt7cumlteMBAAAAAAB6A+EV\n6DVmz951dO3Q3p7MmdOz8wAAAAAAAHSV8Ar0Cq2tO7698I4sXFhbBwAAAAAAUG/CK9ArdPe2wW43\nDAAAAAAA9AbCK9ArtLXt3XUAAAAAAAB7kvAK9AqNjXt3HQAAAAAAwJ4kvAK9QnPz3l0HAAAAAACw\nJwmvQK8wYUIyadLurWlqqq0DAAAAAACoN+EV6DVmzkwauvinUkNDMmNGz84DAAAAAADQVcIr0Gs0\nNyc33rjr+NrQkMyd6zbDAAAAAABA7yG8Ar3KtGnJPffUbiPcmaam2v6pU/fuXAAAAAAAADszuN4D\nALxac3Ptq7U1aWlJ2tqSxsbaNs90BQAAAAAAeiPhFei1JkwQWgEAAAAAgL7BrYYBAAAAAAAAKhJe\nAQAAAAAAACoSXgEAAAAAAAAqEl4BAAAAAAAAKhJeAQAAAAAAACoSXgEAAAAAAAAqEl4BAAAAAAAA\nKhJeAQAAAAAAACoSXgEAAAAAAAAqEl4BAAAAAAAAKhJeAQAAAAAAACoSXgEAAAAAAAAqEl4BAAAA\nAAAAKhJeAQAAAAAAACoSXgEAAAAAAAAqEl4BAAAAAAAAKhJeAQAAAAAAACoSXgEAduKSSy5JURSZ\nPHnyNttnzZqVoigybty4Ha5taWnJBRdckMMOOyz77rtvRowYkcMPPzxNTU35q7/6q/zoRz/K+vXr\ne/YNAAAAAAB7hfAKALCHbdq0KdOmTcuf/umf5hvf+EaefPLJbNy4McOGDctTTz2VRYsW5W//9m9z\n1lln5ec//3m9xwWAPqPjA1Fbfw0ZMiSvfe1r88Y3vjHvec978ulPfzpPPvnkdms7PjTVna8FCxbs\n/TcLAAD0OYPrPQAAQH/z2c9+NjfffHOS5IorrshHP/rRjB8/Pg0NDdmwYUN++tOf5vvf/37mzZtX\n30EBoI8aMmRIRo0alSQpyzJtbW1ZuXJlfv3rX+euu+7K9OnTc+655+b666/PAQcckCR5zWtekzFj\nxmx3rvXr12fVqlVJkgMOOCCDBg3a7pihQ4f24LsBAAD6C+EVAGAPKssyX/7yl5MkH/rQh/KVr3xl\nm/1DhgzJCSeckBNOOCEzZszIxo0b6zEmAPRpp5xyynZXoa5evTpLlizJvHnzcscdd+SOO+7I4sWL\ns2TJkhxyyCG56qqrctVVV213rgULFuStb31rkuShhx7a6WMEAAAAdsathgEA9qDnn38+zzzzTJLk\nz/7sz3Z6bENDgytoAGAPGTlyZN7+9rfntttuy/e///0MGzYsTz/9dM4999x6jwYAAAwQwisAQA95\n+umn6z0CAAxIb3/72/O5z30uSfLAAw/k7rvvrvNEAADAQCC8AgDsQaNHj87rX//6JMmcOXPy2GOP\n1XkiABiYLr300i3PdL311lvrPA0AADAQCK8AAHvYNddckyR56qmncswxx2TixIm58sorM3/+/Pzq\nV7+q83QAMDAMHTo0p59+epLkvvvuq/M0AADAQCC8AgDsYVOmTMlNN92U0aNHJ0mWLVuWL33pS7no\noovypje9KW94wxty7bXXZu3atXWeFAD6t6OPPjpJ7fb/GzZsqPM0AABAfye8AgD0gKlTp+app57K\nHXfckcsvvzzHHXdchg4dmiT57W9/m+nTp+fNb35znn322TpPCgD916hRo7Z8v3LlyjpOAgAADATC\nKwBAD9l3331z7rnn5qtf/WqWLVuWVatW5Z//+Z9zyimnJEkef/zxXH755XWeEnrGJZdckqIoMnny\n5G22z5o1K0VRpCiKHHzwwXn55Zd3eI7p06d3eg6ArirLst4jAAAAA4jwCgCwlwwfPjzvfOc78+Mf\n/zhve9vbkiR33XVXXnjhhTpPBvXxu9/9Ltdff329xwD6sVWrVm35fuurXwEAAHqC8AoAsJcVRZEp\nU0oqODsAACAASURBVKYkqV2J86tf/arOE0H9fOYzn8lLL71U7zGAfurRRx9NkhxyyCEZMmRInacB\nAAD6O+EVAKAORowYseX7jme/wkBy7LHH5nWve11WrFiRL3zhC/UeB+iH1q9fn3vvvTdJctppp9V5\nGgAAYCAQXgEA9qD169dn4cKFuzzu1ltvTVJ7DuwRRxzR02NBrzNs2LBMnz49SfL3f//3Wb16dZ0n\nAvqbuXPn5rnnnkuSXHjhhXWeBgAAGAiEVwCAPWj9+vWZPHlyTj755Fx//fX5xS9+kbIskyQbNmzI\nww8/nPe+97355je/mST54Ac/mOHDh9dzZKibD37wgxk3blxWr16dv/u7v6v3OEA/8i//8i/5y7/8\nyyTJySefnLPPPrvOEwEAAAPB4HoPAADQFxRF0aXtDQ0NGTRoUJYsWZIlS5YkSYYMGZL99tsvq1at\n2hJhk+Scc87JZz/72Z4bGnq5IUOG5JprrsmUKVPyxS9+MVdeeWUOPPDAeo8F9FFr1qzJkiVLMm/e\nvNx+++1pb2/PoYcemm9961v1Hg0AABgghFcAgJ1Yv359ktotgbuyffjw4XnmmWfyve99LwsWLMgj\njzySp556KmvWrMmIESNy8MEH58QTT8yFF16YM888c++8CejFLrroonzmM5/JE088kb/5m7/J5z//\n+XqPBPQBixcvztixY5MkZVnmxRdfzLp167bsL4oi5513Xq677roccMAB9RoTAAAYYIRXAGBAaW1N\nWlqStraksTFpbk4mTNjx8c8++2ySbPeXtjvaniSjR4/OlClTMmXKlD03OPRTgwYNyic/+cmcf/75\n+epXv5pPfOITOeSQQ+o9FtDLbdiwYcu/iwcNGpTGxsYcdNBBOeqoo3LSSSflggsuyLhx4+o7JAAA\nMOAIrwDAgNDSksyenSxatP2+SZOSmTNrEXZr69aty8MPP5wkOfbYY7dsL8syP/7xj7fbDnTPeeed\nl09/+tN59NFHM2fOnNxwww31HgnYi3bnQ1Hz5s3LvHnz9ujvnzx58jaPAgAAAOiuhnoPAADQ0266\nKTnjjM6ja1LbfsYZyc03/2HbihUrcvHFF6etrS2DBg3Kn//5nyepPT/u4x//eH7xi18kqQUjoJqi\nKDJnzpwkyS233JLf/OY3dZ4I2BtaWpKmpuSoo5Irr0xmzKi9HnVUbXtLS70nBAAA2D3CKwDQr7W0\nJJddlrS37/y49vbk0kuTL31pcV772tfmwAMPzB133JEkmT59egYPHpzRo0dn5MiR+eIXv5gkufji\ni3Paaaf19FuAAeFd73pXTjzxxGzYsCGzZs2q9zhAD+vOh6IAAAB6O+EVAOjXZs/edXTt0N6e3Hzz\n+qxatSr7779/Jk2alG984xuZNWtWNm3alOeffz6vec1rcuKJJ+a6667Lzf42GPaoT33qU0mSf/qn\nf8rPf/7zOk8D9JTd/VCUK18BAIC+wjNeAYB+q7V1x1fS7MhPfzo5P/tZ+3bPlhs3bpznv0EPe9vb\n3pZJkyZl0aJFmTlzZo488sh6jwT0gN39UNScOds/hx0AAKA3csUrANBvdfcKGVfWQP1ce+21SZI7\n77wzjzzySJ2nAfa07nwoauHC2joAAIDeTngFAPqttra9uw6o7tRTT82ZZ56Zsizzgx/8oN7jAHuY\nD0UBAAD9mfAKAPRbjY17dx2wZ3Q86xXof3woCgAA6M+EVwCg3+ru8+A8Rw7q64QTTsg555xT7zGA\nHuBDUQAAQH9WlGVZ7xn6haIolh5//PHHL126tN6jAABbaWravWfJNTUlCxb02DjQZ7W21m712dZW\nCyDNzcmECfWeCuhrWluTo47a/XU/+5k/cwAAgJ4zceLELFu2bFlZlhOrnGfwnhoIAKA3mjkzOeOM\npL1918c2NCQzZvT8TNCXtLQks2d3/gGGSZNq/xtzlTjQVRMm1P7s2N0PRYmuAABAX+BWwwBAv9bc\nnNx4Yy2q7kxDQzJ3roAEW7vpptoHF3YUSBYtqu2/+ea9OxfQt82cuet/L3fwoSgAAKAvEV4BgH5v\n2rTknntqV8x0pqmptn/q1L07F/RmLS3JZZft+mrx9vbk0ktrxwN0hQ9FAQAA/ZVbDQMAA0Jzc+3L\ncyqha2bP7totupPacXPmiCNA102blowbV/uzY+HC7fc3NdWudPXnCgAA0JcIrwDAgDJhgtAKu9La\nunvPX0xq4aS11f++gK7zoSgAAKC/EV4BAIBtdPe2wS0tYgmw+3woCgAA6C884xUAANhGW9veXQcA\n0B0rVqxIURQpiiJ33XXXDo+74oorthx355137vC4j3zkIymKIkcdddSWbePGjduytuNr2LBhGTNm\nTI466qhcdNFF+drXvpbVq1d3es7Jkydvt76rXwBA3yO8AgAA22hs3LvrAAC6Y/To0TnyyCOTJIt2\n8pyErfd15bimpqbt9o0YMSJjxozJmDFjst9++2XVqlVpbW3N/Pnzc8UVV+R1r3tdZsyYkY0bN26z\nbtSoUVvWbf01YsSIJElDQ0On+8eMGdP1fxAAQK8hvAIAANtobt676wAAuqsjku4oqL7wwgt5/PHH\nt4TMHR23evXq/OxnP0uSTJo0abv9V111VZYvX57ly5dnxYoVWb9+ff7rv/4r8+fPz8knn5x169bl\nU5/6VM4666xt4uudd965Zd3WX1dddVWS5NBDD+10//Lly7v/DwUAqBvhFQAA2MaECUknf9+4U01N\nntEIAOx9HZH0kUceyUsvvbTd/vvuuy9lWeYd73hHjjjiiPz0pz9NWyfPR7jvvvvS3t6epPMrXjtz\nyCGH5MILL8z999+fT37yk0mSf/u3f8vVV1/d3bcDAPRxwisAALCdmTOThi7+10JDQzJjRs/OAwDQ\nmY5IumnTptx///3b7b/vvvuSJKeddlpOPfXUtLe37/S48ePHZ+zYsbs1Q1EUmTlzZs4999wkyZe/\n/OU899xzu3UOAKB/EF4BAIDtNDcnN9646/ja0JDMnes2wwBAfRx88ME57LDDknR+G+GObaeddlpO\nO+20XR7X2W2Gu2r69OlJknXr1uU73/lOt88DAPRdwisAANCpadOSe+6p3Ua4M01Ntf1Tp+7duQAA\ntraj57y+9NJLeeSRRzJ27Ni88Y1vzKmnntrpcb///e+zbNmybc7VHccee2wOOuigJH+4ghYAGFgG\n13sAAACg92purn21tiYtLUlbW9LYWNvmma4AQG8wadKk3HLLLXnooYfy8ssvZ9iwYUmSxYsXZ9Om\nTVuudD388MNz0EEH5eGHH866deuy7777bjluw4YNSaqF1yQ5+uij88wzz+TJJ5+sdB4AoG8SXgEA\ngF2aMEFoBQB6p45Y+sorr+SBBx7Y8nPHVadb3z741FNPzR133JEHHnggkydP3ua4cePG5dBDD600\ny6hRo5IkK1eurHQeAKBvcqthAAAAAKDPesMb3pBDDjkkyba3Ed76+a4dOrvdcMf3Va92TZKyLCuf\nAwDou4RXAAAAAKBP67iqtSOirl+/Pg8++GD233//HH300VuO64iwWx/3wAMPJNkz4XXVqlVJ/nDl\nKwAwsAivAAAAAECf1hFN//3f/z0bN27Mgw8+mJdffjlvectb0tDwh78CPeaYY7LffvtlyZIl2bBh\nQx566KGsW7cuyba3JO6uRx99NEly2GGHVT4XAND3CK8AAAAAQJ/WEU3Xrl2bpUuXbnlu69a3GU6S\nQYMG5eSTT87atWuzbNmyLccdfPDBOfzwwyvN8JOf/CTLly/v9PcCAAOD8AoAAAAA9GlHHnlkxowZ\nk6R2G+GOWwl3dhXr1rcb3pPPd7322muTJMOHD88555xT+XwAQN8jvAIAAAAAfV5HUF2wYEEWL16c\nYcOG5YQTTtjuuFNPPXXLcffff3+SarcZLssyc+bMybe+9a0kyZVXXpnRo0d3+3wAQN8lvAIAAAAA\nfV7HVas/+tGP0tbWlpNOOilDhw7d7riTTjopQ4YM2XLc1mt3x9NPP51bb701b3nLWzJz5swkyZln\nnpnZs2dXeBcAQF82uN4DAAAAAABU1XHVant7e5IdP2d13333zcSJE7NkyZIkyZgxY3LkkUfu9Nyf\n+9zn8rWvfS1JsmnTprS1tWX9+vVb9g8fPjxXXXVVZsyYkcGD/ZUrAAxU/l8AAAAAANDnHX300Rk1\nalRWrlyZZMfhtWNfR3jd2XEd1q5dm7Vr1yZJhg4dmsbGxhx44IE57rjjctppp+X888/P/vvvvwfe\nBQDQlxVlWdZ7hn6hKIqlxx9//PFLly6t9ygAAAAA0Ge1tiYtLUlbW9LYmDQ3JxMm1HsqAKA/mzhx\nYpYtW7asLMuJVc7jilcAAAAAoO5aWpLZs5NFi7bfN2lSMnNmLcICAPRWDfUeAAAAAAAY2G66KTnj\njM6ja1LbfsYZyc037925AAB2h/AKAAAAANRNS0ty2WVJe/vOj2tvTy69tHY8AEBvJLwCAAAAAHUz\ne/auo2uH9vZkzpyenQcAoLuEVwAAAACgLlpbd3x74R1ZuLC2DgCgtxFeAQAAAIC66O5tg91uGADo\njYRXAAAAAKAu2tr27joAgJ4kvAIAAAAAddHYuHfXAQD0JOEVAAAAAKiL5ua9uw4AoCcJrwAAAABA\nXUyYkEyatHtrmppq6wAAehvhFQAAAACom5kzk4Yu/i1lQ0MyY0bPzgMA0F3CKwAAAABQN83NyY03\n7jq+NjQkc+e6zTAA0HsJrwAAAABAXU2bltxzT+02wp1paqrtnzp1784FALA7Btd7AAAAAACA5uba\nV2tr0tKStLUljY21bZ7pCgD0BcIrAAAAANBrTJggtAIAfZNbDQMAAAAAAABUJLwCAAAAAAAAVCS8\nAgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAA\nVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAA\nAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwC\nAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABU\nJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAA\nAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIA\nAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQkvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFQk\nvAIAAAAAAABUJLwCAAAAAAAAVCS8AgAAAAAAAFTUZ8NrURS/LYqi3MHX8h2sOaUoih8URbGyKIp1\nRVE8WhTFx4qiGLS35wcAAAAAAAD6j8H1HqCiNUm+0Mn2l169oSiKdyf5dpKXk3wzycok70zy+SRv\nSfLenhsTAAAAAAAA6M/6enhdXZblrF0dVBRFY5K5STYlmVyW5cObt89Icm+Sc4uiOL8sy9t6clgA\nAAAAAACgf+qztxreTecmGZ3kto7omiRlWb6cZPrmH6+ox2AAAAAAAABA39fXr3jdpyiKDyT5oyRr\nkzyaZFFZlpteddzpm19/1Mk5FiX5fZJTiqLYpyzLV3psWgAAAAAAAKBf6uvhdWySr79q25NFUUwp\ny3LhVtuO2Pz6i1efoCzLjUVRPJlkQpLDkjy+s19YFMXSHew6smsjAwAAAAAAAP1NX77V8C1JmlOL\nryOSHJ3khiTjkvywKIpjtzp2/82va3Zwro7tI/f8mAAAAAAAAEB/12eveC3L8pOv2vSzJJcXRfFS\nkk8kmZXknB74vRM72775Stjj9/TvAwAAAAAAAHq/vnzF6458bfPrpK22dVzRun8617F9dY9MBAAA\nAAAAAPRr/TG8rtj8OmKrbU9sfh3/6oOLohic5A1JNib5Tc+OBgAAAAAAAPRH/TG8/q/Nr1tH1Hs3\nv769k+MnJRmeZHFZlq/05GAAAAAAAABA/9Qnw2tRFH9cFMWITraPS/KVzT/O32rXt5I8n+T8oihO\n2Or4YUk+tfnHr/bIsAAAAAAAAEC/N7jeA3TT+5J8oiiKRUmeSvJiksOTnJ1kWJIfJPlcx8FlWbYV\nRXFpagF2QVEUtyVZmeRdSY7YvP2be/UdAAAAAAAAAP1GXw2v/y+1YHpckrek9jzX1Ul+nOTrSb5e\nlmW59YKyLL9bFEVTkquT/O/UAu2vkvyfJF969fEAAAAAAAAAXdUnw2tZlguTLOzGuvuTvGPPTwQA\nAAAAAAAMZH3yGa8AAAAAAAAAvYnwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAA\nAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAA\nAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHw\nCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAA\nUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAA\nAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAK\nAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQ\nkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAA\nAABQkfAKAAAAAAAAUJHwCgAAAAAAAFCR8AoAAAAAAABQkfAKAAAAAAAAUJHwCgAAAAAAdNuCBQtS\nFEWKosiCBQvqPQ5A3Qiv8P/Zu/swr+s6X/zP7wCCqCMiOMNJvAEzBfEGKjc0RiXpZtd1j6WViwqy\n3mSiR3/B6bQLEfhLr7RrzUpDFyzzdDqWLp12tw46BnhTK4HHYxPaby1NjUFQYQYEROb7+2Nk8mYG\nhrn7zs3jcV3fa758Pu/36/P6dMX1xXl+3+8PAAAAAAAAtJPgFQAAAAAAaLO1a9cmSQYPHpwxY8aU\nuBuA0ulf6gYAAAAAAIDuoaYmqa5O6uqS8vJk8uRk7Njdz1m+fHmS5HOf+1wOOeSQLugSoHsSvAIA\nAAAAQB9XXZ3Mn5+sWPHuc5MmJXPnNoawzVm+fHn23XffzJo1q3ObBOjmbDUMAAAAAAB92KJFyZQp\nzYeuSePxKVOSxYvffW79+vV56qmnctlll6WioqJzGwXo5qx4BQAAAACAPqq6Orn00qShYffjGhqS\nSy5JDj/87Stfhw8fnmKx2LlNAvQQVrwCAAAAAEAfNX/+nkPXXRoakgULOrcfgJ5M8AoAAAAAAH1Q\nTU3L2wu3ZPnyxnkAvJvgFQAAAAAA+qDq6q6dB9DbCV4BAAAAAKAPqqvr2nkAvZ3gFQAAAAAA+qDy\n8q6dB9DbCV4BAAAAAKAPmjy5a+cB9HaCVwAAAAAA6IPGjk0mTdq7OVVVjfMAeDfBKwAAAAAA9FFz\n5yZlrUwKysqSOXM6tx+AnkzwCgAAAAAAfdTkycntt+85fC0rS+64wzbDALsjeAUAAAAAgD5sxoxk\n6dLGbYSbU1XVeP7ii7u2L4Cepn+pGwAAAAAAAEpr8uTGV01NUl2d1NUl5eWNxzzTFaB1BK8AAAAA\nAECSxpBV0ArQNrYaBgAAAAAAAGgnwSsAAAAAAABAOwleAQAAAAAAANpJ8AoAAAAAAADQToJXAAAA\nAAAAgHYSvAIAAAAAAAC0k+AVAAAAAAAAoJ0ErwAAAAAAAADtJHgFAAAAAAAAaCfBKwAAAAAAAEA7\nCV4BAAAAAAAA2knwCgAAAAAAANBOglcAAAAAAACAdhK8AgAAAAAAALST4BUAAAAAAACgnQSvAAAA\nAAAAAO0keAUAAAAAAABoJ8ErAAAAAAAAQDsJXgEAAAAAAADaSfAKAAAAAAAA0E6CVwAAAAAAAIB2\nErwCAAAAAAAAtJPgFQAAAAAAAKCdBK8AAAC0y7Rp01IoFFIoFDJhwoTdjp06dWoKhUKmTZvW4TUA\nAACglASvAAAAdJjVq1fnvvvuK3kNAAAA6GqCVwAAADrU3Llz09DQUPIaAAAA0JUErwAAAHSIqqqq\nDB48ODU1NfnBD35QshoAAABQCoJXAAAAOkRlZWWuvPLKJMm8efPyxhtvlKQGAAAAlILgFQAAgA4z\ne/bslJeX55lnnsmdd95ZshoAAADQ1QSvAAAAdJiDDz4411xzTZJkwYIF2b59e0lqAAAAQFcTvAIA\nANChrr322gwdOjTPP/98vvOd75SsBgAAAHQlwSsAAAAdqry8PLNnz06SXH/99dmyZUtJagAAAEBX\nErwCAADQ4WbOnJmKioqsW7cut9xyS8lqAAAAQFcRvAIAANDhBg8enC996UtJkhtvvDGbNm0qSQ0A\nAADoKoJXAAAAOsVll12WkSNH5tVXX83Xv/71ktWAvmLatGkpFAoZM2ZMq+d8+9vfTqFQyKBBg7Jx\n48YsW7YshUKh2dd+++2XY489NpdffnnWrFnTiXcCAAA9k+AVAACATjFw4MDMmTMnSXLzzTdnw4YN\nJakBfcVFF12UJFmzZk1+/etft2rOXXfdlSQ5++yzM2TIkLedGzZsWCoqKlJRUZHhw4dn27Zteeqp\np7Jw4cKccMIJuffeezv2BgAAoIcTvAIAANBppk+fntGjR6e+vj433HBDyWpAX3Daaafl8MMPT/Ln\nQHV3nn766Tz22GNJ/hzavtXKlStTW1ub2travPTSS9m+fXuqq6tz9NFHZ8eOHZkxY0bq6+s79iYA\nAKAHE7wCAADQafr375958+YlSW699dasXbu2JDWgLygUCrnggguSJD/84Q/zxhtv7Hb8rnC2srIy\nH/3oR/dYv3///jnjjDNy5513Jkk2bdqUhx56qJ1dAwBA7yF4BQAAoFOdf/75GTNmTLZu3ZoHH3yw\nZDWgL7jwwguTJOvXr8/PfvazFscVi8XcfffdSZK//du/Tb9+/Vp9jeOPP77p/ZYtW9rYKQAA9D6C\nVwAAADpVWVlZ5s+fX/Ia0Be8973vzcSJE5PsfrvhZcuW5Y9//GOS5rcZ3p0nn3yy6f1RRx3Vhi4B\nAKB3ErwCAADQ6c4555yMHz++5DWgL9gVpP70pz/Nxo0bmx2zK5Q96aSTMm7cuFbV3blzZ5YvX57p\n06cnSaqqqnLSSSd1QMcAANA7FIrFYql76BUKhcKq8ePHj1+1alWpWwEAAGiXmpqkujqpq0vKy5PJ\nk5OxY0vdFdBaGzduzIgRI7Jt27YsXLgwl1566dvOv/baa6msrEx9fX1uvvnmXH311U3nli1bltNP\nPz1JMmzYsKYtiBsaGvLKK69k586dGTZsWD7zmc/kq1/9ag444ICuuzEAAOgkEyZMyOrVq1cXi8UJ\n7aljxSsAAD3CtGnTUigUUigUMmHC7v8NPHXq1BQKhUybNq3Da7yzzltf5eXlOfHEEzNr1qy88MIL\ne3uLUHLV1UlVVXLcccnVVydz5jT+PO64xuPV1aXuEGiNIUOG5Oyzz07S/HbD//zP/5z6+vr0798/\n559/fot1NmzYkHXr1mXdunVZv359du7cmSTZvHlzNm7cmPr6+s65AQAA6KEErwAA9DirV6/Offfd\nV/IaAwYMSEVFRSoqKnLIIYdk8+bNeeKJJ3LTTTdl3Lhxefjhh9tVH7rSokXJlCnJihXNn1+xovH8\n4sVd2xfQNru+OPTII4/k97///dvO7QpjP/7xj2f48OEt1vjDH/6QYrHY9HrppZfy4IMPZsKECbn7\n7rszceJEXzQCAIC3ELwCANAjzZ07Nw0NDSWtMXHixNTW1qa2tjbr1q3L5s2bc9ddd2XIkCHZuHFj\nzj333GzdurVdPUJXqK5OLr002dNfh4aG5JJLrHyFnuDMM8/MiBEjkiTf//73m46vXbs21W/+Jd71\nLNjWGj58eE4//fTcf//9GTVqVJ577rnMmzevw3oGAICeTvAKAECPUlVVlcGDB6empiY/+MEPSlaj\nOYMHD84FF1yQW265JUlSW1ubJUuWdFh96Czz5+85dN2loSFZsKBz+wHar1+/fpk6dWqStwevd999\nd3bu3JmhQ4fmrLPOalPtfffdN+edd16S5J577ml/swAA0EsIXgEA6FEqKytz5ZVXJknmzZuXN954\noyQ1due8885LWVnjP7VXrVrVobWho9XUtLy9cEuWL2+cB3Rvu1a0PvPMM3n00UeT/DmE/cxnPpN9\n9tmnzbUPO+ywJEl9fX02bNjQzk4BAKB3ELwCANDjzJ49O+Xl5XnmmWdy5513lqxGSwYOHJhhw4Yl\nSerq6jq0NnS0tm4bbLth6P7Gjh2bCRMmJGl8ruvjjz+eJ598MsnebzP8Ti+++GLT+wEDBrSrFgAA\n9BaCVwAAepyDDz4411xzTZJkwYIF2b59e0lqtGTr1q1Zv359kmTIkCEdVhc6Q1u/G+A7BdAz7ApY\n77nnntxxxx1JkmOOOSYf/OAH21xzx44dTVvpjxo1KgceeGD7GwUAgF5A8AoAQI907bXXZujQoXn+\n+efzne98p2Q1mrNo0aIUi8Ukycknn9xhdaEzlJd37Tyga332s5/NgAED8uqrr2bhwoVJ2r7ataGh\nIWvWrMm5556bmjf3G585c2aH9QoAAD2d4BUAgB6pvLw8s2fPTpJcf/312bJlS0lq7FIsFvPs+cTm\nHgAAIABJREFUs8/mpptuaqp5+OGH56yzzmpzTegKkyd37Tygaw0bNix/+Zd/maQxOC0rK8vUqVNb\nNfcDH/hAKisrm1777rtvxowZk5/85CdJkunTp+eqq67qtN4BAKCnEbwCANBjzZw5MxUVFVm3bl1u\nueWWLq+xfPnyFAqFFAqFlJWV5cgjj8ysWbOydevWjBgxIkuWLMk+++zTpr6gq4wdm0yatHdzqqoa\n5wE9w1tXuJ5xxhk59NBDWzVvw4YNWbduXdMrSUaOHJlPfepT+bd/+7csXrw4ZWV+tQQAALv0L3UD\nAADQVoMHD86XvvSlXH311bnxxhtzxRVX7PVz5tpTY8CAARk6dGiSpFAoZL/99suoUaNy5pln5u/+\n7u9y0EEH7fU9QSnMnZtMmZI0NOx5bFlZMmdO5/cE/FlNTVJd3fhs5fLyxhXne/Plh7/5m79p2gJ/\nT0477bRWjwUAAN7O1xIBAOjRLrvssowcOTKvvvpqvv71r3dpjYkTJ6a2tja1tbVZu3Zt/uM//iNL\nly7NrFmzhK70KJMnJ7ff3hiq7k5ZWXLHHbYZhq5SXd24wvy445Krr2780sPVVzf+uaqq8TwAANB9\nCF4BAOjRBg4cmDlvLr+7+eabs2HDhpLUgJ5uxoxk6dLGMKc5VVWN5y++uGv7gr5q0aLGlegrVjR/\nfsWKxvOLF3dtXwAAQMsErwAA9HjTp0/P6NGjU19fnxtuuKFkNaCnmzw5WbYs+c1vkm98I1mwoPHn\nb37TeNxKV+ga1dXJpZfuefvvhobkkkusfAUAgO5C8AoAQI/Xv3//zJs3L0ly6623Zu3atSWpAb3F\n2LHJVVcl//APjT/35lmSQPvNn9+6Zy4njeMWLOjcfgAAgNYRvAIA0Cucf/75GTNmTLZu3ZoHH3yw\nZDUAoD1qalreXrgly5c3zgMAAEpL8AoAQK9QVlaW+fPnl7wGALRHW7cNtt0wAACUnuAVAIBe45xz\nzsn48eNLXgMA2qqurmvnAQAAHadQLBZL3UOvUCgUVo0fP378qlWrSt0KAECPUFPTuDqnri4pL08m\nT/YcSQC45Zbk6qv3ft43vtH4TGYAAGDvTZgwIatXr15dLBYntKdO/45qCAAAWqO6Opk/v/nn102a\nlMyd2xjCAkBf1NbPQJ+dAABQerYaBgCgyyxalEyZ0nzomjQenzIlWby4a/sCgO5i7NjGLyLtjaoq\nu0YAAEB3IHgFAKBLVFcnl16aNDTsflxDQ3LJJY3jAaAvmjs3KWvlb2zKypI5czq3HwAAoHUErwAA\ndIn58/ccuu7S0JAsWNC5/QBAdzV5cnL77XsOX8vKkjvusM0wAAB0F4JXAAA6XU1Ny9sLt2T58sZ5\nANAXzZiRLF3auI1wc6qqGs9ffHHX9gUAALSsf6kbAACg92vrtsHV1Z5ZB0DfNXly46umpvEzsa4u\nKS9vPObzEQAAuh/BKwAAna6urmvnAUBvMnasoBUAAHoCWw0DANDpysu7dh4AAAAAdDXBKwAAnW7y\n5K6dBwAAAABdTfAKAECnGzs2mTRp7+ZUVdlWEQAAAICeQ/AKAECXmDs3KWvlvz7LypI5czq3HwAA\nAADoSIJXAAC6xOTJye237zl8LStL7rjDNsMAAAAA9CyCVwAAusyMGcnSpY3bCDenqqrx/MUXd21f\nAAAAANBe/UvdAAAAfcvkyY2vmpqkujqpq0vKyxuPeaYrAAAAAD2V4BUAgJIYO1bQCgAAAEDvYath\nAHqdadOmpVAopFAoZMKECbsdO3Xq1BQKhUybNq3Da7yzzltf5eXlOfHEEzNr1qy88MILLdZ++umn\nc9VVV2XcuHE54IADMnDgwIwcOTIf/OAHc/nll+eHP/xhXnnlld32BwAAAABA5xO8AtCrrV69Ovfd\nd1/JawwYMCAVFRWpqKjIIYccks2bN+eJJ57ITTfdlHHjxuXhhx9+15zbb789xx9/fL75zW/mN7/5\nTbZs2ZL99tsv69evz8qVK7Nw4cJ89rOfzV133dWu3gAAAAAAaD/BKwC93ty5c9PQ0FDSGhMnTkxt\nbW1qa2uzbt26bN68OXfddVeGDBmSjRs35txzz83WrVubxj/yyCO5/PLL8/rrr+cjH/lIli9fnm3b\ntuWVV17J1q1b87vf/S7f+ta38qEPfSiFQqFd9wYAAAAAQPsJXgHotaqqqjJ48ODU1NTkBz/4Qclq\nNGfw4MG54IILcssttyRJamtrs2TJkqbz3/zmN1MsFnP88cfn5z//eSZNmpR99tknSVIoFPLe9743\nn//85/Poo4/msssu67C+AAAAAABoG8ErAL1WZWVlrrzyyiTJvHnz8sYbb5Skxu6cd955KStr/Dhe\ntWpV0/Enn3wySfLxj388/fr1222NQYMGdWhPAAAAAADsPcErAL3a7NmzU15enmeeeSZ33nlnyWq0\nZODAgRk2bFiSpK6u7l3nX3zxxQ69HgAAAAAAnUPwCkCvdvDBB+eaa65JkixYsCDbt28vSY2WbN26\nNevXr0+SDBkypOn4+9///iTJ//yf/zP33Xdfh10PAAAAAIDOIXgFoNe79tprM3To0Dz//PP5zne+\nU7IazVm0aFGKxWKS5OSTT246Pnv27AwePDg7duzIJz/5yRxxxBGZPn16brvttqxatSo7d+7ssB4A\nAAAAAGg/wSsAvV55eXlmz56dJLn++uuzZcuWktTYpVgs5tlnn81NN93UVPPwww/PWWed1TRm7Nix\neeCBBzJ27NgkyXPPPZfvfve7ueKKK/L+978/Bx98cC6//PI8//zzbe4DAAAAAICOI3gFoE+YOXNm\nKioqsm7dutxyyy1dXmP58uUpFAopFAopKyvLkUcemVmzZmXr1q0ZMWJElixZkn322edtcz70oQ/l\nySefzLJly/Jf/+t/zaRJk1JeXp4k2bRpUxYuXJhx48bloYceatP9AAAAAADQcQSvAPQJgwcPzpe+\n9KUkyY033phNmzZ1aY0BAwakoqIiFRUVqayszOjRo3PmmWfma1/7WmpqanLiiSc2O69QKKSqqio3\n3HBDli9fnldeeSUPP/xwLrroohQKhWzatCmf/vSns3Xr1r2+HwAAAAAAOo7gFYA+47LLLsvIkSPz\n6quv5utf/3qX1pg4cWJqa2tTW1ubtWvX5j/+4z+ydOnSzJo1KwcddFCr6/Tr1y+nnHJKvvvd72b+\n/PlJkrVr1+bnP//5Xt8LAAAAAAAdR/AKQJ8xcODAzJkzJ0ly8803Z8OGDSWp0VFmzJjR9P53v/td\nyfoAAAAAAEDwCkAfM3369IwePTr19fW54YYbSlajI+y3335N79/5fFgAAAAAALqW4BWAPqV///6Z\nN29ekuTWW2/N2rVrS1JjT5YtW5adO3fudswPfvCDpvctPSMWAAAAAICuIXgFoM85//zzM2bMmGzd\nujUPPvhgyWrszhe+8IUcddRRmTdvXlauXJkdO3YkSRoaGvKHP/wh/+2//bdcddVVSRpD10mTJnV4\nDwAAAAAAtJ7gFYA+p6ysLPPnzy95jd0ZMGBAnn322XzlK1/JBz/4wQwaNChDhw7NoEGDMmrUqNxw\nww3ZsWNHjj322CxZsiT9+vXrtF4AAAAAANgzwSsAfdI555yT8ePHl7xGS37xi19kyZIlmTlzZv7i\nL/4iQ4cOTX19ffr165eRI0fmr/7qr7Jo0aL8n//zf3L44Yd3Sg8AAAAAALReoVgslrqHXqFQKKwa\nP378+FWrVpW6FYBep6Ymqa5O6uqS8vJk8uRk7NhSdwUAAAAAQG8wYcKErF69enWxWJzQnjr9O6oh\nAOho1dXJ/PnJihXvPjdpUjJ3bmMICwAAAAAApWarYQC6pUWLkilTmg9dk8bjU6Ykixd3bV8AAAAA\nANAcwSsA3U51dXLppUlDw+7HNTQkl1zSOB4AAAAAAEpJ8ApAtzN//p5D110aGpIFCzq3HwAAAAAA\n2BPBKwDdSk1Ny9sLt2T58sZ5AAAAAABQKoJXALqVtm4bbLthAAAAAABKSfAKQLdSV9e18wAAAAAA\noCMIXgHoVsrLu3YeAAAAvNW0adNSKBTe9SovL8+JJ56YWbNm5YUXXnjbnGeffbbZOQMGDEhFRUXO\nPPPM/NM//VPeeOONFq87b968Zmvsv//+OfbYY3PFFVfk6aef7uzbBwDaQfAKQLcyeXLXzgMAAIDm\n7ApNKyoqcsghh2Tz5s154oknctNNN2XcuHF5+OGHm5130EEHNc3bd99989JLL+WBBx7IJZdcktNP\nPz2vvfbabq9bVlbWNL+ioiLbtm3LU089ldtuuy0nnHBCfvzjH3fG7QLQC7X0ZaIDDjggY8eOzRVX\nXJE1a9a0OL+5uYVCIQMHDsxhhx2WT33qU/nf//t/d+EddX+CVwC6lbFjk0mT9m5OVVXjPAAAAOgo\nEydOTG1tbWpra7Nu3bps3rw5d911V4YMGZKNGzfm3HPPzdatW98177777muaV1dXlz/96U/5/Oc/\nnyR5+OGHM2/evN1ed+TIkU3za2tr89prr+Vf/uVfcuihh2b79u258MIL86c//akzbhmAXuqdXyZ6\n7bXX8tvf/ja33XZbTjzxxPzoRz/a7fzy8vK3fSkoSZ5//vnce++9+djHPpZrr722K26jRxC8AtDt\nzJ2blLXyE6qsLJkzp3P7AQAAgMGDB+eCCy7ILbfckiSpra3NkiVL9jhvxIgR+da3vpUzzzwzSfL9\n739/r667zz775C//8i/z3//7f0+SbN26Nd/73vf2snsA+rJ3fplo27Zt+dnPfpYjjjgir7/+eqZP\nn57169e3OP8b3/jG274UtG3bttTU1OQTn/hEkuQf//Efs2LFiq66nW5N8ApAtzN5cnL77XsOX8vK\nkjvusM0wAAAAXee8885L2Zv/wbpq1apWz5syZUqSxsD2lVde2evrTpo0Ke95z3v2+roA8E4DBgzI\nxz72saYv9WzZsiX33ntvq+cXCoWMGTMmP/rRjzJkyJAkyb/8y790Sq89jeAVgG5pxoxk6dLGbYSb\nU1XVeP7ii7u2LwAAAPq2gQMHZtiwYUmSurq6Vs8rFotN73fu3Nmma+8KXvfmugDQkg996EPZf//9\nkyS//e1v93r+4MGDM3r06CSN4S1J/1I3AAAtmTy58VVTk1RXJ3V1SXl54zHPdAUAAKAUtm7d2rQd\n465VPq2xdOnSJMn++++f4cOHt+naf/zjH/f6ugCwO7u+GNSWLwVt3bo1zzzzTJLkqKOO6tC+eirB\nKwDd3tixglYAAAC6h0WLFjX9kvrkk0/e4/i1a9fmq1/9ah544IEkydSpU9t03X/9139NbW1tq68L\nAHvy6KOPNq1UHTVq1F7NffrppzNr1qxs3LgxQ4cOzUUXXdQZLfY4glcAAAAAgN0oFot57rnn8uMf\n/zhz585Nkhx++OE566yz3jX2nHPOyT777JMkee2111JfX990bvz48fnqV7+6V9f+05/+lJ/97GeZ\nPXt2kqS8vNwvtwFolx07duTBBx/M5ZdfnqTxma+f/vSnWxx/9dVX54tf/GLTnzdu3Jjt27dn4MCB\nOeecc3L99ddn6NChnd53TyB4BQAAAAB4h+XLl6dQKDR7bsSIEVmyZElTwPpWr776arNzZsyYkVtv\nvbXZOW/13HPPtXjdAw88MPfcc0/TM2YBoDUeffTRVFZWJmn8MtGGDRvS0NCQJCkrK8vChQtz6KGH\ntji/rq6u2eeLv/7669m0aVNefvnlzmm8ByordQMAAAAAAN3NgAEDUlFRkYqKilRWVmb06NE588wz\n87WvfS01NTU58cQTm533i1/8IsViMcViMbW1tfnud7+bgw8+OIsXL873vve9PV63rKzsbdc98sgj\nU1VVlS9/+cv57W9/mylTpnT0rQLQy+3YsSPr1q3LunXr8tJLLzWFrkOHDs2///u/Z/r06budf+ed\ndzZ9thWLxdTX1+fxxx/PtGnTUl1dnTPOOCP3339/V9xKt2fFKwAAAADAO0ycODHLli1rV42Kiopc\ndNFFGT16dCZNmpQrr7wyH/jAB1oMbZNk5MiRefbZZ9t1XQB4q6qqqqbPtO3bt+epp57Kddddlx//\n+MeZMWNGli1bloMOOqjV9fbff/+ceOKJWbx4cbZu3Zof/vCHmTlzZmpqatKvX79OuouewYpXAAAA\nAIBOdOqpp2bq1Kl5/fXXc80115S6HQD6sIEDB+aEE07IPffck49+9KP5v//3/+ayyy5rc71p06Yl\nSZ5++uk88cQTHdRlzyV4BQAAAADoZH//93+fQqGQZcuW5YEHHih1OwD0cYVCIbfcckv69euXH/3o\nR1m+fHmb6hx22GFN73//+993VHs9luAVAAAAAKCTve9978tf//VfJ0muu+66EncDAMnRRx+dT3/6\n00kavyDUFi+++GLT+wEDBnRIXz2Z4BUAAAAAoAvMmjUrSbJ8+fI8/PDDJe4GAJIvfOELSZJHHnmk\nTc82v+eee5ren3TSSR3VVo8leKVXueOOOzJv3rw89NBDnTIeAAAAANrqlFNOycSJE5MkCxYsKHE3\nANAYln7kIx9Jsnc7Mqxbty5f+tKX8k//9E9JkrPPPvtt2w73VYJXepUJEybkxhtvzH/+z//5bcvb\nO2o8AAAAALTH7NmzkyRLly7NypUrS9wNAPz5s6m6ujq/+tWv3nX+6quvTmVlZdPrgAMOSGVlZa6/\n/voUi8WMHz8+ixYt6uq2u6VCsVgsdQ+9QqFQWDV+/Pjxq1atKnUrfd6SJUvyyU9+MqeeemoefPDB\n9OvXr0PHAwAAANCz1NQk1dVJXV1SXp5MnpyMHVvqrgBg7+zt59m0adPyve99L1VVVXvcRnj8+PF5\n/PHH84lPfCL/+q//miQpFArNju3fv38OOuigHH/88TnvvPMyffr0Hv981wkTJmT16tWri8XihPbU\nEbx2EMFr93LjjTdm9uzZmTt3br7yla90+HgAAAAAur/q6mT+/GTFinefmzQpmTu38ZfWANCd+Tzr\nfB0VvNpqmF5p1qxZmTFjRq677rpWPQx6b8cDAAAA0L0tWpRMmdL8L6mTxuNTpiSLF3dtXwCwN3ye\n9SxWvHYQK14BAAAAoHuorm78JXRDw57HlpUlS5daKQRA9+PzrOtY8QoAAAAA0Iz581v3S+qkcdyC\nBZ3bDwC0hc+znkfwCgAAAAD0GjU1LW/H2JLlyxvnAUB34fOsZxK8AgAAAAC9RnV1184DgM7g86xn\nErwCAAAAAL1GXV3XzgOAzuDzrGcSvAIAAAAAvUZ5edfOA4DO4POsZxK8AgAAAAC9xuTJXTsPADqD\nz7OeSfAKAAAAAPQaY8cmkybt3ZyqqsZ5ANBd+DzrmQSvAAAAAECvMnduUtbK33yWlSVz5nRuPwDQ\nFj7Peh7BKwAAAADQq0yenNx++55/WV1Wltxxh20ZAeiefJ71PIJXAAAAAKDXmTEjWbq0cdvF5lRV\nNZ6/+OKu7QsA9obPs56lf6kbAAAAAADoDJMnN75qapLq6qSuLikvbzzmGXgA9BQ+z3oOwSsAAAAA\n0KuNHesX0wD0fD7Puj9bDQMAAAAAAAC0kxWvdGuWzQMAAAAAANATCF7plqqrk/nzkxUr3n1u0qRk\n7tzGEBYAAAAAAAC6A1sN0+0sWpRMmdJ86Jo0Hp8yJVm8uGv7AgAAAAAAgJYIXulWqquTSy9NGhp2\nP66hIbnkksbxAAAAAAAAUGqCV7qV+fP3HLru0tCQLFjQuf0AAAAAAABAawhe6TZqalreXrgly5c3\nzgMAAAAAAIBSErzSbbR122DbDQMAAAAAAFBqgle6jbq6rp0HAAAAAAAAHUXwSrdRXt618wAAAAAA\nAKCjCF7pNiZP7tp5AAAAAAAA0FEEr3QbY8cmkybt3ZyqqsZ5AAAAAAAAUEqCV7qVuXOTslb+v7Ks\nLJkzp3P7AQAAAAAAgNYQvNKtTJ6c3H77nsPXsrLkjjtsMwwAAAAAAED3IHil25kxI1m6tHEb4eZU\nVTWev/jiru0LAAAAAAAAWtK/1A1AcyZPbnzV1CTV1UldXVJe3njMM10BAAAAAADobgSvdGtjxwpa\nAQAAAAAA6P5sNQwAAAAAAADQToJXAAAAAAAAgHYSvAIAAAAAAAC0k+AVAAAAAAAAoJ0ErwAAAAAA\nAADtJHgFAAAAAAAAaCfBKwAAAAAAAEA7CV4BAAAAAAAA2knwCgAAAAAAANBOfS54LRQKhxYKhcWF\nQuFPhUJhe6FQeLZQKNxcKBQOKnVvAAAAAAAAQM/Uv9QNdKVCoTA6yaNJDknykyRPJflgkquTfKxQ\nKJxSLBZfLmGLAAAAAAAAQA/U11a83prG0PWqYrH4N8Vi8YvFYvGMJP+Y5H1J/t+SdgcAAAAAAAD0\nSH0meH1zteuUJM8m+fY7Tn85yZYkFxQKhf26uDUAAAAAAACgh+szwWuS09/8ubRYLDa89USxWKxP\n8kiSwUn+oqsbAwAAAAAAAHq2vvSM1/e9+fN3LZz//9K4IvboJNUtFSkUCqtaOHVM21sDAAAAAAAA\nerK+tOL1wDd/bmrh/K7jQ7qgFwAAAAAAAKAX6UsrXjtEsVic0NzxN1fCju/idgAAAAAAAIBuoC+t\neN21ovXAFs7vOr6xC3oBAAAAAAAAepG+FLw+/ebPo1s4/943f7b0DFgAAAAAAACAZvWl4PUXb/6c\nUigU3nbfhULhgCSnJHktya+6ujEAAAAAAACgZ+szwWuxWHwmydIkRyT5/DtOfyXJfkm+XywWt3Rx\nawAAAAAAAEAP17/UDXSxK5I8muSWQqEwOcmaJCcnOT2NWwz/fQl7AwAAAAAAAHqoPrPiNWla9fr+\nJN9NY+D6/yQZneQbSf6iWCy+XLruAAAAAAAAgJ6qr614TbFYfD7J9FL3AQAAAAAAAPQefWrFKwAA\nAAAAAEBnELwCAAAAAAAAtJPgFQAAAAAAAKCdBK8AAAAAAAAA7SR4BQAAAAAAAGgnwSsAAAAAAABA\nOwleAQAAAAAAANpJ8AoAAAAAAADQToJXAAAAAAAAgHYSvAIAAAAAAAC0k+AVAAAAAAAAoJ0ErwAA\nAAAAAADtJHgFAAAAAAAAaCfBKwAAAAAAAEA7CV4BAAAAAAAA2knwCgAAAAAAANBOglcAAAAAAACA\ndhK8AgAAAAAAALST4BUAAAAAAACgnQSvAAAAAAAAAO0keAUAAAAAAABoJ8ErAAAAAAAAQDsJXgEA\nAAAAAADaSfAKAAAAAAAA0E6CVwAAAAAAAIB2ErwCAAAAAAAAtJPgFQAAAAAAAKCdBK8AAAAAAAAA\n7SR4BQAAAAAAAGgnwSsAAAAAAABAOwleAQAAAAAAANpJ8AoAAAAAAADQToJXAAAAAAAAgHYSvAIA\nAAAAAAC0k+AVAAAAAAAAoJ0ErwAAAAAAAADtJHgFAAAAAAAAaCfBKwAAAAAAAEA7CV4BAAAAAAAA\n2knwCgAAAPQa69evT6FQSKFQyE9+8pMWx33uc59rGnffffe1OG7mzJkpFAo57rjjmo4dccQRTXN3\nvfr165eDDjooJ598cr7yla/klVdeabHmsmXL3jV/12u//fbLsccem8svvzxr1qxp2/8IAABASQhe\nAQAAgF5j+PDhOeaYY5IkK1asaHHcW8+1ZlxVVdW7zu23336pqKhIRUVFhgwZko0bN+axxx7LvHnz\nctxxx+Xpp5/eY7/Dhg1rqjF8+PBs27YtTz31VBYuXJgTTjgh99577x5rAAAA3YPgFQAAAOhVdoWk\nLQWqL7/8ctasWZOKiordjtu4cWN+85vfJEkmTZr0rvNf+MIXUltbm9ra2rz88supr6/Pt7/97Qwa\nNChr167NhRdeuMdeV65c2VTjpZdeyvbt21NdXZ2jjz46O3bsyIwZM1JfX9+q+wYAAEpL8AoAAAD0\nKrtC0scffzybN29+1/mHHnooxWIxn/jEJ/K+970vTzzxROrq6pod19DQkKT5Fa/vtP/+++eKK67I\nnDlzkiSPPfZYnnrqqb3qvX///jnjjDNy5513Jkk2bdqUhx56aK9qAAAApSF4BQAAAHqVXSHpzp07\n88gjj7zr/K4g88Mf/nBOPfXUNDQ07Hbc0UcfncrKylZff8qUKU3vf/vb3+5V77scf/zxTe+3bNnS\nphoAAEDXErwCAAAAvcp73vOejBo1Kknz2wjvOvbhD384H/7wh/c4rrlthnenWCw2vd+5c+dezd3l\nySefbHp/1FFHtakGAADQtfqXugEAAACAjlZVVZXf//737wpUN2/enMcffzyVlZU56qijUigUkrw7\neH3ttdeyevXqplp7Y+nSpU3vdwXArbVz5848/PDDueyyy5qufdJJJ+1VDQAAoDSseAUAAAB6nV2r\nVFeuXJlt27Y1HX/00Uezc+fOppWuo0ePzogRI/LrX/86W7dufdu4HTt2JGl98Lp58+bcdtttue66\n65IkY8aMyfjx43c75wMf+EAqKytTWVmZQw45JAMHDsxpp52Wl19+OVdeeWV++tOftv6mAQCAkhK8\nAgAAAL3OrrB0+/bt+fd///em47ue2/rW7YNPPfXUvP76682OO+KIIzJy5Mhmr3HTTTc1habDhg3L\nAQcckCuuuCLbtm3L0KFDc/fddzetqG3Jhg0bsm7duqxbty7r169v2pp48+bN2bhxY+rr69tw9wAA\nQCkIXgEAADrQtGnTUigUMmbMmFbP+fa3v51CoZBBgwZl48aNWbZsWQqFQrOv/fbbL8cee2wuv/zy\nrFmzptl6p512Wovzd/eC3uTII4/MoYcemuTt2wi/9fmuu5x66qktjtvdatctW7Y0haYvv/xy0/EJ\nEybkqaeeatUWwX/4wx9SLBabXi+99FIefPDBTJgwIXfffXcmTpyYF154oTW3DAAAlJjgFQAAoANd\ndNFFSZI1a9bk17/+davm3HXXXUmSs88+O0OGDHnbuWHDhqWioiIVFRUZPnx4tm3blqfX3RWhAAAg\nAElEQVSeeioLFy7MCSeckHvvvfdd9YYOHdo0Z0+vXQYOHNjWW4Zua9eq1l0h6uuvv57HHnssBx54\nYMaNG9c0blcI+9Zxu1a/7i54/fKXv9wUmG7atCn3339/TjzxxKxatSrXXnttm3oePnx4Tj/99Nx/\n//0ZNWpUnnvuucybN69NtQAAgK4leAUAAOhAp512Wg4//PAkfw5Ud+fpp5/OY489luTPoe1brVy5\nMrW1tamtrc1LL72U7du3p7q6OkcffXR27NiRGTNmvGsr0vvuu69pzu5eX/7yl5vm3Hzzze25beiW\ndoWmv/zlL/PGG2/ksccey7Zt23LKKaekrOzPvxI5/vjjc8ABB+RXv/pVduzYkZUrVzY97/WtWxLv\nTnl5eT7ykY/kgQceyIgRI3L33Xfn1ltvbXPv++67b84777wkyT333NPmOgAAQNcRvAIAAHSgQqGQ\nCy64IEnywx/+MG+88cZux+8KZysrK/PRj350j/X79++fM844I3feeWeSZNOmTU3PotwbK1euzH/5\nL/8lSTJ16tRcfvnle10DurtdoemWLVuyatWqpr8rb91mOEn69euXD33oQ9myZUtWr17dNO4973lP\nRo8evVfXPPjgg3PdddclSf7hH/4hr776apv7P+yww5Ik9fX12bBhQ5vrAAAAXUPwCgAA0MEuvPDC\nJMn69evzs5/9rMVxxWIxd999d5Lkb//2b9OvX79WX+P4449ver9ly5a96u+VV17Jueeem9dffz3H\nHXdcFi5cuFfzoac45phjmrbUXrFiRdNWws2tYn3rdsOteb7r7lx44YU57LDD8uqrr+brX/96m2ok\nyYsvvtj0fsCAAW2uAwAAdA3BKwAAQAd773vfm4kTJybZ/XbDy5Ytyx//+MckzW8zvDtPPvlk0/uj\njjqq1fOKxWKmTp2a5557LgcccEDuvffeDB48eK+uDT3JrkB12bJlefTRRzNo0KC8//3vf9e4U089\ntWncI488kqT12wy/U//+/XPNNdckSb71rW9l06ZNe11jx44dWbJkSZJk1KhROfDAA9vUCwAA0HUE\nrwAAAJ1gV5D605/+NBs3bmx2zK5Q9qSTTsq4ceNaVXfnzp1Zvnx5pk+fnqRxRd5JJ53U6r6uu+66\nplW4ixcvztFHH93qudAT7Vq1+vOf/zx1dXU5+eSTs88++7xr3Mknn5wBAwY0jXvr3Lb4u7/7uxx0\n0EHZtGlTvvnNb7Z6XkNDQ9asWZNzzz03NTU1SZKZM2e2uQ8AAKDrCF4BAAA6wXnnnZdBgwZl+/bt\nueeee951/rXXXsu9996bZPerXT/wgQ+ksrIylZWVOeSQQzJw4MCcdtppefnll3PllVfmpz/9aat7\neuCBBzJv3rwkyTXXXJNPfepTe3dT0APtWrXa0NCQ5N3Pd91l3333zYQJE5rGVVRU5Jhjjmnzdfff\nf/987nOfS5LcfPPN2bx5c7Pj3vp3vLKyMvvuu2/GjBmTn/zkJ0mS6dOn56qrrmpzHwAAQNcRvAIA\nAHSCIUOG5Oyzz07S/HbD//zP/5z6+vr0798/559/fot1NmzYkHXr1mXdunVZv359du7cmSTZvHlz\nNm7cmPr6+lb188ILL+T8889PQ0NDTjnllHzta19rw11BzzNu3LgMHTq06c8tBa/vPLe7ca111VVX\nZdCgQXn55Zdz2223NTvmrX/H161blyQZOXJkPvWpT+Xf/u3fsnjx4pSV+fUNAAD0BIVisVjqHnqF\nQqGwavz48eNXrVpV6lYAAIBu4uc//3k+/vGPJ0meeeaZjBo1quncRz/60SxdujRnnXVW/tf/+l9v\nm7ds2bKcfvrpSZI//OEPOeKII5rOrV+/Pr/5zW8yZ86cPPLIIzn88MPz8MMP59BDD22xjx07dqSq\nqiq//OUvc8ghh+Txxx/Pf/pP/6kD7xQ6V01NUl2d1NUl5eXJ5MnJ2LGl7goAAOgtJkyYkNWrV68u\nFosT2lPHVyYBAAA6yZlnnpkRI0YkSb7//e83HV+7dm2qq6uT7H6b4eYMHz48p59+eu6///6MGjUq\nzz33XNP2wS35whe+kF/+8pfp169f/sf/+B9CV3qM6uqkqio57rjk6quTOXMafx53XOPxN/8aAQAA\ndAuCVwAAgE7Sr1+/TJ06Ncnbg9e77747O3fuzNChQ3PWWWe1qfa+++6b8847L0mafYbsLj/60Y9y\nyy23JEkWLFiQM844o03Xg662aFEyZUqyYkXz51esaDy/eHHX9gUAANASwSsAAEAn2rWi9Zlnnsmj\njz6a5M8h7Gc+85nss88+ba592GGHJUnq6+uzYcOGd51/+umnM2PGjCTJX/3VX+WLX/xim68FXam6\nOrn00qShYffjGhqSSy6x8hUAAOgeBK8AAACdaOzYsZkwofERMXfddVcef/zxPPnkk0n2fpvhd3rx\nxReb3g8YMOBt51577bV88pOfTH19fY488sjcddddKRQK7boedJX58/ccuu7S0JAsWNC5/QAAALRG\n/1I3AAAA0NtddNFFWbVqVe65556UlTV+//WYY47JBz/4wTbX3LFjR5YsWZIkGTVqVA488MC3nb/0\n0ktTU1OTQYMG5d57781BBx3U9huALlRT0/L2wi1Zvrxx3tixndMTAABAa1jxCgAA0Mk++9nPZsCA\nAXn11VezcOHCJG1f7drQ0JA1a9bk3HPPTU1NTZJk5syZbxtz223/P3v3Hl1XWecB/7vT+y2Fcikg\nSAcYUaJy8zLg0BSCVfGGlaGoRVquIw4gDLAcpHk7LTguFRhgGJXaCjNcFBScAeQFiROq1lGhKBoQ\nHS6K1ZZW2gZaoNDs949AXqBpm+Q0OQn5fNba65yz9/N79m93ra42+Z797K/k2muvTZJcfvnl2X//\n/SvoHvpWT5cNttwwAABQbe54BQAA6GXbb7993v/+9+e73/1u2traUlNTkxkzZnSp9u1vf3uGDBnS\n8XnVqlVZv359x+dZs2bl9NNPf0XNZz7zmY73559/fs4///wuneumm27KwQcf3KWx0FtaW/u2DgAA\nYGsRvAIAAPSB4447rmNp4MMOOyy77rprl+pWrlz5is/Dhw/Pbrvtlne+8505/vjj8773vW+jmpcH\ns8uXL+9yjy+vg2qpre3bOgAAgK1F8AoAANAFLS3tS5m2trYHPA0N3Xue5JFHHpmyLLs0dsqUKV0e\n25lKaqHaGhr6tg4AAGBrEbwCAABsRlNTMndusmjRxscmT04aGwU+sDXV1bX/3ers79ym1Nd374sQ\nAAAAvaGm2g0AAAD0VwsWJFOnbjoAWrSo/fjChX3bF7zWNTYmNV38jUVNTTJ7du/2AwAA0BWCVwAA\ngE40NSUnn5y0tW1+XFtbctJJ7eOBraOhIbnyyi2HrzU1yfz57joHAAD6B8ErAABAJ+bO3XLo+pK2\ntmTevN7tBwabE05I7ryzfRnhztTXtx8//vi+7QsAAGBTPOMVAADgVVpauvd8ySS5++72Os+ZhK2n\noaF9a2lpv6u8tTWprW3f5+8aAADQ3wheAQAAXqWnywY3NQmDoDfU1fm7BQAA9H+WGgYAAHiV1ta+\nrQMAAAAGPsErAADAq9TW9m0dAAAAMPAJXgEAAF6loaFv6wAAAICBT/AKAADwKnV1yeTJ3aupr/cM\nSgAAABjMBK8AAACdaGxMarr4E1NNTTJ7du/2AwAAAPRvglcAAIBONDQkV1655fC1piaZP98ywwAA\nADDYCV4BAAA24YQTkjvvbF9GuDP19e3Hjz++b/sCAAAA+p+h1W4AAACgP2toaN9aWpKmpqS1Namt\nbd/nma4AAADASwSvAAAAXVBXJ2gFAAAANs1SwwAAAAAAAAAVErwCAAAAAAAAVEjwCgAAAAAAAFAh\nwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAA\nAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAAAAAAVEjw\nCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAA\nAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAA0ItmzpyZ\noigyZcqUV+yfM2dOiqLIpEmTNqq56qqrUhTFRtuoUaMyadKkHH300fn+97/fNxcAQJcIXgEAAAAA\noB+bOHFix1aWZX7/+9/nxhtvzNSpU3PWWWdVuz0AXiR4BQAAAACAfmzZsmUd27p163L//ffn0EMP\nTZJccskl+d73vlflDgFIBK8AAAAAADBg1NTU5C1veUtuvvnm7LDDDkmS//iP/6hyVwAkglcAAAAA\nABhwxo8fn3e84x1JkgceeKDK3QCQCF4BAAAAAGBAKssySbJhw4YqdwJAIngFAAAAAIABZ/Xq1fnZ\nz36WJNljjz2q3A0AieAVAAAAAAAGjLIs86tf/Sof/ehHs3LlyiTJjBkzqtwVAEkytNoNAAAAAAAA\nm7bTTjt1vF+9enWee+65js/HH398jj766Gq0BcCrCF4BAAAAAKAfW758+Ub7ampq8rWvfS0nnnhi\nFToCoDOWGgYAqEBzc3OKokhRFGlubq52OwAAALwGlWWZsizzwgsv5NFHH83s2bOTJGeffXbuvffe\nKncHwEsErwAAAAAAMAAMGTIkkyZNyty5czNv3rysWbMmRx99dNauXVvt1gCI4BUAAAAAAAacc845\nJ3vssUceeeSRfPnLX652OwBE8AoAUJEpU6Z0LPk0ZcqUarcDAADAIDFs2LB89rOfTZJcdNFFWbVq\nVZU7AkDwCgAAAAAAA9AnP/nJTJw4MU899VQuvfTSarcDMOgJXgEAAAAAYAAaMWJETjvttCTJZZdd\nlqeeeqrKHQEMboJXAAAAAADoA0VRdGt/V5x66qkZO3ZsVq1alX/7t3/r8TwAVE7wCgAAAAAAvWj9\n+vVJklGjRnVpf3dsu+22OfHEE5Mkl1xySdatW9fjuQCojOAVAAAAAAC6qKUlueyy5IIL2l9bWrZc\ns3z58iTJ9ttv36X9STJz5syUZZmyLLc4/yWXXJKyLPPEE09k9OjRXbgKAHrD0Go3AAAAAAAA/V1T\nUzJ3brJo0cbHJk9OGhuThoaNjz3zzDO55557kiT77rtvx/6yLPOjH/1oo/0ADFzueAUAqEBzc3OK\nokhRFGlubq52OwAAAPSCBQuSqVM7D12T9v1TpyYLF75y/4oVK3LccceltbU1Q4YMybRp05Ika9as\nyZlnnpnf/va3SZKjjz66N9sHoI+44xUAAAAAADahqSk5+eSkrW3z49rakpNOSnbfPRk1anE++MEP\n5sknn+w4fv7552fo0KHZYYcdsnLlyo79xx13XA455JDeah+APuSOVwAAAAAA2IS5c7ccur6krS2Z\nNy9Zv359Vq1alfHjx2fy5Mm5/vrrM2fOnGzYsCErV67M2LFj8453vCNXXHFFFr76NlkABix3vAIA\nvExLS/u3mVtbk9ra9ufz1NVtevyUKVNSlmXfNQgAAECfaWnZ9PLCm3L33ckOO0xJWydp7aRJk/wM\nCfAaJngFAEh72Dp3buc/UE+enDQ2toewAAAADB5NTT2v29yXeAF4bbLUMAAw6C1YkEyduulvMS9a\n1H7c6k8AAACDS2tr39YBMLAJXgGAQa2pKTn55C0/r6etLTnppJ5/2xkAAICBp7a2b+sAGNgErwDA\noDZ37pZD15e0tSXz5vVuPwAAAPQfPX3kjEfVAAxOglcAYNBqadn08sKbcvfd7XUAAAC89tXVJZMn\nd6+mvt7zXQEGK8ErADBo9XTZYMsNAwAADB6NjUlNF3+TXlOTzJ7du/0A0H8JXgGAQau1tW/rAAAA\nGHgaGpIrr9xy+FpTk8yfb5lhgMFM8AoADFq1tX1bBwAAwMB0wgnJnXe2LyPcmfr69uPHH9+3fQHQ\nvwytdgMAANXS028h+/YyAADA4NPQ0L61tLQ/gqa1tf2LuQ0NnukKQDvBKwAwaNXVJZMnJ4sWdb2m\nvt4P1AAAAINZXZ2fCwHonKWGAYBBrbFxy8/peUlNTTJ7du/2AwAAAAAMTIJXAGBQa2hIrrxyy+Fr\nTU0yf75lhgEAAACAzgleAYBB74QTkjvvbF9GuDP19e3Hjz++b/sCAAAAAAYOz3gFAEj7nawNDUlL\nS9LUlLS2JrW17fs8uwcAAAAA2BLBKwDAy9TVCVoBAAAAgO6z1DAAAAAAAABAhQSvAAAAAAAAABUS\nvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAA\nAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAAAAAAVEjwCgAAAAAAAFAhwSsAAAAAAABAhQSv\nAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAA\nAFRI8AoAAAAArzFTpkxJURRpbm7u2Nfc3JyiKDJlypSq9QUA8FomeAUAAAAAAACokOAVAAAAAAAA\noEKCVwAAAAAAAIAKCV4BAAAAAAAAKiR4BQAAAAAAAKiQ4BUAAAAAAACgQkOr3QAAAAAAsHU1Nzdv\ntG/KlCkpy7LvmwEAGCTc8QoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAAAAAAVEjw\nCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAA\nAECFhla7AQAAAABg81pakqampLU1qa1NGhqSurpqdwUAwMsJXgEAAACgn2pqSubOTRYt2vjY5MlJ\nY2N7CAsAQPVZahgAAAAA+qEFC5KpUzsPXZP2/VOnJgsX9m1fAAB0TvAKAAAAAP1MU1Ny8slJW9vm\nx7W1JSed1D4eAIDqErwCAAAAQD8zd+6WQ9eXtLUl8+b1bj8AAGyZ4BUAAAAA+pGWlk0vL7wpd9/d\nXgcAQPUIXgEAAACgH+npssGWGwYAqC7BKwAAAAD0I62tfVsHAMDWIXgFAAAAgH6ktrZv6wAA2DoE\nrwAAAADQjzQ09G0dAABbh+AVAAAAAPqRurpk8uTu1dTXt9cBAFA9glcAAAAA6GcaG5OaLv7mrqYm\nmT27d/sBAGDLBK8AAAAA0M80NCRXXrnl8LWmJpk/3zLDANCfzZw5M0VRbLSNGzcudXV1OfXUU/Pg\ngw9usr6z2qIoMmLEiLz+9a/PUUcdlTvuuGOLfdx000058sgjs9tuu2XEiBGpra3NG97whhx++OGZ\nM2dOmpubU5bl1rz0QafwB7h1FEVx7wEHHHDAvffeW+1WAAAAAHiNaGpK5s1L7r5742P19e13ugpd\nAaB/mzlzZq6++uoMGzYsEyZMSJKUZZmVK1emra0tSTJ8+PBcc801+bu/+7uN6ouiSJLU1tZm1KhR\nHftXrVqV9evXd3w+88wzc/HFF29Uv27duhx11FG5/fbbO/YNHz48Y8aMyZo1azp6eGnObbbZpsIr\nHngOPPDALFmyZElZlgdWMo87XgEAAACgn2poSJqbk1//Orn00vYQ9tJL2z83NwtdAWAgOfjgg7Ns\n2bIsW7Ysy5cvz7PPPpvbb789kyZNyvr16zNr1qysWLFik/WXXnppR/2yZcvy7LPPpqWlJUcccUSS\n5JJLLsmiRYs2qjvzzDNz++23Z9iwYTnvvPPy2GOP5dlnn82TTz6Zp59+Oj/84Q9zzjnnZOLEib12\n7YOF4BUAAAAA+rm6uuT005Pzz29/raurdkcAQKWGDRuW9773vbn22muTJGvXrs13vvOdLtcXRZF9\n9tknN954Y8ddqrfeeusrxrS2tuaqq65Kknz+85/PhRdemN13373jLtpRo0blb//2b/PFL34xf/jD\nH1JbW7sVrmzwErwCAAAAAABAlRx00EEZO3ZskuSBBx7odv3o0aOz5557JmkPb1/uoYce6liO+AMf\n+MBm5xk+fHhqtvSAeTbLnx4AAAAAAABUUVmWSZINGzZ0u/aZZ57Jww8/nCTZa6+9Njlu6dKlPWuO\nLhO8AgAAAAAAQJUsXry4407VPfbYo1u1Dz30UKZPn57Vq1dnwoQJOe64415xvK6uLqNGjUqSnHPO\nOXnssce2Ss90TvAKAAAAAAAAfez555/PHXfckRkzZiRpf+br9OnTNzn+jDPOyE477dSxjRw5Mm98\n4xtz5513Ztq0afnJT36SCRMmvKJm9OjROeecc5Ik9913X/bcc8+8613vyjnnnJMbb7wxjz/+eO9d\n4CA0tNoNAAAAAAAAwGvd4sWLs9NOOyVpX1p45cqVaWtrS5LU1NTka1/7WnbddddN1re2tqa1tXWj\n/evXr8+aNWvyl7/8pdO6OXPmZOTIkfn85z+fp59+OosXL87ixYs7ju+zzz751Kc+lVNOOSXDhg2r\n5BIHPXe8AgAAAAAAQC97/vnns3z58ixfvjxPPPFER+g6YcKE/PSnP82sWbM2W/+Nb3wjZVl2bE89\n9VTuu+++zJw5M01NTTnssMPy/e9/f6O6oijyT//0T1m6dGmuvvrqzJo1K3V1dRkyZEiS5IEHHshp\np52Www47LOvWrdv6Fz6ICF4BAAAAAACgl9XX13eEps8++2x+8Ytf5KijjsqTTz6ZE044IatWrerW\nfGPHjs1+++2XhQsX5phjjsmzzz6b0047LRs2bOh0fG1tbT75yU9m4cKF+fWvf52VK1fm+uuvT11d\nXZLkRz/6UT73uc9VfJ2DmeAVAAAAAAAA+tCIESOy77775oYbbsh73vOe3H///TnllFN6PN/MmTOT\nJA899FB++ctfdqlmm222yTHHHJN77rmnI3y9+uqrO+7EpfsErwAAAAAAAFAFRVHksssuy5AhQ3Lj\njTfm7rvv7tE8r3/96zveP/LII92qHTlyZD7xiU8kSVatWpUVK1b0qAcErwAAAAAAAFA1b3jDGzJ9\n+vQk6fFSv0uXLu14P2zYsG7XjxkzpuP98OHDe9QDglcAAAAAAACoqrPPPjtJ8uMf/zjNzc3drr/h\nhhs63u+///4d71euXJlf/OIXm61ta2vLt771rSTJ7rvvnm233bbb56ed4BUAAAAAAACqaP/998/h\nhx+eJLngggu6XLd8+fKcd955+frXv54k+fCHP/yKZYeXLVuW/fffP+9+97tz1VVX5fe//33HsWef\nfTbNzc2ZOnVqFi9enCQ5/fTTt8blDFpDq90AAAAAAAAADHbnnntu7rrrrjQ1NeV///d/8zd/8zev\nOH7GGWfks5/9bMfntWvX5umnn+74fMABB2TBggWvqBk6dGiKoshdd92Vu+66K0kyYsSIjB49OqtW\nrXrF2E9/+tP5zGc+s7Uva1ARvAIAAAAAAECVvfvd787++++f++67L/Pmzcttt932iuOtra1pbW3t\n+Dx06NDssMMOeetb35qjjz46s2bN2uj5rm984xvz+9//PrfccksWLVqU+++/P48//nhaW1szbty4\nTJo0KQcddFBmzZq1UdBL9xVlWVa7h9eEoijuPeCAAw649957q90KAAAAAAAAvaSlJWlqSlpbk9ra\npKEhqaurdldU4sADD8ySJUuWlGV5YCXzuOMVAAAAAAAAtqCpKZk7N1m0aONjkycnjY3tISyDV021\nGwAAAAAAAID+bMGCZOrUzkPXpH3/1KnJwoV92xf9i+AVAAAAAAAANqGpKTn55KStbfPj2tqSk05q\nH8/gJHgFAAAAAACATZg7d8uh60va2pJ583q3H/ovwSsAAAAAAAB0oqVl08sLb8rdd7fXMfgIXgGo\nyLp16/KVr3wlH/zgB/P6178+o0ePzpgxY/JXf/VXOeqoo3LNNdfkmWeeeUXNpEmTUhTFK7aRI0dm\n4sSJefOb35xjjz02X/3qV7N69eoqXRUAAAAAQM+XDbbc8OAkeAWgx2655ZbsueeeOfXUU3Prrbfm\n8ccfT01NTYYMGZLHHnss3/nOd3Lsscdmr732yg9+8ION6seMGZOJEydm4sSJGTduXFatWpWWlpZc\nc801+dSnPpVddtkls2fPzgsvvFCFqwMAAAAABrvW1r6tY2ATvALQI1dddVWOPPLILFu2LHvvvXf+\n8z//MytXrszTTz+d1tbWrF69Ot/+9rczZcqU/OlPf8qiTtbjOPvss7Ns2bIsW7YsK1asyPr16/P4\n44/nmmuuyUEHHZRnnnkmF1xwQd73vvcJXwEAAACAPldb27d1DGyCVwC67Ze//GX+/u//Pm1tbTni\niCNy3333ZcaMGdluu+06xowfPz4f/ehH8z//8z/55je/mXHjxnVp7l133TWf+MQn8uMf/zj//M//\nnCS566678rnPfa5XrgUAAAAAYFMaGvq2joFN8ApAt51//vl57rnn8rrXvS7XXXddRo0atdnx06dP\nz1lnndWtcxRFkcbGxhx11FFJkssvvzxPPPFEj3sGAAAAAOiuurpk8uTu1dTXt9cx+AheAeiWpUuX\n5rbbbkuSnH766Rk/fnyX6oqi6NH5zj///CTJM888k5tvvrlHcwAAAAAA9FRjY1LTxUStpiaZPbt3\n+6H/ErwC0C3Nzc0pyzJJ8qEPfajXz7fvvvtm5513TpL88Ic/7PXzAQAAAAC8XENDcuWVWw5fa2qS\n+fMtMzyYCV4B6JYHH3wwSTJixIjsvffefXLOt7zlLUmSRx99tE/OBwAAAADwcieckNx5Z/sywp2p\nr28/fvzxfdsX/cvQajcAwMDyl7/8JUmy7bbb9nj54O6aMGFCkuTJJ5/sk/MBAAAAALxaQ0P71tKS\nNDUlra1JbW37Ps90JRG8AjAAvLS0MQAAAABAtdXVCVrpnKWGAeiW7bbbLkmyasfcliIAACAASURB\nVNWqPgtEV61aleT/v/MVAAAAAAD6G8ErAN3ypje9KUny3HPP5aGHHuqTc95///1Jkj322KNPzgcA\nAAAAAN014ILXoigmFUVRbmb75mZqjyuK4mdFUTxdFMWaoiiai6L4QF/2DzDQ1dfXdzzb9b//+797\n/Xy/+MUvsmzZsiTJIYcc0uvnAwAAAACAnhhwwevL/DLJP3eyfbuzwUVRfDnJVUl2TjI/yTVJ3pLk\nlqIo/qEP+gV4Tdh1111zxBFHJEkuv/zytLa2dqmura2tR+e78MILkySjR4/ORz7ykR7NAQAAAAAA\nvW1otRuowC/KspzTlYFFURyc5B+TPJzk7WVZrnpx/5eS3Jvky0VR3FqW5WO91CvAa8oFF1yQu+66\nK3/84x/z8Y9/PN/+9rczcuTITY7/5je/maVLl+Yf//Efu3yOsixzwQUX5Nvfbv8+zRlnnJEddtih\n4t4BAAAAAKA3DOQ7Xrvj7198vfCl0DVJXgxar0gyIsmsKvQFMCDtt99+ueKKK1IURW677bbsv//+\nueaaa/Lkk092jFmzZk1uuummHHroofnYxz6Wp556qktzL126NNddd13e9a53pbGxMUnynve8J3Pn\nzu2VawEAGOhmzpyZoihSFEUOPPDAzY6dMWNGiqLIzJkzt/ocL3nhhRdy1VVX5b3vfW923nnnDB8+\nPNtuu23e9KY35f3vf3++8IUv5Gc/+1l3LhEAAGBAGMh3vO5SFMUpSbZL8pckPynL8v5NjD3sxdf/\nt5NjtyeZ/eKY/2erdwnwGnXCCSdku+22yymnnJLf/OY3OfbYY5MkY8eOTVEUrwhad9999xx22GEb\nzfHlL385X/3qV5MkGzZsSGtra9avX99xfPTo0Tn77LMze/bsDB06kP/JAgDoG0uWLMlNN92UadOm\nVWWOFStW5Igjjsg999zTsW/kyJEpyzIPPfRQfvOb3+R73/texo8fn9WrV/e4RwAAgP5oIP8W+90v\nbh2KomhOclxZln942b4xSV6X5OmyLP/cyTy/e/H1DV05aVEU927i0Bu7Ug/wWnLkkUfm3e9+d66+\n+urcdtttuf/++7Ny5coURZFJkyblbW97W6ZNm5Zp06ZlxIgRG9WvXbs2a9euTZIMHz48tbW12XHH\nHbP//vvnkEMOyTHHHJPx48f39WUBAAxojY2NOfLII1NT0/NFrno6x4wZM3LPPfdk3LhxmT17do49\n9tjstNNOSZKnnnoqP/3pT3PzzTfntttu63FvAAAA/dVADF7XJZmX5LtJHnlx31uTzElyaJKmoij2\nK8ty7YvHXvqN/ZpNzPfS/m22fqsAA0NLS9LUlLS2JrW1SUNDUlfXtdoxY8bk1FNPzamnntrl8z32\n2GM9axQAgE2qr6/Pz3/+87S0tOS6667LjBkz+nSO3/zmN7nzzjuTJAsXLsxRRx31iuPjxo3L4Ycf\nnsMPPzwXXXRRt3sDAADo76ryjNeiKB4riqLsxnbNS7VlWT5RlmVjWZZLyrJc/eK2KMnUJD9NsleS\nE3ur97IsD+xsS/Kb3jonQG9pakrq65M3vzk544xk9uz21ze/uX1/U1O1OwQAoKt22mmn/MM//EOS\nZM6cOXnhhRf6dI5f/epXHe8/8IEPbHbsyJEju90bAABAf1eV4DXJw0ke6sb2py1NWJblC0m+/uLH\nyS879NIdrZtaq/Kl/R4uAwwqCxYkU6cmixZ1fnzRovbjCxf2bV8AAPTcueeem9ra2jz88MP5xje+\nUbU5li5d2qM6AACAgawqwWtZlg1lWb6xG9u5XZx6xYuvY152rrVJliYZWxTFzp3U/PWLr7/t+RUB\nDCxNTcnJJydtbZsf19aWnHSSO18BAAaK7bbbLmeeeWaSZN68eXnuuef6bI4DDzyw4/2nP/3prFix\nYjOjAQAAXnuqdcdrb/mbF18fedX+H7z4+t5Oat73qjEAr3lz5245dH1JW1syb17v9gMAwNZz1lln\nZcKECXn88cfz1a9+tc/m2GOPPfLJT34ySXLHHXdk1113zeGHH57zzz8///Vf/yWIBQAAXvMGXPBa\nFMUBRVFs1HdRFA1Jznzx4zWvOvzST4mfK4pi25fVTEry6STPJenZ+kkAA0xLy6aXF96Uu+9urwMA\noP+rra3Nuee2Lxz1L//yL1m7dm2fzTF//vycddZZGT58eNavX5+mpqZceOGFOfLII7PjjjvmHe94\nR6699tqUZdntngAAAPq7ARe8Jrk4yeNFUdxYFMUlL25NSe5KMiLJ7LIsF7+84MXPFyfZM8n9L9Zc\nkeSeJBOSnF2W5WN9ehUAVdLTZYMtNwwAMHCcdtppmThxYpYvX57LLrusz+YYPnx4Lrrooo47ZT/2\nsY/lr//6r1MURZLk5z//eWbMmJHp06enratLsAAAAAwQAzF4/c8k9yV5e5KTkpya9ue03pBkclmW\nF3RWVJblPyaZlWRZkpOTfDJJS5IPlmX5b33QN0C/0Nrat3UAAPS90aNH57zzzkuSfOlLX8qaNWv6\ndI4dd9wxp5xySq677rr89re/zZ///OfMnz8/u+22W5LkxhtvzOWXX97tngAAAPqzARe8lmW5oCzL\nD5RlOaksy7FlWY4oy/L1ZVlOL8vyh1uovaosy7eXZTmmLMtxZVnWl2V5a1/1DtAf1Nb2bR0AANVx\nyimnZLfddsuqVaty0UUXVW2OJJk4cWJOPPHELFmyJBMnTkySLFy4sMfzAQAA9EcDLngFoDINDX1b\nBwBAdYwYMSKzZ89Okvzrv/5rVq5cWZU5Xm777bfPhz/84STJb3/724rmAgAA6G8ErwCDTF1dMnly\n92rq69vrAAAYWGbNmpU999wzTz31VL7whS9UbY6XGzNmTJL258ECAAC8lgheAQahxsakpov/AtTU\nJC/e5AAAwAAzdOjQzJkzJ0ny7//+7/nzn//ca3M8+uijefjhhzc717p16/Ld7343SbLffvt1uxcA\nAID+TPAKMAg1NCRXXrnl8LWmJpk/3zLDAAAD2cc//vHss88+eeaZZ/KDH/yg1+ZoaWnJ3nvvnWnT\npuWGG254RUC7du3a3HLLLTnkkEPy6KOPJknOOOOMHvUCAADQXwleAQapE05I7ryzfRnhztTXtx8/\n/vi+7QsAgK2rpqYmc+fO7fU5hg0blg0bNuTmm2/O9OnTs8suu2T06NHZZpttMnbs2HzoQx/KkiVL\nMmTIkFx44YWZNm1aRT0BAAD0N0Or3QAA1dPQ0L61tCRNTUlra1Jb277PM10BAF47pk2blgMOOCBL\nlizptTne85735KGHHsott9ySH/3oR/n1r3+dpUuX5umnn84222yTPfbYI5MnT86JJ56YOv/ZBAAA\nXoOKsiyr3cNrQlEU9x5wwAEH3HvvvdVuBQAAgAHMl+IAAAD61oEHHpglS5YsKcvywErmcccrAAAA\n9ANNTcncucmiRRsfmzw5aWxsD2EBAADonzzjFQAAAKpswYJk6tTOQ9ekff/UqcnChX3bFwAAAF0n\neAUAAIAqampKTj45aWvb/Li2tuSkk9rHAwAA0P8IXgEAAKCK5s7dcuj6kra2ZN683u0HAACAnhG8\nAgAAQJW0tGx6eeFNufvu9joAAAD6F8ErAAAAVElPlw223DAAAED/I3gFAACAKmlt7ds6AAAAeo/g\nFQAAAKqktrZv6wAAAOg9glcAAACokoaGvq0DAACg9wheAQAAoErq6pLJk7tXU1/fXgcAAED/IngF\nAACAKmpsTGq6+NN5TU0ye3bv9gMAAEDPCF4BAACgihoakiuv3HL4WlOTzJ9vmWEAAID+SvAKAAAA\nVXbCCcmdd7YvI9yZ+vr248cf37d9AQAA0HVDq90AAAAA0H4na0ND0tKSNDUlra1JbW37Ps90BQAA\n6P8ErwAAANCP1NUJWgEAAAYiSw0DAAAAAAAAVEjwCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAA\nABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoA\nAAAAAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAMAAVhRFj7YpU6Z0Ot+DDz6Ys88+O/vuu28m\nTJiQkSNHZrfddssHP/jBLFy4MM8//3zfXiAAAADAADG02g0AAAA9N3HixE73P/nkk3n++eczcuTI\njB8/fqPjEyZMeMXntra2fPazn83FF1+cDRs2JEmGDRuWMWPG5I9//GP++Mc/5tZbb80XvvCF3HDD\nDdlvv/22/sUAAAAADGDueAUAgAFs2bJlnW4HH3xwkmT69OmdHr/pppteMc+MGTPypS99KRs2bMgx\nxxyTe+65J88991xWrVqV1atXZ+HChdl5553zu9/9LvX19bnnnnuqcbkAAAAA/ZbgFQAABrkrrrgi\n119/fZLki1/8Yq6//voceOCBKYoiSTJ+/PjMmjUr9957b/baa6+0trZm+vTpefrpp6vZNgAAAEC/\nIngFAIBB7JlnnsmcOXOSJO9///tzzjnnbHLszjvvnGuuuSZFUeSRRx7J1772tT7qEgAAAKD/E7wC\nAMAgdtNNN2XlypVJks997nNbHP/Od74zhx9+eJIIXgEAAABeRvAKAACDWHNzc5Jkxx13zEEHHdSl\nmiOPPDJJ8rvf/S5/+tOfeqs1AAAAgAFF8AoAAIPYAw88kCTZd999u1zz1re+teP9gw8+uNV7AgAA\nABiIBK8AADCIPfnkk0mS7bbbrss122+/fcf7v/zlL1u9JwAAAICBSPAKAAAAAAAAUCHBKwAADGIT\nJkxI0r07V1euXLlRPQAAAMBgJ3gFAIBB7E1velOS5Je//GWXa+6///6O9/vss89W7wkAAABgIBK8\nAgDAIHbooYcmSZ544on85Cc/6VLNd7/73STJXnvtlV122aXXegMAAAAYSASvAAAwiE2bNi3bb799\nkuTzn//8Fsf/7Gc/y1133ZUkOeWUU3q1NwAAAICBRPAKAACD2KhRo9LY2JgkufXWW/OlL31pk2P/\n/Oc/5xOf+ETKssykSZMErwAAAAAvI3gFAIBB7rTTTsvRRx+dJDn33HPz8Y9/PEuWLOk43tramm98\n4xt529velv/7v//L2LFj861vfSvjxo2rVssAAAAA/c7QajcAAABU33XXXZfXve51ueyyy3L99dfn\n+uuvz/DhwzN69OisXr26Y9yee+6ZG264IQcccEAVuwUAAADofwSvAADQT7W0JE1NSWtrUlubNDQk\ndXW9c64hQ4bk4osvzoknnpivf/3rueuuu/L4449n3bp12WWXXbLffvvlIx/5SI477rgMGzasd5oA\nAAAAGMAErwAA0M80NSVz5yaLFm18bPLkpLGxPYTdnObm5h6de5999snFF1/co1oAAACAwcwzXgEA\noB9ZsCCZOrXz0DVp3z91arJwYd/2BQAAAMDmCV4BAKCfaGpKTj45aWvb/Li2tuSkk9rHAwAAANA/\nCF4BAKCfmDt3y6HrS9raknnzercfAAAAALpO8AoAAP1AS8umlxfelLvvbq8DAAAAoPoErwAA0A/0\ndNlgyw0DAAAA9A+CVwAA6AdaW/u2DgAAAICtS/AKAAD9QG1t39YBAAAAsHUJXgEAoB9oaOjbOgAA\nAAC2LsErAAD0A3V1yeTJ3aupr2+vAwAAAKD6BK8AANBPNDYmNV38H3pNTTJ7du/2AwAAAEDXCV4B\nAKCfaGhIrrxyy+FrTU0yf75lhgEAAAD6E8ErAAD0IyeckNx5Z/sywp2pr28/fvzxfdsXAAAAAJs3\ntNoNAAAAr9TQ0L61tCRNTUlra1Jb277PM10BAAAA+ifBKwAA9FN1dYJWAAAAgIHCUsMAAAAAAAAA\nFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAAAAAAVEjwCgAA\nAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECF\nBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAA\nAAAAVEjwCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAKAAAAAAAAUCHB\nKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAAQIUErwAAAAAA\nAAAVErwCAAAAAAAAVEjwCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIAAAAAAABUSPAK\nAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQIcErAAAAAAAA\nQIUErwAAAAAAAAAVErwCAAAAAAAAVEjwCgAAAAAAAFAhwSsAAAAAAABAhQSvAAAAAAAAABUSvAIA\nAAAAAABUSPAKAAAAAAAAUCHBKwAAAAAAAECFBK8AAAAAAAAAFRK8AgAAAAAAAFRI8AoAAAAAAABQ\nIcErAAAAAAAAQIUErwAAAAAAAAAVErwCAAAAANCnZs6cmaIoNtrGjRuXurq6nHrqqXnwwQc3Wd9Z\n7bBhw7Ljjjvm8MMPz4IFC7Jhw4Y+vCIAELwCAAAAAFAlw4YNy8SJEzNx4sTsuOOOWbduXR544IF8\n5StfyX777Zcbb7xxs/W1tbUd9aNHj86KFSvS1NSUE088MYceemjWrVvXR1cCAIJXAAAAAACq5OCD\nD86yZcuybNmyLF++PM8++2xuv/32TJo0KevXr8+sWbOyYsWKTdZfeumlHfVr1qzJ0qVLc+KJJyZJ\nfvjDH+a8887rq0sBAMErAAAAAAD9w7Bhw/Le97431157bZJk7dq1+c53vtPl+l122SXz58/PYYcd\nliT5+te/nueff75XegWAVxO8AgAAAADQrxx00EEZO3ZskuSBBx7odv3HPvaxJO3B7UMPPbRVewOA\nTRG8AgAAAADQ75RlmSTZsGFDt2tf97rXdbxvbW3daj0BwOYIXgEAAAAA6FcWL16ctWvXJkn22GOP\nbtf/4Q9/6Hi/zTbbbLW+AGBzBK8AAAAAAPQLzz//fO64447MmDEjSfszX6dPn96tOdra2rJw4cIk\nyfjx47P33ntv9T4BoDNDq90AAAAAAACD0+LFi7PTTv8fe/ce5WVZ743/fQ8M4AmQRKjwsNE8YSqS\ntcVHIckDltnBJA+IiMdKW7t82j0ZqOiu9q+eXdt0J5pKSj2KYpS5PdS4wTamlofUEW1rWWEBIuAg\nOpzm+/tjZBSZGWbmOwdgXq+17jXfue/rc92f27WcBbznuu7BSeq3Fl6yZEnq6uqSJBUVFZk2bVqG\nDBnSorneeOONzJ8/P5dddlkeeeSRJMnnPve59OjRo2OaB4B3sOIVAAAAgHZ1xhlnpCiKjY4ddtgh\nw4YNy+c+97nMnz+/yfrGaouiSO/evbPrrrvmxBNPzL333ttk/aWXXtpo/XbbbZf3ve99mTBhQkMo\nA3StNWvWZNGiRVm0aFEWL17cELoOGDAgDz/8cCZOnNhs/cSJExv+H992220zYsSI/PznP0+SfPzj\nH8+ll17a0Y8AAA0ErwAAAAB0iMrKygwaNCiDBg3KzjvvnNdffz3PPPNMfvCDH+Sggw7Kbbfd1mx9\n3759G+oHDRqUJPnrX/+aWbNm5dhjj82XvvSlZusrKio2qF+9enWef/753HTTTTn00EPzve99r92e\nFWibUaNGpVQqpVQqpba2Nk888UROPPHELF26NJMmTcqyZcuarX/7z4n3vve92X///XPKKafkZz/7\nWX72s5+lV69enfQkACB4BQAAAKCDjBw5MgsXLszChQuzaNGi1NbW5u67787uu++e1atXZ+LEiXn5\n5ZebrP/3f//3hvqFCxemtrY21dXVOe6445Ik3/3ud/PAAw80Wb/LLrtsVD9v3rwcdNBBqaury5e/\n/OU8/fTT7f7cQNv07t07Bx54YGbOnJljjjkmTz75ZM4999xma97+c2LBggV56qmn8uMf/zgf//jH\nO6lrAHiL4BUAAACATlFZWZljjz02P/7xj5MkK1euzKxZs1pcXxRF9ttvv9x2223p379/kuQXv/hF\ni+t79OiRkSNHZvbs2amsrExdXV1mzJjRuocAOlxRFLnyyivTo0eP3HbbbZk7d25XtwQALSJ4BQAA\nAKBTHXroodl+++2TJM8880yr67fddtvsscceSerD29babbfdstdee7X5/kDH22uvvTJu3LgkycUX\nX9zF3QBAywheAQAAAOh0pVIpSbJu3bpW177xxht54YUXkiR77rlnp98f6BwXXXRRkmTevHmZM2dO\n1zYDAC0geAUAAACgUz344IMNK1WHDh3aqtrnnnsu48aNy/LlyzNgwIBMmDCh1fd/8cUX8z//8z9t\nuj/QeYYPH56PfOQjSZIrrriii7sBgE0TvAIAAADQKdasWZN77703p512WpL6d76u30q0MV/84hcz\nePDghqNPnz7ZZ599ct999+VTn/pUfvOb32TAgAEtvv+6devym9/8Jp/85CezZs2aJGnoBdg8feUr\nX0mSVFVV5aGHHuribgCgeT27ugEAAAAAtk4PPvhgBg8enKR+a98lS5akrq4uSVJRUZFp06ZlyJAh\nTdbX1NSkpqZmo/OrV6/Oq6++mldeeaXZ+//1r39tuH+SLF26tCFwTZJLL700H/rQh1r1TEDnOuqo\nozJ8+PA8/vjjufzyy3PXXXd1dUsA0CTBKwAAAAAdYs2aNVm0aNFG5wcMGJB77703H/jAB5qtv/HG\nG3PGGWc0fP/aa6/l+eefz5VXXpkbb7wx8+bNy89//vMcddRRjdbX1dU1ev8+ffpk1qxZOe6441r3\nQECTqquTqqqkpibp2zcZMyYZNqzp8dOnT8/06dNbNPdjjz220bn172kGgM2JrYYBAAAA6BCjRo1K\nqVRKqVRKbW1tnnjiiZx44olZunRpJk2alGXLlrVqvu233z4HHXRQbrjhhnz2s59NbW1tLrjggqxb\nt67R8bvttlvD/VevXp1nn302559/fmpra3PuuefmxRdfbIenhO6tqioZNSrZf//ki19MJk+u/7r/\n/vXnq6q6ukMA6DyCVwAAAAA6XO/evXPggQdm5syZOeaYY/Lkk0/m3HPPbfN861fCPvfcc/n973+/\nyfGVlZXZe++98x//8R85++yzs2DBgpx88skNWx8DrXf99cnRRycPPND49QceqL9+ww2d2xcAdBXB\nKwAAAACdpiiKXHnllenRo0duu+22zJ07t03z7Lrrrg2f//jHP7aq9l//9V/Tr1+/PPTQQ7n55pvb\ndH/o7qqqknPOSTb1uwt1dcnZZ1v5CkD3IHgFAAAAoFPttddeGTduXJLk4osvbtMcL730UsPnysrK\nVtXuuOOO+fznP58kufTSS7N27do29QDd2dSpmw5d16urSy6/vGP7AYDNgeAVAAAAgE530UUXJUnm\nzZuXOXPmtLp+5syZDZ+HDx/e6voLLrggvXv3zosvvpgZM2a0uh66s+rqprcXbsrcufV1ALA1E7wC\nAAAA0OmGDx+ej3zkI0mSK664osV1ixYtyte+9rX88Ic/TJKccMIJG2w73FKDBw/O+PHjkyTf/OY3\nvesVWqGt2wbbbhiArZ3gFQAAAIAu8ZWvfCVJUlVVlYceemij61/84hczePDghmOHHXbI4MGD881v\nfjOlUikHH3xwrr/++jbf/6KLLkpFRUX+8Ic/5NZbb23zPNDd1NR0bh0AbCkErwAAAABsUnV1cuWV\nyRVX1H9tjy1DjzrqqIZtgi9v5AWQNTU1WbRoUcNRW1ubgQMHZsyYMZk2bVoeeuihvOtd72rz/ffe\ne+98/OMfT5J84xvfSKlUavNc0J307du5dQCwpSj8gbJ9FEXx6MEHH3zwo48+2tWtAAAAALSbqqpk\n6tTG3+d4xBHJlCnJmDGd3xfQdaqrk/33b33d008nw4a1fz8AUK4RI0bksccee6xUKo0oZx4rXgEA\nAABo1PXXJ0cf3XjomtSfP/ro5IYbOrcvoGsNG1b/ixetMWqU0BWArZ/gFQAAAICNVFUl55yT1NU1\nP66uLjn77PrxQPcxZUpS0cJ/Xa6oSCZP7th+AGBzIHgFAAAAYCNTp246dF2vri5p5BWtwFZszJjk\n2ms3Hb5WVCTXXWdLcgC6B8ErAAAAABuorm56e+GmzJ1bXwd0H5MmJffdV7+NcGNGjaq/fuaZndsX\nAHSVnl3dAAAAAACbl7ZuG1xV5R2O0N2MGVN/VFfX/wyoqUn69q0/5+cBAN2N4BUAAACADdTUdG4d\nsOUbNkzQCgC2GgYAAABgA337dm4dAABsDQSvAAAAAGxgzJjOrQMAgK2B4BUAAACADQwblhxxROtq\nRo2yzSgAAN2b4BUAAACAjUyZklS08F+OKiqSyZM7th8AANjcCV4BAAAA2MiYMcm11246fK2oSK67\nzjbDAAAgeAUAAACgUZMmJffdV7+NcGNGjaq/fuaZndsXAABsjnp2dQMAAAAAbL7GjKk/qquTqqqk\npibp27f+nHe6AgDAWwSvAAAAAGzSsGGCVgAAaI6thgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAA\nAKBMglcAAAAAAACAMgleAQAAAAAAAMokeAUAAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4B\nAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACgTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFAAAAAAAA\nKJPgFQAAAAAAAKBMglcAAAAAAACAMgleAQAAAAAAAMokeAUAAAAAAAAok+AVAAAAAAAAoEyCVwAA\nAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACgTIJXAAAAzljYDgAAIABJREFUAAAA\ngDIJXgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAAAACAMgleAQAAAAAAAMokeAUA\nAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACg\nTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFeJszzjgjRVFkv/32a3HN1VdfnaIo0qdPnyxfvjxz5sxJ\nURSNHtttt1323XffnHfeeZk/f36Tc44ePXqj2l69emXgwIHZZ599ctJJJ+Xf/u3fsnDhwvZ4bAAA\nAAAAoEyCV4C3mTBhQpJk/vz5+d3vfteimptuuilJcsIJJ6R///4bXNtpp50yaNCgDBo0KAMHDkxt\nbW2effbZTJs2LQceeGBmzZrV7Nx9+vRpqO/fv39WrFiR5557Lrfddlu+/OUvZ5dddsn555+flStX\ntuFpAQAAAACA9iJ4BXib0aNHZ7fddkvyVqDanOeeey6PPPJIkrdC27f77W9/m4ULF2bhwoVZvHhx\nVq1alaqqquy1115Zs2ZNJk2alBUrVjQ5/7hx4zaor62tzaJFi3LHHXfk2GOPzdq1a3PNNddk5MiR\nqampaeNTAwAAAAAA5RK8ArxNURQZP358kuSWW27J2rVrmx2/PpwdPHhwjjnmmE3O37Nnzxx55JG5\n8cYbkySvvvpqfv3rX7eqx5133jmf/OQnc/fdd+eGG25IURR58sknc/bZZ7dqHgAAAAAAoP0IXgHe\n4fTTT0+SvPzyy7n77rubHFcqlTJjxowkyamnnpoePXq0+B4HHHBAw+dytgmeOHFivvzlLydJbrvt\ntjz55JNtngsAAAAAAGg7wSvAO7zvfe/LyJEjkzS/3fCcOXPyl7/8JUnj2ww356mnnmr4vOeee7ah\ny7f87//9v9OrV6+USqX8v//3/8qaCwAAAAAAaBvBK0Aj1gepd955Z5YvX97omPWh7PDhw/P+97+/\nRfOuW7cuc+fOzcSJE5Mko0aNyvDhw8vqdeedd86IESOSpNXbFgMAAAAAAO1D8ArQiJNOOil9+vTJ\nqlWrMnPmzI2uv/7665k1a1aS5le7HnLIIRk8eHAGDx6cnXfeOb17987o0aPzyiuv5Atf+ELuvPPO\ndul3ffD7pz/9qV3mAwAAAAAAWkfwCtCI/v3754QTTkjS+HbDP/3pT7NixYr07Nkzp5xySpPzLFmy\nJIsWLcqiRYvy8ssvZ926dUmS1157LcuXL8+KFSvapd8BAwYkSZYuXdou8wEAAAAAAK0jeAVowhln\nnJEkmTdvXv74xz9ucG19GDt27NgMHDiwyTn+9Kc/pVQqNRyLFy/O/fffnxEjRmTGjBkZOXJkFixY\nUHavpVKp7DkAAAAAAIC2E7wCNOGoo47Ku9/97iTJzTff3HD+73//e6qqqpI0v81wYwYOHJgPf/jD\n+eUvf5mhQ4fmz3/+cy699NKye122bFmSt1a+AgAAAAAAnUvwCtCEHj165LTTTkuyYfA6Y8aMrFu3\nLgMGDMjxxx/fprm32WabnHTSSUnS6DtkW+vJJ59MkgwdOrTsuQAAAAAAgNYTvAI0Y/2K1hdeeCEP\nPvhgkrdC2M9+9rPp1atXm+feddddkyQrVqzIkiVL2jzP4sWL89hjjyVJDj/88DbPAwAAAAAAtJ3g\nFaAZw4YNy4gRI5LUv9f18ccfz1NPPZWk9dsMv9NLL73U8LmysrLN83z729/O6tWrUxRFTjnllLJ6\nAgAAAAAA2qZnVzcAsLmbMGFCHn300cycOTMVFfW/r7LPPvvkgx/8YJvnXLNmTWbPnp2kfnvgfv36\ntWme6dOn5//+3/+bpH4F7v7779/mngAAAAAAgLaz4hVgE04++eRUVlZm2bJlmTZtWpK2r3atq6vL\n/Pnz85nPfCbV1dVJkgsuuKBVcyxZsiSzZ8/Occcdl4kTJ6ZUKuWggw7Ktdde26aeAAAAAACA8lnx\nCrAJO+20Uz760Y9m9uzZqaurS0VFRU477bQW1R5yyCHp0aNHw/fLli3L6tWrG76fOHFiLrzwwibr\nb7311txzzz1J6kPbmpqarFq1quF6ZWVlzjrrrHznO9/Jtttu29pHAwAAAAAA2ongFegWqquTqqqk\npibp2zcZMyYZNqzl9RMmTGjYGvjII4/MkCFDWlS3ZMmSDb7v1atXdtlll3zoQx/KmWeembFjxzZb\nX1tbm9ra2iT1IesOO+yQ3XbbLQcccEBGjhyZU045JYMGDWr5gwAAAAAAAB1C8Aps1aqqkqlTkwce\n2PjaEUckU6bUh7Cb8olPfCKlUqlF9xw9enSLxzZlzpw5ZdUDAAAAAACdyztega3W9dcnRx/deOia\n1J8/+ujkhhs6ty8AAAAAAGDrI3gFtkpVVck55yR1dc2Pq6tLzj67fjwAAAAAAEBbCV6BrdLUqZsO\nXderq0suv7xj+wEAAAAAALZugldgq1Nd3fT2wk2ZO7e+DgAAAAAAoC0Er8BWp63bBttuGAAAAAAA\naCvBK7DVqanp3DoAAAAAAADBK7DV6du3c+sAAAAAAAAEr8BWZ8yYzq0DAAAAAAAQvAJbnWHDkiOO\naF3NqFH1dQAAAAAAAG0heAW2SlOmJBUt/AlXUZFMntyx/QAA0H6WLl2ab37zmzn88MMzePDg9OrV\nK4MGDcr/+l//K9/4xjfyyiuvbFRTFEWbjtGjRzfMsfvuu6coilx66aWb7LG5saNHj27x/WfPnr1B\n7RlnnNHouB122CHDhg3L5z73ucyfP7+1/0kBAABoBz27ugGAjjBmTHLttck55yR1dU2Pq6hIrrvO\nNsMAAFuKn/zkJ/n85z+f5cuXJ0kqKirSr1+/LFmyJIsXL868efPy7W9/O1dffXVOOeWUhrpBgwY1\nOt/SpUuzZs2a9OnTJ/369dvo+oABAzrmQZIm7/nOMY2prKxs6K1UKmXJkiV55pln8swzz+T666/P\njBkz8pnPfKbdewYAAKBpVrwCW61Jk5L77qvfRrgxo0bVXz/zzM7tCwCAtpk2bVpOO+20LF++PCNG\njMh//ud/5o033sjSpUtTW1ube+65J4ccckiWL1+e0047LdOmTWuoXbhwYaPHyJEjkyTjxo1r9Pod\nd9zRYc/T1D3ffhx77LGN1o4cObJhzKJFi1JbW5u77747u+++e1avXp2JEyfm5Zdf7rDeAQAA2FiX\nB69FUVQWRfHFoihuLIriiaIoVhdFUSqK4qwW1E4oiuKRoiheK4ri1aIo5hRF8bFmxm9TFMVlRVE8\nVxRFbVEUi4uimFkUxb7t+1TA5mLMmGTOnOTpp5N///fk8svrvz79dP15K10BALYMjz/+eC688MKU\nSqWccMIJ+c1vfpOxY8emV69eSepXgB5zzDF58MEHc8IJJ6RUKuXCCy/ME0880cWdd47Kysoce+yx\n+fGPf5wkWblyZWbNmtXFXQEAAHQvm8NWw9sl+d6bnxclWZhkl00VFUXxnSRfTrIgyXVJeiX5bJI7\ni6K4oFQqXfWO8b2T/DLJYUl+l+Tf37zPZ5J8tCiKI0ul0sPt8kTAZmfYsPoDAIAt09e//vWsXr06\n73nPe3LTTTelsrKy0XE9e/bMj370o+y77775+9//nsmTJ+fOO+/s5G67zqGHHprtt98+r732Wp55\n5pmubgcAAKBb6fIVr0leT3JckveUSqXBSW7YVEFRFCNTH7q+kOSAUqn0T6VS6fNJRiRZmuQ7RVHs\n/o6yL6U+dL09yYdKpdI/l0qlU5KcmGTbJDcURbE5/PcAAADgbRYsWJC77747SfKFL3whffv2bXZ8\nv3798oUvfCFJctddd2XBggUd3uPmpFQqJUnWrVvXxZ0AAAB0L10eNJZKpdWlUunuUqn091aUnffm\n138plUrL3jbXi0muTtI7ycT154uiKN5W85VSqVT3tpqfJfl1kv2SNPEmSAAAALrK3LlzG8LET3zi\nEy2qWT+uVCrlgQce6LDeNjcPPvhgVq5cmSQZOnRoF3cDAADQvWwOWw23xZFvfr2nkWt3J5n85phL\n3jy3R5Jdk/yhVCr9qYmaw9+s+a/2bRUAAIByrN8yt3fv3tl7771bVLPPPvukV69eWb16debPn9+u\n/XznO9/JNddc0+yYl19+eZPz3Hrrrbnnnsb+WluvX79+ee6551rU05o1a3L//ffnvPPqf+e4srIy\n48aNa1EtAAAA7WOLC16LotguyXuTvNbEKtn/efPrXm87t/5v5n9oYtrGapq6/6NNXNpnU7UAAAC0\n3tKlS5MkO+64YyoqWrZxU0VFRXbccccsWrQor7zySrv2s3LlyoZVpeWora1NbW1ts9eb8uCDD2bw\n4MFJ6lf1LlmyJHV19Zs7VVRUZNq0aRkyZEjZPQIAANByXb7VcBv0e/Prq01cX3++f5k1AABbrNdf\nfz0/+MEPcvzxx2fXXXfNtttum+222y7/8A//kBNPPDEzZszIG2+80WT9Sy+9lKlTp+bwww/Pu9/9\n7vTq1Sv9+vXL/vvvn7POOiu/+tWvGrb9BOhuLrnkkpRKpWaP3XbbbZPzTJgwodk5li9f3mTtmjVr\nsmjRoixatCiLFy9uCF0HDBiQhx9+OBMnTmyyFgAAgI7RLsFrURQvFkVRasUxoz3u2xVKpdKIxo4k\nz3Z1bwAASXLnnXdmjz32yOc+97n84he/yF//+tdUVFSkR48eefHFFzNr1qyMHz8+e+65Z+6///4N\nakulUq644orsueeeueSSS/Lf//3fWbhwYbbbbrusWrUq1dXVuf7663PUUUflQx/6UBYsWNBFTwl0\nJwMGDEiSLFu2rCFg3JS6urosW7Zsg/qtyahRoxoC2tra2jzxxBM58cQTs3Tp0kyaNKnh2QEAAOg8\n7bXi9YUkz7Xi+FsZ91q/OrVfE9fXn3/7rwa3pQYAYIszffr0fOITn8jChQuz99575+abb86SJUvy\n2muvpaamJsuXL8/tt9+e0aNH529/+1seeOCBDerPOuusTJ48ObW1tTn66KNz77335vXXX8+yZctS\nW1ubv/zlL/mP//iP7LHHHvntb3+b559/voueFOhO9t133yTJqlWrWvzO02effTarV69Okuy3334d\n1tvmoHfv3jnwwAMzc+bMHHPMMXnyySdz7rnndnVbAAAA3U67vOO1VCqNaY95WnivlUVRvJTkvUVR\nvLuR97y+782vb3+f6/q/mTf1DtfGagAAtii///3vc95556Wuri7HHXdcbr/99myzzTYbjOnXr18+\n/elP59Of/nRuvfXWDVasTps2LTfccEOS5LLLLsuUKVM2uscuu+yS888/P2effXamTJnS4nctApRj\n9OjRKYoipVIps2fPbghimzN79uwkSVEUOeKIIzq6xc1CURS58sors99+++W2227L3LlzM2rUqK5u\nCwAAoNvYUv+lbP2eeMc2cm3sO8Yk9Sty/5Jkr6Io/qGFNQAAW5Svf/3rWbVqVd773vfmJz/5yUah\n6zuNGzcuX/rSl5IktbW1DUHrxz72sUZD17fr2bNnvvGNb+Twww9vn+YBmjFkyJCMHVv/17arrroq\nNTU1zY6vqanJVVddlSQ57rjjMmTIkA7vcXOx1157Zdy4cUmSiy++uIu7AQAA6F621OD1mje/XlwU\nxY7rTxZFsXuSzydZleTG9edLpVLpbTX/X1EUFW+rOSHJ4UmeSTK3Q7sGAOggL730Uu66664kyYUX\nXph+/Zp6w8KGiqJIktxxxx1ZvHhxkmTy5Mktvu/6eoCONnXq1FRWVuZvf/tbTj/99KxZs6bRcWvX\nrs2ECRPy97//PZWVlZk6dWond9r1LrrooiTJvHnzMmfOnK5tBgAAoBvZLILXoii+WhTF9KIopif5\nxJunJ64/VxTFWW8fXyqVHkzyb0n2SPJkURTfLYri6iS/SzIgyUWlUunFd9zm35I8mOTEJA8XRfGt\noih+kuT2JK8nObNUKtV10CMCAHSoOXPmpP53zZKPf/zjra7/r//6ryTJoEGD8sEPfrBdewNoDyNG\njMh3v/vdJMnPfvazjBw5Mvfcc09DALt27drcd999Oeywwxq2Gf7e976Xgw8+uMt67irDhw/PRz7y\nkSTJFVdc0cXdAAAAdB/t8o7XdnBskne+eGbkm8d6P3z7xVKp9OWiKJ5K/QrXc5LUJXksybdLpdIv\n3nmDUqm0qiiKo5J8NcnJSf4pSU2S2UkuKZVKz7TTswAAdLr58+cnSXr37p299967zfUHHnhgu/YF\n0J4+//nPp2/fvrngggvyu9/9LmPHjk1FRUX69++fV199NevWrUuS9O3bN1dddVXGjx/fxR0379Zb\nb80999zT7JiLLrqoYQVra3zlK1/Jr371q1RVVeWhhx7KP/7jP7a1TQAAAFposwheS6XS6DbWTU8y\nvRXjX08y5c0DAGCr8corryRJdtxxxzZt/7u+fsCAAe3aF0BTqquTqqqkpibp2zcZMyYZNmzTdePH\nj89xxx2XadOm5T//8z/zhz/8IcuXL8+AAQPyvve9L2PHjs15552XnXbaqeMfoky1tbWpra1tdsxr\nr73WprmPOuqoDB8+PI8//nguv/zyhu3oAQAA6DibRfAKAABA91BVlUydmjzwwMbXjjgimTKlPoRt\nzrve9a587Wtfy9e+9rWy+2nNO1BffPHFdhlbzntXp0+fnunTp7do7GOPPdbm+wAAANB6m8U7XgEA\nKM+73vWuJMmyZcsa3vXalvqlS5e2a18Ab3f99cnRRzceuib1548+Ornhhs7tCwAAANqD4BUAYCuw\n7777JklWrVqV5557rs31v//979u1L4D1qqqSc85J6uqaH1dXl5x9dv14AAAA2JIIXgEAtgKjRo1q\neLfrz3/+81bXf/jDH06SLFq0KI888ki79gaQ1G8vvKnQdb26uuTyyzu2HwAAAGhvglcAgK3AkCFD\nctxxxyVJvv/976empqZFdXVvpiCf/OQnM3DgwCTJFVdc0eL71rU0RQG6terqprcXbsrcufV1AAAA\nsKUQvAIAbCWuuOKK9O7dOwsWLMgpp5yS2traZsffcsst+e53v5sk2WabbXLZZZclSe68885cvoml\nZmvXrs3/+T//J//93//dPs0DW7W2bhtsu2EAAAC2JIJXAICtxEEHHZSrr746RVHkrrvuyvDhwzNj\nxowsXbq0Ycyrr76aO+64Ix/+8Idz8sknZ8WKFQ3Xzj///Jx++ulJkilTpuTYY4/NL3/5yw0C3AUL\nFuSaa67JPvvsk29961tWvAIt0sJF+O1WBwAAAF2hZ1c3AABA+5k0aVLe9a535dxzz82zzz6b8ePH\nJ0m23377FEWxQdC622675cgjj9ygfvr06Rk6dGi++c1v5t577829996boijSv3//vPHGGxuEsIcd\ndlj22muvznkwYIvWt2/n1gEAAEBXKEqlUlf3sFUoiuLRgw8++OBHH320q1sBALYS1dX122zW1NSH\nD2PGJMOGtax25cqV+dGPfpS77rorTz75ZJYsWZKiKDJo0KB84AMfyKc+9al86lOfSu/evRutX7Bg\nQX74wx/ml7/8ZZ5//vksW7Ysffr0ya677pqRI0fmlFNOyejRo9vvYYGtWnV1sv/+ra97+umW/9wD\nAACAthoxYkQee+yxx0ql0ohy5hG8thPBKwDQXqqqkqlTkwce2PjaEUckU6bUh7AAW5JRoxr/udbc\n+DlzOqwdAAAAaNBewat3vAIAbEauvz45+uimw4kHHqi/fsMNndsXQLmmTEkqWvg30IqKZPLkju0H\nAAAA2pvgFQBgM1FVlZxzTlJX1/y4urrk7LPrxwNsKcaMSa69dtPha0VFct11VvYDAACw5RG8AgBs\nJqZO3XToul5dXXL55R3bD0B7mzQpue+++m2EGzNqVP31M8/s3L4AAACgPfTs6gYAAEiqq1v37sMk\nmTu3vm7YsI7pCaAjjBlTf1RX16/cr6lJ+vatP+fnGQAAAFsywSsAwGagrdsGV1UJKoAt07Bhfn4B\nAACwdbHVMADAZqCmpnPrAAAAAID2JXgFANgM9O3buXUAAAAAQPsSvAIAbAbGjOncOgAAAACgfQle\nAQA2A8OGJUcc0bqaUaO8HxEAAAAANheCVwCAzcSUKUlFC/90VlGRTJ7csf0AAAAAAC0neAUA2EyM\nGZNce+2mw9eKiuS662wzDAAAAACbE8ErAMBmZNKk5L776rcRbsyoUfXXzzyzc/sCAAAAAJrXs6sb\nAABgQ2PG1B/V1UlVVVJTk/TtW3/OO10BAAAAYPMkeAUA2EwNGyZoBQAAAIAtha2GAQAAAAAAAMok\neAUAAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAA\nAACgTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAAAACAMgle\nAQAAAAAA2tkZZ5yRoiiy3377tbjm6quvTlEU6dOnT5YvX545c+akKIpGj+222y777rtvzjvvvMyf\nP79F87/66qv53ve+l+OOOy677LJLtt1222yzzTYZMmRIjj322HzjG9/I888/39ZHhm5P8AoAAAAA\nANDOJkyYkCSZP39+fve737Wo5qabbkqSnHDCCenfv/8G13baaacMGjQogwYNysCBA1NbW5tnn302\n06ZNy4EHHphZs2Y1O/cPf/jD7Lbbbvmnf/qn3H333VmwYEFDyPvSSy/l3nvvzcUXX5y99tor48aN\ny+rVq9vw1NC9CV4BAAAAAADa2ejRo7PbbrsleStQbc5zzz2XRx55JMlboe3b/fa3v83ChQuzcOHC\nLF68OKtWrUpVVVX22muvrFmzJpMmTcqKFSsanfuSSy7J2WefnVdffTWHHHJIZs6cmVdeeSUrV67M\nsmXLsmrVqsybNy///M//nP79+2fmzJl5/fXXy3h66J4ErwAAAAAAAO2sKIqMHz8+SXLLLbdk7dq1\nzY5fH84OHjw4xxxzzCbn79mzZ4488sjceOONSeq3Ef71r3+90bhf/OIXmTp1apLk/PPPz0MPPZTP\nfOYzGTBgQMOYXr16ZeTIkfnWt76VP//5zzn33HNTFEXLHhRoIHgFAAAAAADoAKeffnqS5OWXX87d\nd9/d5LhSqZQZM2YkSU499dT06NGjxfc44IADGj6vXLlyo3m/+tWvJkk++MEP5vvf/34qKpqPhnbY\nYYdcc8016devX4t7AOoJXgEAAAAAADrA+973vowcOTJJ89sNz5kzJ3/5y1+SNL7NcHOeeuqphs97\n7rnnBtfmzZuX6urqJMlXv/rVVgW6QOsJXgEAAAAAADrI+iD1zjvvzPLlyxsdsz6UHT58eN7//ve3\naN5169Zl7ty5mThxYpJk1KhRGT58+AZj5syZk6R+W+KWbF8MlEfwCgAAAAAA0EFOOumk9OnTJ6tW\nrcrMmTM3uv76669n1qxZSZpf7XrIIYdk8ODBGTx4cHbeeef07t07o0ePziuvvJIvfOELufPOOzeq\nmT9/fpJkjz32yLbbbttOTwQ0RfAKAAAAAADQQfr3758TTjghSePbDf/0pz/NihUr0rNnz5xyyilN\nzrNkyZIsWrQoixYtyssvv5x169YlSV577bUsX748K1as2Khm6dKlSZIdd9yxyXnPO++8hkD37cd3\nvvOdVj0nIHgFAAAAAADoUGeccUaS+neu/vGPf9zg2vowduzYsRk4cGCTc/zpT39KqVRqOBYvXpz7\n778/I0aMyIwZMzJy5MgsWLCg1b0tX768IdB9+/Haa6+1ei7o7gSvAAAAAAAAHeioo47Ku9/97iTJ\nzTff3HD+73//e6qqqpI0v81wYwYOHJgPf/jD+eUvf5mhQ4fmz3/+cy699NINxgwYMCBJsmzZsibn\nueWWWzYIdA877LBW9QG8RfAKAAAAAADQgXr06JHTTjstyYbB64wZM7Ju3boMGDAgxx9/fJvm3mab\nbXLSSSclyUbvkN13332TJC+88EJef/31Ns0PtJzgFQAAAAAAoIOtX9H6wgsv5MEHH0zyVgj72c9+\nNr169Wrz3LvuumuSZMWKFVmyZEnD+dGjRydJ1q5dm3vvvbfN8wMtI3gFAAAAAADoYMOGDcuIESOS\n1L/X9fHHH89TTz2VpPXbDL/TSy+91PC5srKy4fNhhx2WYcOGJUn+9V//NevWrSvrPkDzBK8AAAAA\nAACdYH3AOnPmzFx33XVJkn322Scf/OAH2zznmjVrMnv27CTJ0KFD069fv4ZrRVHkW9/6VpLk4Ycf\nzgUXXJC6uro23wtonuAVAAAAAACgE5x88smprKzMsmXLMm3atCRtX+1aV1eX+fPn5zOf+Uyqq6uT\nJBdccMFG4z72sY9lypQpSZIf/OAH+cd//MfMnDkzS5cubRizbt26VFdXZ8qUKXniiSfa1A+Q9Ozq\nBgAAAAAAALqDnXbaKR/96Ecze/bs1NXVpaKiIqeddlqLag855JD06NGj4ftly5Zl9erVDd9PnDgx\nF154YaO1l112WXbZZZdcdNFF+e1vf5tx48YlSbbbbrv06dMnNTU1WbNmTZL6VbKnnnpqzjnnnLY+\nJnRbglcAAAAAAIBOMmHChIatgY888sgMGTKkRXVLlizZ4PtevXpll112yYc+9KGceeaZGTt2bLP1\nZ511Vk488cTceOONue+++/L000/nlVdeycqVKzNw4MDsv//+Ofzww3Paaadl9913b9OzQXdXlEql\nru5hq1AUxaMHH3zwwY8++mhXtwIAAAAAAHSg6uqkqiqpqUn69k3GjEmGDevqroC2GjFiRB577LHH\nSqXSiHLmseIVAAAAAACgBaqqkqlTkwce2PjaEUckU6bUh7BA91TR1Q0AAAAAAABs7q6/Pjn66MZD\n16T+/NFHJzfc0Ll9AZsPwSsAAAAAAEAzqqqSc85J6uqaH1dXl5y57DtTAAAgAElEQVR9dv14oPsR\nvAIAAAAAADRj6tRNh67r1dUll1/esf0AmyfBKwAAAAAAQBOqq5veXrgpc+fW1wHdi+AVAAAAAACg\nCW3dNth2w9D9CF4BAAAAAACaUFPTuXXAlkvwCgAAAAAA0IS+fTu3DthyCV4BAAAAAACaMGZM59YB\nWy7BKwAAAAAAQBOGDUuOOKJ1NaNG1dcB3YvgFQAAAAAAoBlTpiQVLUxUKiqSyZM7th9g8yR4BQAA\nAAAAaMaYMcm11246fK2oSK67zjbD0F0JXgEAAAAAADZh0qTkvvvqtxFuzKhR9dfPPLNz+wI2Hz27\nugEAAAAAAIAtwZgx9Ud1dVJVldTUJH371p/zTldA8AoAAAAAANAKw4YJWoGN2WoYAAAAAAAAoEyC\nVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACgTIJXAAAAAAAAgDIJXgEAAAAA\nAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAAAACAMgleAQAAAAAAAMokeAUAAAAAAAAok+AV\nAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACgTIJXAAAAAAAA\ngDIJXgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAAAACAMgleAQAAAAAAAMokeAUA\nAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT4BUAAAAAAACg\nTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAAAACAMgleAQAA\nAAAAAMokeAUAAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4BQAAAAAAACiT\n4BUAAAAAAACgTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFAAAAAAAAKJPgFQAAAAAAAKBMglcAAAAA\nAACAMgleAQAAAAAAAMokeAUAAAAAAAAok+AVAAAAAAAAoEyCVwAAAAAAAIAyCV4BAAAAAAAAyiR4\nBQAAAAAAACiT4BUAAAAAAACgTIJXAAAAAAAAgDIJXgEAAAAAAADKJHgFAAAAALqNM844I0VRpCiK\nVFZWZvHixc2O/9nPftYwviiKTJ8+fYPru++++wbXi6JInz59MmjQoOy///4ZP358rrnmmixfvrwD\nnwoA2BwIXgEAAACAbmnt2rX5yU9+0uyYH/3oRy2aa7vttsugQYMyaNCg7LDDDlm2bFmqq6szY8aM\nnH/++XnPe96TyZMnZ+3ate3ROgCwGRK8AgAAAADdzq677pokuemmm5ocs3Tp0tx1113ZfvvtM2DA\ngGbnu+iii7Jw4cIsXLgwL7/8clavXp2//vWvmTFjRg499NC88cYbueKKKzJ27FjhKwBspQSvAAAA\nAEC3c+ihh2aPPfbI448/nurq6kbH3HLLLVm9enU+/elPZ5tttmn1PYYMGZJTTz018+bNy2WXXZYk\n+dWvfpWLL764rN4BgM2T4BUAAAAA6JbGjx+fpOlVr+vPn3766WXdpyiKTJkyJSeeeGKS5Pvf//4m\n3y0LAGx5BK8AAAAAQLe0Pnj98Y9/nLq6ug2u/eEPf8jDDz+cXXbZJaNHj26X+339619Pkrzxxhv5\n6U9/2i5zAgCbD8ErAAAAANAtDR06NIcddlheeumlVFVVbXBt/WrXU089NRUV7fPPqAceeGDe/e53\nJ0l+/etft8ucAMDmQ/AKAAAAAHRb67cRvvnmmxvOlUqlzJgxY4Pr7eX9739/kuRPf/pTu84LAHQ9\nwSsAAAAA0G2ddNJJ6dOnT+64446sXLkySTJ37tz8+c9/zgc+8IHsu+++7Xq/AQMGJEmWLl3arvMC\nAF1P8AoAAAAAdFv9+/fP8ccfn5UrV2bWrFlJ3tpmuL1Xuyb1q2kBgK2T4BUAAAAA6Nbevt3wG2+8\nkdtvvz2VlZU5+eST2/1ey5YtS/LWylcAYOsheAUAAAAAurVjjz02AwcOzP3335+rrroqK1asyNix\nY7PTTju1+72efPLJJMnQoUPbfW4AoGsJXgEAAACAbq1nz545+eSTU1dXl4svvjhJMn78+Ha/zxNP\nPJGFCxcmSQ4//PB2nx8A6FqCVwAAAACg21u/3fCaNWuy44475vjjj2/3e/zLv/xLkmTbbbfNJz/5\nyXafHwDoWj27ugEAAAAAgK42YsSIXHrppVmxYkUOOOCA9O7du93mLpVKueKKK3L77bcnSb74xS9m\n4MCB7TY/ALB5ELwCAAAAACS55JJL2nW+l156KXPnzs1VV12V3/zmN0mSY445JlOnTm3X+/D/t3f/\nUXaX9Z3A309IoiE4iK4YaKsgK5XGqiQgaSJJYU6zZauLnqNWT+spEELx7FHwx3pg2YRf7p7aItIV\nXQsS6c9jtbrQ3eMPNJYEzMoCOWCJQKglpFRIoQKzARHIPPvHvQPDZGYyk2/m3rmT1+uc7/nOfb7P\nc+9z//ice7/3Pd/nCwDTg+AVAAAAAKChyy67LF/4wheSJLt27crAwECeeeaZ548feOCB+fjHP541\na9Zk9mw/ywLATOQTHgAAAACgoSeffDJPPvlkkmTu3Lnp6+vLoYcemmOPPTYnnnhi3ve+9+Xggw/u\n8iwBgKkkeAUAAAAAetqWLcn69cnAQNLXl/T3JwsXjt732muvzbXXXjvp13jwwQdHbd+2bduknwsA\nmJkErwAAAABAT1q/PrnkkmTjxt2PLV+erF3bCmEBADphVrcnAAAAAAAwWddck6xcOXromrTaV65M\n1q3r7LwAgP2X4BUAAAAA6Cnr1ydnnZUMDo7fb3AwWb261R8AYKoJXgEAAACAnnLJJXsOXYcMDiaX\nXjq18wEASASvAAAAAEAP2bJl7OWFx7JhQ2scAMBUErwCAAAAAD1jb5cNttwwADDVBK8AAAAAQM8Y\nGOjsOACAiRK8AgAAAAA9o6+vs+MAACZK8AoAAAAA9Iz+/s6OAwCYKMErAAAAANAzFi5Mli+f3JgV\nK1rjAACmkuAVAAAAAOgpa9cmsyb4y+asWcmaNVM7HwCARPAKAAAAAPSY/v7kqqv2HL7OmpVcfbVl\nhgGAzhC8AgAAAAA9Z9Wq5IYbWssIj2bFitbxM87o7LwAgP3X7G5PAAAAAABgb/T3t7YtW5L165OB\ngaSvr9Xmnq4AQKcJXgEAAACAnrZwoaAVAOg+Sw0DAAAAAAAANCR4BQAAAAAAAGhI8AoAAAAAAADQ\nkOAVAAAAAAAAoCHBKwAAAAAAAEBDglcAAAAAAACAhgSvAAAAAAAAAA0JXgEAAAAAAAAaErwCAAAA\nAAAANCR4BQAAAAAAAGhI8AoAAAAAAADQkOAVAAAAAAAAoCHBKwAAAAAAAEBDglcAAAAAAACAhgSv\nAAAAAAAAAA0JXgEAAAAAAAAaErwCAAAAAAAANCR4BQAAAAAAAGhI8AoAAAAAAADQkOAVAAAAAAAA\noCHBKwAAAAAAAEBDglcAAAAAAACAhgSvAAAAAAAAAA0JXgEAAAAAAAAaErwCAAAAAAAANCR4BQAA\nAAAAAGhI8AoAAADMWKeddlpKKXvcrrjiitx6663PP77zzjvHfM5TTjnl+X6bN28es9873vGOlFLy\n9re/fSreGgAAMM3M7vYEAAAAAKbanDlz8opXvGLM4/Pnz8+iRYty0EEHZefOndm4cWPe/OY379Zv\n165d2bRp0/OPN27cmEWLFu3Wb3BwMN///veTJCtWrNgH7wAAAJjuXPEKAAAAzHhLly7Nww8/POa2\nevXqHHDAAVm2bFmSVqA6mjvvvDMDAwN59atfPW6/v//7v89jjz2WJFm+fPkUvCMAAGC6EbwCAAAA\ntA2FpDfddNOox4eC1rPOOivz58/PzTffPG6/gw46KIsXL56CmQIAANON4BUAAACgbWhZ4B07duTe\ne+/d7fhQIHvSSSdlyZIleeSRR3L33XeP2W/p0qWZPdudngAAYH8geAUAAABoO/744zNv3rwkoy8j\nfNNNN2XOnDk54YQTcuKJJ47bL7HMMAAA7E8ErwAAAABtc+fOzZIlS5LsHqjec889eeSRR7J48eIc\neOCBedvb3jZqv61bt+bhhx9O8sIVtAAAwMwneAUAAABmvE2bNmXBggWjbqeffvqL+g5dpToyUB16\nPHSl65IlSzJ79uzd7gc71G/evHl561vfOiXvBwAAmH7cZAQAAACY8Z599tns2LFj1GOPPfbYix4P\nXaW6ffv2PPDAA3nta1+bZPflg+fPn59jjz02t956a7Zt25YjjjjiRf1OOOGEzJ07d5+/FwAAYHpy\nxSsAAAAw461YsSK11lG366677kV9lyxZ8nxgOvyq140bN6aUkmXLlj3fNtpyw0N/W2YYAAD2L4JX\nAAAAgGHmzZuX4447LskLIer27duzffv2vPGNb8whhxzyfN+hZYeH+j344IPZtm1bEsErAADsbwSv\nAAAAACMMhaZDgerI+7sOGXnF64YNG5Ikc+fOzZIlSzoyVwAAYHoQvAIAAACMMHQf161bt2bHjh3P\n37d1ZPD6qle9KkcffXTuu+++F/U7/vjjM2/evM5OGgAA6CrBKwAAAMAIy5YtywEHHJCkdTXr0BWt\nQ4HscMOXG3Z/VwAA2H8JXgEAAABGeNnLXpZjjz02SfK1r30t99xzT173utfl8MMP363v0HLDX//6\n13P33XcnGT2gBQAAZjbBKwAAAMAohq5a/epXv5pk92WGhwy1f+UrX0mSzJ49O8uWLevADAEAgOlE\n8AoAAAAwiqGrVgcHB5OMHbweddRROeyww57vt2jRohx00EGdmSQAADBtzO72BAAAAAAmasuWZP36\nZGAg6etL+vuThQun5rVOPPHEzJo1a4/Ba9JabnjoyljLDAMAwP6p1Fq7PYcZoZRy+6JFixbdfvvt\n3Z4KAAAAzDjr1yeXXJJs3Lj7seXLk7VrWyEsAADAZC1evDibN2/eXGtd3OR5LDUMAAAATGvXXJOs\nXDl66Jq02leuTNat6+y8AAAAhhO8AgAAANPW+vXJWWcl7dV+xzQ4mKxe3eoPAADQDYJXAAAAYNq6\n5JI9h65DBgeTSy+d2vkAAACMRfAKAAAATEtbtoy9vPBYNmxojQMAAOg0wSsAAAAwLe3tssGWGwYA\nALpB8AoAAABMSwMDnR0HAADQhOAVAAAAmJb6+jo7DgAAoAnBKwAAADAt9fd3dhwAAEATglcAAABg\nWlq4MFm+fHJjVqxojQMAAOg0wSsAAAAwba1dm8ya4K8Xs2Yla9ZM7XwAAADGIngFAAAApq3+/uSq\nq/Ycvs6alVx9tWWGAQCA7hG8AgAAANPaqlXJDTe0lhEezYoVreNnnNHZeQEAAAw3u9sTAAAAANiT\n/v7WtmVLsn59MjCQ9PW12tzTFQAAmA4ErwAAAEDPWLhQ0AoAAExPlhoGAAAAAAAAaEjwCgAAAAAA\nANCQ4BUAAAAAAACgIcErAAAAAAAAQEOCVwAAAAAAAICGBK8AAAAAAAAADQleAQAAAAAAABoSvAIA\nAAAAAAA0JHgFAAAAAAAAaEjwCgAAAAAAANCQ4BUAAAAAAACgIcErAAAAAAAAQENdD15LKXNKKeeU\nUr5USrmjlPJMKaWWUs4cZ8xp7T5jbWePMW5eKeXiUsq9pZSnSyn/Ukr5SinlmKl7hwAAAAAAAMBM\nN7vbE0gyP8kV7b93JHk4yS9NcOz1Se4Ypf22kQ2llJck+U6SZe3jf9x+nfck+a1Sysm11lsmN3UA\nAAAAAACA6RG8PpXk3ye5o9b6UCnloiQXTnDsdbXWayfY96Npha5/k+S3a62DSVJK+esk1yVZV0r5\n1aF2AAAAAAAAgInq+lLDtdZnaq3frLU+NFWvUUopSYaWH/7E8HC11np9kpuS/EqSFVM1BwAAAAAA\nAGDm6nrw2tBbSinnllLOK6V8oJTyi2P0OyrJa5JsrbXeP8rxb7b3J0/JLAEAAAAAAIAZbTosNdzE\nOSMe7yqlfDHJubXWp4e1/3J7v3WM57mvvT96Ty9YSrl9jENv2NNYAAAAAAAAYGbq1Ste70/yobQC\n1flJDk/y3iTbkvx+knUj+h/c3j8xxvMNtb98n84SAAAAAAAA2C/skyteSynbkrx2EkP+stb6u3v7\nerXWDUk2DGt6KslXSyk/SHJnkveXUj5Va71zb19jnNdePFp7+0rYRfv69QAAAAAAAIDpb18tNfzj\nJE/vsdcLfrKPXvdFaq3/VEr5RpLfSbI8rRA2eeGK1oNHHfhC++NTMS8AAAAAAABgZtsnwWuttX9f\nPM8+8kh7P39Y273t/Vj3cH19ez/WPWABAAAAAAAAxtSr93gdzwnt/T8Oa/txku1Jji6lHDnKmFPa\n++9N5cQAAAAAAACAmakng9dSynGjtM0qpZyf5NeSPJrkW0PHaq01yRfaD/+wlDJr2LhTk5yY5Ed5\n8X1jAQAAAAAAACZkX93jtZFSynlJ3tB++Jb2/vRSytvaf99ca/3isCG3llLuSuserv+c1j1alyV5\nY5KnkvxOrXVgxMtcnuTtSd6d5JZSyvokr0nynvaYM2qtg/v2nQEAAAAAAAD7g2kRvCb5zSQrRrQt\nbW9DhgevlyV5a5KTk7wiyWBaSwl/LsnltdbhywwnSWqtPy+l/EaS85K8P8lHkgwkuS7JhbXWH+2b\ntwIAAAAAAADsb6ZF8Fpr/fVJ9v9Pe/k6TyVZ294AAAAAAAAA9omevMcrAAAAAAAAwHQieAUAAAAA\nAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4B\nAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABA\nQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAA\nAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8\nAgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAA\ngIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAA\nAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQk\neAUAgP3EaaedllLKhLYrrrgiSfLQQw/lkEMOSSkla9asGff5r7zyypRScuCBB+a+++7rxFsCAAAA\nmDYErwAAsJ+ZM2dOXv3qV4+7zZ8/P0ly2GGH5bLLLkuSfOpTn8oPf/jDUZ/zgQceyPnnn58kufji\ni/P617++M28GAAAAYJqY3e0JAAAAnbV06dLceOONE+6/atWqfPnLX853v/vdrFq1Kj/4wQ9ywAEH\nvKjP6tWrs3Pnzhx33HH56Ec/uo9nDAAAADD9ueIVAADYo6uuuirz58/Pbbfdlssvv/xFx9atW5fv\nfOc7mTNnTq655prdQlkAAACA/YHgFQAA2KMjjzwyn/zkJ5MkF1544fP3cP3JT36Sj33sY0mS888/\nP29605u6NkcAAACAbhK8AgAAE/LhD384S5Ysyc9+9rOsXr06tdZ88IMfzOOPP56FCxfmggsu6PYU\nAQAAALpG8AoAAEzIrFmzsm7dusydOzcbNmzIu971rvzt3/5tZs2alWuuuSZz587t9hQBAAAAumZ2\ntycAAAB01qZNm7JgwYJx+2zdujV9fX27tR9zzDFZs2ZN1qxZk+uvvz5Jcu655+aEE06YkrkCAAAA\n9ArBKwAA7GeeffbZ7NixY9w+g4ODYx5bvXp1LrroouzatSt9fX259NJL9/UUAQAAAHqOpYYBAGA/\ns2LFitRax91e/vKXjzn+E5/4RHbt2pUkGRgYyNe//vVOTR0AAABg2hK8AgAAE/btb387f/Znf5Yk\nWblyZZLkIx/5SB599NFuTgsAAACg6wSvAADAhOzcuTNnnXVWkuTMM8/Mddddl6OOOiqPPvpozjnn\nnC7PDgAAAKC7BK8AAMCEnHfeedm+fXsOP/zwXHbZZZk3b16uuuqqJMlf/dVf5Vvf+laXZwgAAADQ\nPYJXAABgj26++eZ8/vOfT5J8/vOfz8EHH5wkOfnkk3PGGWckSc4+++zs3Lmza3MEAAAA6CbBKwAA\nMK6nn346q1atSq01733ve3Pqqae+6Phll12WBQsW5IEHHsgFF1zQpVkCAAAAdJfgFQAAGNdFF12U\nrVu35pWvfGU++9nP7nb8kEMOeb79yiuvzC233NLpKQIAAAB0neAVAAD2M5s2bcqCBQvG3c4555wk\nyebNm/PpT386SfKZz3wmhx566KjP+e53vzvvfOc7Mzg4mDPPPDPPPvtsx94PAAAAwHQgeAUAgP3M\ns88+mx07doy7PfHEE3nuuedyxhln5Lnnnsspp5ySD3zgA+M+7+c+97kcfPDBueuuu/IHf/AHHXo3\nAAAAANNDqbV2ew4zQinl9kWLFi26/fbbuz0VAAD2I1u2JOvXJwMDSV9f0t+fLFzY7VkBAAAA9I7F\nixdn8+bNm2uti5s8z+x9NSEAAKBz1q9PLrkk2bhx92PLlydr17ZCWAAAAAA6w1LDAADQY665Jlm5\ncvTQNWm1r1yZrFvX2XkBAAAA7M8ErwAA0EPWr0/OOisZHBy/3+Bgsnp1qz8AAAAAU0/wCgAAPeSS\nS/Ycug4ZHEwuvXRq5wMAAABAi+AVAAB6xJYtYy8vPJYNG1rjAAAAAJhaglcAAOgRe7tssOWGAQAA\nAKae4BUAAHrEwEBnxwEAAAAwcYJXAADoEX19nR0HAAAAwMQJXgEAoEf093d2HAAAAAATJ3gFAIAe\nsXBhsnz55MasWNEaBwAAAMDUErwCAEAPWbs2mTXBb/GzZiVr1kztfAAAAABoEbwCAEAP6e9Prrpq\nz+HrrFnJ1VdbZhgAAACgUwSvAADQY1atSm64obWM8GhWrGgdP+OMzs4LAAAAYH82u9sTAAAAJq+/\nv7Vt2ZKsX58MDCR9fa0293QFAAAA6DzBKwAA9LCFCwWtAAAAANOBpYYBAAAAAAAAGhK8AgAAAAAA\nADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAA\nAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAh\nwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAA\nAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4B\nAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABA\nQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAA\nAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8\nAgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAA\ngIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAA\nAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQk\neAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAA\nAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsA\nAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABo\nqNRauz2HGaGU8q/z5s17xTHHHNPtqQAAAAAAAAATdPfdd+dnP/vZT2utr2zyPILXfaSUcn+SviTb\nujwV6FVvaO/v6eosgKmgvmFmUtswc6lvmLnUN8xMahtmLvXdOUckGai1HtnkSQSvwLRQSrk9SWqt\ni7s9F2DfUt8wM6ltmLnUN8xc6htmJrUNM5f67j3u8QoAAAAAAADQkOAVAAAAAAAAoCHBKwAAAAAA\nAEBDglcAAAAAAACAhgSvAAAAAAAAAA2VWmu35wAAAAAAAADQ01zxCgAAAAAAANCQ4BUAAAAAAACg\nIcErAAAAAAAAQEOCVwAAAAAAAICGBK8AAAAAAAAADQleAQAAAAAAABoSvAIAAAAAAAA0JHgFplwp\nZU4p5ZxSypdKKXeUUp4ppdRSypnjjDmt3Wes7ewxxs0rpVxcSrm3lPJ0KeVfSilfKaUcM3XvEPZP\ne1Pbw8b+Xinl/5ZSdpZSniil3FhKefs4/dU2TAOllCP28Pn85XHGTqrugc4rpfxiKWVdKeUnpZSf\nl1K2lVKuKKUc0u25AeNr1+tYn88PjzFmaSnlG6WUn5ZSflZK+WEp5dxSygGdnj/s70op7y6lfLaU\nclMpZaBdu3+xhzGTrmHfyaHzJlPfzrlnhtndngCwX5if5Ir23zuSPJzklyY49vokd4zSftvIhlLK\nS5J8J8my9vE/br/Oe5L8Vinl5FrrLZObOjCOvartUsplSQmZ2k0AAAipSURBVD6W5MEkVyeZm+R9\nSf5XKeVDtdYrR/RX2zD93JnkulHa7xqt82TrHui8UspRSTYlOTSt7+D3JHlrknOS/GYpZVmt9V+7\nOEVgz57IC9/Ph9s5sqGUcmqSryV5OslfJ/lpknck+Uxa37vfM3XTBEbxX5K8Oa16fTDJG8brvDc1\n7Ds5dM2k6rvNOXcPK7XWbs8BmOFKKXOT9Ce5o9b6UCnloiQXJllda/3iGGNOS/KlJKfXWq+d4Ouc\nn+S/JfmbJL9dax1st5+a1gfVj5L86lA70Mxe1vbSJN9P8uMkx9daH2u3H5Hk9rTC3DfUWrcNG6O2\nYZpo1+r9Sf601nraBMdMuu6BziulfDvJyiQfrrV+dlj75Uk+kuRPaq2jrjoDdF8pZVuS1FqPmEDf\nviT/kOTgJMtqrbe121+a5HtJfi3J+2utY15VA+xbpZST0gpM/iHJiiR/l+Qva62/O0rfSdew7+TQ\nPZOs7yPinLvnWWoYmHK11mdqrd+stT40Va9RSilJhn4I+sTwAKbWen2Sm5L8SlofbsA+sJe1PVSn\n/3Xoi2D7ubYl+VySlyQ5fahdbcOMMKm6BzqvfbXryiTb0qrL4S5M8mSSD5RS5nd4asDUeHeSVyX5\n8lBgkyS11qfTuionST7YjYnB/qrW+ne11vvqxK6S2psa9p0cumSS9b031Pc0I3gFpru3tO9PcV4p\n5QOllF8co99RSV6TZGut9f5Rjn+zvT95SmYJTNRQDX5rlGOj1anahunp8FLK75dS/nN7/6Zx+k62\n7oHOO6m9v2HkChK11v+X1n/QH5hkSacnBkzKS0opv9v+fD6nlHLSGPd6HO+zeWOSp5Isbd/yA5h+\n9qaGfSeH3uKcu4e5xysw3Z0z4vGuUsoXk5zb/k++Ib/c3m8d43nua++P3peTAyaufZXMLyTZOcZV\nsqPVqdqG6ek32tvzSik3Jvm9Wuv2YW17U/dA503k83ZlWrW6viMzAvbGgiR/PqLt/lLK6bXWDcPa\nxqz5WutzpZT7kyxM8rokd0/JTIEmJlXDvpNDT3LO3cNc8QpMV/cn+VBaXybnJzk8yXvTWv7s95Os\nG9H/4Pb+iTGeb6j95ft0lsBk7E2dqm2YXp5KcmmSxUkOaW9D96j59STrRyxFqoahN6hV6H1fStKf\nVvg6P8mvJvmTJEck+WYp5c3D+qp56G2TrWE1D73DOfcMIHgFJqSUsq2UUiex/UWT16u1bqi1Xllr\n3VprfarW+lCt9atpLYP2WJL3jzhxBPZCp2sb6L4mdV9r/Zda69pa6+Za6+PtbWNaV8LdkuTfJjmz\nW+8NAPZXtdaLa63fq7XuaJ9D31VrPTvJ5UnmJbmouzMEAPbEOffMYKlhYKJ+nOTpPfZ6wU+mYhK1\n1n8qpXwjye8kWZ7kzvahof/eOXjUgS+0Pz4V84Ie1sna3ps6Vduw7+3zum8vafbFJCek9fn8x+1D\nahh6g1qFmesLST6W1ufzEDUPvW2yNazmocc55+4tgldgQmqt/d2ewzCPtPfDl1W4t70fa73617f3\nY923CvZLnaztWuuTpZR/TvILpZTDRrn3xGh1qrZhH5vCut/t83kv6x7oPJ+3MHONdf58XFo1f/vw\nzqWU2UmOTPJckn/sxASBSZtUDftODjOGc+4eYalhoBed0N4PPwn8cZLtSY4upRw5yphT2vvvTeXE\ngD0aqsHfHOXYaHWqtqF3LGnvR/5IO9m6Bzrv79r7laWUF/1OUEp5WZJlad1v6gednhjQ2Gifz+N9\nNi9PcmCSTbXWn0/lxIC9tjc17Ds59D7n3D1C8ApMS6WU40Zpm1VKOT/JryV5NMm3ho7VWmtaSygl\nyR8O/8GolHJqkhOT/CjJhqmcN7BHQ3V6QSnlkKHGUsoRSf5jkp8n+dJQu9qG6aWUsmhkKNNu70/y\nkfbDkfeCnlTdA51Xa/1xkhuSHJFWXQ53cVr/Vf/ntdYnOzw1YAJKKceUUuaP0n5EkivbD4d/Pv9N\nWufU7xt+7l1KeWmST7Yf/o8pmSywL+xNDftODj3AOffMUFq/ZwJMrVLKeUne0H74liRvTrIpyX3t\ntptrrV8c1r8muSute7j+c1rr0S9L8sa0/tv+XbXWG0a8xkvS+u+dpUluS7I+yWuSvCfJM0lOrrXe\nMhXvD/ZXk63t9phPJ/lokgfTOmGcm+S3k7wyyYdqrVeO6K+2YZoopdyY1lJFm9Kq4SR5U5KT23+v\nqbV+cpRxk6p7oPNKKUelVduHJrk+yd1prTRzUlpLky2ttf5r92YIjKWUclFa93HdmOSBJP8vyVFJ\nfivJS5N8I61z6GeGjXlnWp/JTyf5cpKfJvkPSX653f7e6kdD6Jh2Tb6z/XBBkn+X1lVtN7XbHq21\nfnxE/0nVsO/k0B2TqW/n3DOD4BXoiPaHxopxuvxprfW0Yf3/KMlb0/qgeUWSwbSWG/1ukstrraPe\na6aUcmCS85K8P61gZiDJjUkurLX+qOn7AF5ssrU9bNxpaf3X3a+kVd+bk/xRrfV/j/E6ahumgVLK\nqiTvSusfof5NkjlJdiT5P0murLXeNM7Y0zKJugc6r5TyS0kuSWuZslcmeSjJ/0xyca31sW7ODRhb\nKWVFkrOTHJvWD7rzkzye5I4kf57WFeu7/QBYSlmW5IK0VpV6aZJ/SLIuyX+vte7qzOyB5Pl/oLhw\nnC4P1FqPGDFm0jXsOzl03mTq2zn3zCB4BQAAAAAAAGjIPV4BAAAAAAAAGhK8AgAAAAAAADQkeAUA\nAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAAAABAQ4JXAAAAAAAAgIYErwAAAAAAAAAN\nCV4BAAAAAAAAGhK8AgAAAAAAADQkeAUAAAAAAABoSPAKAAAAAAAA0JDgFQAAAAAAAKAhwSsAAAAA\nAABAQ4JXAAAAAAAAgIYErwAAAAAAAAANCV4BAAAAAAAAGvr/vhIZjUIPKGMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77053d9c18>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 902,
       "width": 943
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
